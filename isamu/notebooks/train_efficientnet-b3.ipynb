{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ab8fad",
   "metadata": {
    "id": "tt86b3JrLbCn"
   },
   "source": [
    "# colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275a742",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19935,
     "status": "ok",
     "timestamp": 1633333059989,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "-xBsxFOLLjyM",
    "outputId": "6e9709a7-c287-4b5d-c093-e9be86e3b070"
   },
   "outputs": [],
   "source": [
    "# mount google drive path into colab notebook\n",
    "# if you run this notebook in kaggle notebook or other platform, comment out the following code\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535476b3",
   "metadata": {
    "id": "edebA9OXL7EP"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65cf70",
   "metadata": {
    "id": "E5EO7gnsuwWF"
   },
   "outputs": [],
   "source": [
    "root = '/content/drive/MyDrive/Colab Notebooks/g2net/' # set your root directory in your google drive. if you use Kaggle notebook, set this to '.'\n",
    "OUTPUT_DIR = root + '/final/' # set your current folder to save model weights and outputs files\n",
    "DATA_DIR = root + '/data/'    # put whiten profile data here to run whiten process on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73486551",
   "metadata": {
    "id": "I6hHnIrtyj-2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# CQT params\n",
    "FMIN=22\n",
    "FMAX=None\n",
    "WINDOW_TYPE='nuttall'\n",
    "BINS=64\n",
    "HOP_LENGTH = 32\n",
    "SCALE=1\n",
    "NORM=1\n",
    "OCTAVE=12\n",
    "\n",
    "SMOOTHING=0.00\n",
    "ST=int(4096 / 16 * 7)\n",
    "EN=int(4096 / 16 * 15)\n",
    "\n",
    "# FOLD\n",
    "NUM_FOLDS = 5\n",
    "FOLDS=[0, 1, 2, 3, 4] \n",
    "\n",
    "# MODEL params\n",
    "LR=1e-4 \n",
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Efficientnet params\n",
    "EFFICIENTNET_SIZE = 3 # 3, 4, 5, 7\n",
    "WEIGHTS = \"imagenet\" #\"noisy-student\" or \"imagenet\"\n",
    "\n",
    "# If you want to use other model, set the model path provided by tfhub.  https://tfhub.dev/\n",
    "TFHUB_MODEL=None # 'https://tfhub.dev/google/imagenet/inception_v3/feature_vector/5' # inception-v3: \n",
    "\n",
    "WHITE=True\n",
    "MIXED=True # use mixed precision to speed up. but it does not work with tf models\n",
    "HARDEN=None\n",
    "MIXUP_PROB = 0.0\n",
    "EPOCHS = 20\n",
    "R_ANGLE = 0 / 180 * np.pi\n",
    "S_SHIFT = 0.0\n",
    "T_SHIFT = 0.0\n",
    "LABEL_POSITIVE_SHIFT = 1.0\n",
    "\n",
    "SEED = 2021\n",
    "\n",
    "# G2Net SKF(Stratified KFold) dataset\n",
    "# gs-path were generated from https://www.kaggle.com/yamsam/g2net-skf-path\n",
    "# tf reccords Dataset are stored in https://www.kaggle.com/vincentwang25/g2net-skf\n",
    "FILES =['gs://kds-545de03072f7f12c036f4c687111566f0a27586e55b81c3a5ee34eff']\n",
    "\n",
    "# Pseudo Label dataset\n",
    "# gs-path were generated from  https://www.kaggle.com/yamsam/g2net-tpu-soft-pseudo-path\n",
    "# tf reccords Dataset are stored in https://www.kaggle.com/yamsam/g2net-public-s-01, https://www.kaggle.com/yamsam/g2net-public-s-02\n",
    "PFILES = ['gs://kds-bc6ce0467b324bf699c0f253c26655b210f3d84d5f73624eecd9f0c9', \n",
    "          'gs://kds-2d2718f40449c5e3ad78362852d1ac9328d70f6cdf5ed5f2c4d6edd1']\n",
    "#PFILES=[] # set [], if you do not use pseudo labeling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2132dbd",
   "metadata": {
    "id": "47614e53"
   },
   "outputs": [],
   "source": [
    "!pip install efficientnet tensorflow_addons > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b144fef",
   "metadata": {
    "id": "5930c28a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import efficientnet.tfkeras as efn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from scipy.signal import get_window\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad6045",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1633333068249,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "e91524c8",
    "outputId": "14367b0a-c8c4-46f9-d586-6c0d091a046e"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c495bb8e",
   "metadata": {
    "id": "02c18519"
   },
   "outputs": [],
   "source": [
    "SAVEDIR = Path(OUTPUT_DIR + \"models\")\n",
    "SAVEDIR.mkdir(exist_ok=True)\n",
    "\n",
    "OOFDIR = Path(OUTPUT_DIR + \"oof\")\n",
    "OOFDIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ebac4",
   "metadata": {
    "id": "c70bc2a1"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47809c45",
   "metadata": {
    "id": "53835b0d"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdb2bd",
   "metadata": {
    "id": "a73e2541"
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    TPU_DETECTED = False\n",
    "    try:\n",
    "        if MIXED and TFHUB_MODEL is None:\n",
    "          tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "          tf.config.experimental_connect_to_cluster(tpu)\n",
    "          tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "          strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "          policy = mixed_precision.Policy('mixed_bfloat16')\n",
    "          mixed_precision.set_global_policy(policy)\n",
    "          tf.config.optimizer.set_jit(True)\n",
    "        else:\n",
    "          tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "          tf.config.experimental_connect_to_cluster(tpu)\n",
    "          tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "          strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        print(\"Running on TPU:\", tpu.master())\n",
    "        TPU_DETECTED = True\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "\n",
    "    return strategy, TPU_DETECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64813771",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8376,
     "status": "ok",
     "timestamp": 1633333077384,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "1871c557",
    "outputId": "463ec06c-8cb8-48ae-e6b7-0bff885e070a"
   },
   "outputs": [],
   "source": [
    "strategy, tpu_detected = auto_select_accelerator()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a1c5c",
   "metadata": {
    "id": "57dd2278"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f07d0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633333077386,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "99de507f",
    "outputId": "75d0bfdb-ada4-4787-8e51-22415d092f46"
   },
   "outputs": [],
   "source": [
    "gcs_paths = []\n",
    "for file in FILES:\n",
    "    gcs_paths.append(file)\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84073d1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1633333077762,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "f5da9d2c",
    "outputId": "2a72d4f6-de34-429a-c562-98a2155d6394"
   },
   "outputs": [],
   "source": [
    "fold_files = []\n",
    "for path in gcs_paths:\n",
    "    for foldi in range(5):\n",
    "        folds = []\n",
    "        folds.extend(np.sort(np.array(tf.io.gfile.glob(path + f\"/tr{foldi}_*.tfrecords\")))) # !!!\n",
    "        fold_files.append(folds)\n",
    "\n",
    "        print(f\"train_files fold{foldi}: \", len(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14771a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1633333077763,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "M_DJ8YZXG5S-",
    "outputId": "38c626c4-7788-49f6-ff2b-72258cafa0b7"
   },
   "outputs": [],
   "source": [
    "p_gcs_paths = []\n",
    "for file in PFILES:\n",
    "    p_gcs_paths.append(file)\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82541d5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633333077764,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "-VOGx1AlG9Mf",
    "outputId": "f86a0a3d-4a96-426c-b11a-c96b00c1dfec"
   },
   "outputs": [],
   "source": [
    "pseudo_files = []\n",
    "\n",
    "for path in p_gcs_paths:\n",
    "  pseudo_files.extend(np.sort(np.array(tf.io.gfile.glob(path + f\"/*.tfrecords\")))) # !!!\n",
    "\n",
    "print(f\"pseudo_files: \", len(pseudo_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9a152",
   "metadata": {
    "id": "6b64040d"
   },
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Here's the main contribution of this notebook - Tensorflow version of on-the-fly CQT computation. Note that some of the operations used in CQT computation are not supported by TPU, therefore the implementation is not a TF layer but a function that runs on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdabb69",
   "metadata": {
    "id": "f909e45a"
   },
   "outputs": [],
   "source": [
    "def create_cqt_kernels(\n",
    "    q: float,\n",
    "    fs: float,\n",
    "    fmin: float,\n",
    "    n_bins: int = 84,\n",
    "    bins_per_octave: int = 12,\n",
    "    norm: float = 1,\n",
    "    window: str = \"hann\",\n",
    "    fmax: Optional[float] = None,\n",
    "    topbin_check: bool = True\n",
    ") -> Tuple[np.ndarray, int, np.ndarray, float]:\n",
    "    fft_len = 2 ** _nextpow2(np.ceil(q * fs / fmin))\n",
    "    \n",
    "    if (fmax is not None) and (n_bins is None):\n",
    "        n_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "    elif (fmax is None) and (n_bins is not None):\n",
    "        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "    else:\n",
    "        warnings.warn(\"If nmax is given, n_bins will be ignored\", SyntaxWarning)\n",
    "        n_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "        \n",
    "    if np.max(freqs) > fs / 2 and topbin_check:\n",
    "        raise ValueError(f\"The top bin {np.max(freqs)} Hz has exceeded the Nyquist frequency, \\\n",
    "                           please reduce the `n_bins`\")\n",
    "    \n",
    "    kernel = np.zeros((int(n_bins), int(fft_len)), dtype=np.complex64)\n",
    "    \n",
    "    length = np.ceil(q * fs / freqs)\n",
    "    for k in range(0, int(n_bins)):\n",
    "        freq = freqs[k]\n",
    "        l = np.ceil(q * fs / freq)\n",
    "        \n",
    "        if l % 2 == 1:\n",
    "            start = int(np.ceil(fft_len / 2.0 - l / 2.0)) - 1\n",
    "        else:\n",
    "            start = int(np.ceil(fft_len / 2.0 - l / 2.0))\n",
    "\n",
    "        sig = get_window(window, int(l), fftbins=True) * np.exp(\n",
    "            np.r_[-l // 2:l // 2] * 1j * 2 * np.pi * freq / fs) / l\n",
    "        \n",
    "        if norm:\n",
    "            kernel[k, start:start + int(l)] = sig / np.linalg.norm(sig, norm)\n",
    "        else:\n",
    "            kernel[k, start:start + int(l)] = sig\n",
    "    return kernel, fft_len, length, freqs\n",
    "\n",
    "\n",
    "def _nextpow2(a: float) -> int:\n",
    "    return int(np.ceil(np.log2(a)))\n",
    "\n",
    "def prepare_cqt_kernel(\n",
    "    sr=22050,\n",
    "    hop_length=512,\n",
    "    fmin=32.70,\n",
    "    fmax=None,\n",
    "    n_bins=84,\n",
    "    bins_per_octave=12,\n",
    "    norm=1,\n",
    "    filter_scale=1,\n",
    "    window=\"hann\"\n",
    "):\n",
    "    q = float(filter_scale) / (2 ** (1 / bins_per_octave) - 1)\n",
    "    print(q)\n",
    "    return create_cqt_kernels(q, sr, fmin, n_bins, bins_per_octave, norm, window, fmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f63b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1633333078265,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "0ab66746",
    "outputId": "3cf40486-60c5-4034-adcc-1da934e35a1e"
   },
   "outputs": [],
   "source": [
    "\n",
    "cqt_kernels, KERNEL_WIDTH, lengths, _ = prepare_cqt_kernel(\n",
    "    sr=2048,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    fmin=FMIN,\n",
    "    fmax=FMAX,\n",
    "    n_bins=BINS,\n",
    "    norm=NORM,\n",
    "    window=WINDOW_TYPE,\n",
    "    bins_per_octave=OCTAVE,\n",
    "    filter_scale=SCALE)\n",
    "LENGTHS = tf.constant(lengths, dtype=tf.float32)\n",
    "CQT_KERNELS_REAL = tf.constant(np.swapaxes(cqt_kernels.real[:, np.newaxis, :], 0, 2))\n",
    "CQT_KERNELS_IMAG = tf.constant(np.swapaxes(cqt_kernels.imag[:, np.newaxis, :], 0, 2))\n",
    "PADDING = tf.constant([[0, 0],\n",
    "                        [KERNEL_WIDTH // 2, KERNEL_WIDTH // 2],\n",
    "                        [0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a89d2",
   "metadata": {
    "id": "da3bc9ed"
   },
   "outputs": [],
   "source": [
    "def create_cqt_image(wave, hop_length=16):\n",
    "    CQTs = []\n",
    "    for i in range(3):\n",
    "        x = wave[i][ST:EN]\n",
    "        x = tf.expand_dims(tf.expand_dims(x, 0), 2)\n",
    "        x = tf.pad(x, PADDING, \"REFLECT\")\n",
    "\n",
    "        CQT_real = tf.nn.conv1d(x, CQT_KERNELS_REAL, stride=hop_length, padding=\"VALID\")\n",
    "        CQT_imag = -tf.nn.conv1d(x, CQT_KERNELS_IMAG, stride=hop_length, padding=\"VALID\")\n",
    "        CQT_real *= tf.math.sqrt(LENGTHS)\n",
    "        CQT_imag *= tf.math.sqrt(LENGTHS)\n",
    "\n",
    "        CQT = tf.math.sqrt(tf.pow(CQT_real, 2) + tf.pow(CQT_imag, 2))\n",
    "        CQTs.append(CQT[0])\n",
    "    return tf.stack(CQTs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa165e2",
   "metadata": {
    "id": "2a575fd0"
   },
   "outputs": [],
   "source": [
    "def read_labeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_image(example[\"wave\"], IMAGE_SIZE), tf.reshape(tf.cast(example[\"target\"], tf.float32), [1])\n",
    "\n",
    "\n",
    "def read_unlabeled_tfrecord(example, return_image_id):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_image(example[\"wave\"], IMAGE_SIZE), example[\"wave_id\"] if return_image_id else 0\n",
    "\n",
    "\n",
    "def count_data_items(filenames): \n",
    "    # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "def mixup(image, label, probability=0.5, aug_batch=64 * 8):\n",
    "    imgs = []\n",
    "    labs = []\n",
    "    for j in range(aug_batch):\n",
    "        p = tf.cast(tf.random.uniform([], 0, 1) <= probability, tf.float32)\n",
    "        k = tf.cast(tf.random.uniform([], 0, aug_batch), tf.int32)\n",
    "        a = tf.random.uniform([], 0, 1) * p\n",
    "\n",
    "        img1 = image[j]\n",
    "        img2 = image[k]\n",
    "        imgs.append((1 - a) * img1 + a * img2)\n",
    "        lab1 = label[j]\n",
    "        lab2 = label[k]\n",
    "        labs.append((1 - a) * lab1 + a * lab2)\n",
    "    image2 = tf.reshape(tf.stack(imgs), (aug_batch, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    label2 = tf.reshape(tf.stack(labs), (aug_batch,))\n",
    "    return image2, label2\n",
    "\n",
    "\n",
    "def time_shift(img, shift=T_SHIFT):\n",
    "    if shift > 0:\n",
    "        T = IMAGE_SIZE\n",
    "        P = tf.random.uniform([],0,1)\n",
    "        SHIFT = tf.cast(T * P, tf.int32)\n",
    "        return tf.concat([img[-SHIFT:], img[:-SHIFT]], axis=0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def rotate(img, angle=R_ANGLE):\n",
    "    if angle > 0:\n",
    "        P = tf.random.uniform([],0,1)\n",
    "        A = tf.cast(angle * P, tf.float32)\n",
    "        return tfa.image.rotate(img, A)\n",
    "    return img\n",
    "\n",
    "\n",
    "def spector_shift(img, shift=S_SHIFT):\n",
    "    if shift > 0:\n",
    "        T = IMAGE_SIZE\n",
    "        P = tf.random.uniform([],0,1)\n",
    "        SHIFT = tf.cast(T * P, tf.int32)\n",
    "        return tf.concat([img[:, -SHIFT:], img[:, :-SHIFT]], axis=1)\n",
    "    return img\n",
    "\n",
    "def img_aug_f(img):\n",
    "#    img = time_shift(img)\n",
    "#    img = spector_shift(img)\n",
    "    #img = tf.image.random_flip_left_right(img) \n",
    "#    img = tf.image.random_brightness(img, 0.2)\n",
    "#    img = AUGMENTATIONS_TRAIN(image=img)['image']\n",
    "    # img = rotate(img)\n",
    "    #print(img.shape)\n",
    "    img = swap_img(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def swap_img(img):\n",
    "   p = tf.random.uniform([],0,1)\n",
    "   if p < 0.2:\n",
    "     img = tf.stack([img[:,:,1], img[:,:,0], img[:,:,2]],axis=2)\n",
    "     return  img\n",
    "   else:\n",
    "     return img\n",
    "\n",
    "\n",
    "def imgs_aug_f(imgs, batch_size):\n",
    "    _imgs = []\n",
    "    DIM = IMAGE_SIZE\n",
    "    for j in range(batch_size):\n",
    "        _imgs.append(img_aug_f(imgs[j]))\n",
    "\n",
    "    return tf.reshape(tf.stack(_imgs),(batch_size,DIM,DIM,3))\n",
    "\n",
    "\n",
    "def label_positive_shift(labels):\n",
    "    return labels * LABEL_POSITIVE_SHIFT\n",
    "\n",
    "\n",
    "def aug_f(imgs, labels, batch_size):\n",
    "    #imgs, label = mixup(imgs, labels, MIXUP_PROB, batch_size)\n",
    "    imgs = imgs_aug_f(imgs, batch_size) \n",
    "    return imgs, labels\n",
    "\n",
    "# used for whitening\n",
    "window = tf.cast(np.load(DATA_DIR+'window.npy'), tf.float64)\n",
    "arv_w = tf.cast(np.load(DATA_DIR+'avr_w.npy'), tf.complex64)\n",
    "\n",
    "def whiten(c):\n",
    "  #print (c.shape)\n",
    "  c2 = tf.concat([tf.reverse(-c, axis=[1])[:,4096-2049:-1] + 2 *c[:,:1], c, tf.reverse(-c, axis=[1])[:,1:2049] + 2*c[:,-2:-1]],axis=1)\n",
    "  #print (c2.shape)\n",
    "  c3 = tf.math.real(tf.signal.ifft(tf.signal.fft(tf.cast(1e20*c2*window, tf.complex64))/arv_w))[:,2048:-2048]\n",
    "  #print (c3.shape)\n",
    "  return c3\n",
    "\n",
    "\n",
    "def prepare_image(wave, dim=256):\n",
    "    wave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n",
    "    #wave = tf.cast(wave, tf.float32)\n",
    "    if WHITE:\n",
    "      wave = whiten(wave)\n",
    "    # normalized_waves = []\n",
    "    # for i in range(3):\n",
    "    #     normalized_wave = wave[i] - means[i]\n",
    "    #     normalized_wave = normalized_wave / stds[i]\n",
    "\n",
    "    #     normalized_waves.append(normalized_wave)\n",
    "    # wave = tf.stack(normalized_waves)\n",
    "    wave = tf.cast(wave, tf.float32)\n",
    "    image = create_cqt_image(wave, HOP_LENGTH)\n",
    "    #image = tf.keras.layers.Normalization()(image)\n",
    "    image = tf.image.resize(image, size=(dim, dim))\n",
    "    return tf.reshape(image, (dim, dim, 3))\n",
    "\n",
    "\n",
    "def get_dataset(files, batch_size=16, repeat=False, shuffle=False, aug=True, labeled=True, return_image_ids=True):\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n",
    "    ds = ds.cache()\n",
    "\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1024 * 10, reshuffle_each_iteration=True)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "\n",
    "    if labeled:\n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), num_parallel_calls=AUTO)\n",
    "\n",
    "    ds = ds.batch(batch_size * REPLICAS)\n",
    "    if aug:\n",
    "        ds = ds.map(lambda x, y: aug_f(x, y, batch_size * REPLICAS), num_parallel_calls=AUTO)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b94167",
   "metadata": {
    "id": "cIWQPTKqdKx8"
   },
   "source": [
    "# soft pseudo labeling dateaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9b377",
   "metadata": {
    "id": "A5roMFPsMOta"
   },
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "  return -tf.math.log(1./x - 1.)\n",
    "\n",
    "def read_softlabeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.float32)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "\n",
    "    label = tf.cast(example[\"target\"], tf.float32)\n",
    "    temperature = 2\n",
    "\n",
    "    if HARDEN and label > 0.75: # Only harden confident positives\n",
    "      label = tf.math.sigmoid(logit(label) * temperature)\n",
    "\n",
    "    return prepare_image(example[\"wave\"], IMAGE_SIZE), tf.reshape(label, [1])\n",
    "\n",
    "\n",
    "def get_soft_dataset(files, batch_size=16, repeat=False, shuffle=False, aug=True, labeled=True, return_image_ids=True):\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n",
    "    ds = ds.cache()\n",
    "\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1024 * 10, reshuffle_each_iteration=True)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "\n",
    "    ds = ds.map(read_softlabeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    ds = ds.batch(batch_size * REPLICAS)\n",
    "    if aug:\n",
    "        ds = ds.map(lambda x, y: aug_f(x, y, batch_size * REPLICAS), num_parallel_calls=AUTO)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29981977",
   "metadata": {
    "id": "a835683f"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54456db5",
   "metadata": {
    "id": "4a384d21"
   },
   "outputs": [],
   "source": [
    "def build_model(size=256, efficientnet_size=0, weights=\"imagenet\", count=0):\n",
    "    inputs = tf.keras.layers.Input(shape=(size, size, 3))\n",
    "    \n",
    "    if TFHUB_MODEL:\n",
    "      load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "      loaded_model = hub.load(TFHUB_MODEL, options=load_options)\n",
    "      efn_layer = hub.KerasLayer(loaded_model, trainable=True) \n",
    "      x = efn_layer(inputs)\n",
    "    else:\n",
    "      efn_string= f\"EfficientNetB{efficientnet_size}\"\n",
    "      efn_layer = getattr(efn, efn_string)(input_shape=(size, size, 3), weights=weights, include_top=False)\n",
    "      x = efn_layer(inputs)\n",
    "      x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(1e-3, count)\n",
    "    opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate=LR)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=SMOOTHING)\n",
    "    #loss = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=[\"AUC\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885abd5",
   "metadata": {
    "id": "ec45dfe6"
   },
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8, replicas=8):\n",
    "    lr_start   = 1e-4\n",
    "    lr_max     = 0.000015 * replicas * batch_size\n",
    "    lr_min     = 1e-7\n",
    "    lr_ramp_ep = 3\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.7\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7cc3c",
   "metadata": {
    "id": "z44mU7F9647i"
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "def display_one_flower(image, title, subplot, red=False):\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis('off')\n",
    "    # for i in range(3):\n",
    "    #   image[i,:] -= image[i,:].min()\n",
    "    #   image[i,:] /= image[i,:].max()\n",
    "#    print (image.shape)\n",
    "    plt.imshow(image[:,:,0].transpose())\n",
    "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
    "    return subplot+1\n",
    "\n",
    "def dataset_to_numpy_util(dataset, N):\n",
    "    dataset = dataset.unbatch().batch(N)\n",
    "    for images, labels in dataset:\n",
    "        numpy_images = images.numpy()\n",
    "        numpy_labels = labels.numpy()\n",
    "        break;  \n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "def display_9_images_from_dataset(dataset):\n",
    "    subplot=331\n",
    "    plt.figure(figsize=(13,13))\n",
    "    images, labels = dataset_to_numpy_util(dataset, 9)\n",
    "    for i, image in enumerate(images):\n",
    "        title = labels[i]\n",
    "        subplot = display_one_flower(image, f'{title}', subplot)\n",
    "        if i >= 8:\n",
    "\n",
    "            break;\n",
    "              \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9ac8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1633333079162,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "r0_q-InvvZia",
    "outputId": "690430f9-4cfc-4a79-b1d2-234c5d688eef"
   },
   "outputs": [],
   "source": [
    "fold_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b344a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "executionInfo": {
     "elapsed": 30223,
     "status": "ok",
     "timestamp": 1633333109379,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "hF2EMdm27GrY",
    "outputId": "5b0477b0-bb1b-4d1c-9925-3c0b8fa1c322"
   },
   "outputs": [],
   "source": [
    "# plot CQT images\n",
    "ds = get_dataset(fold_files[0], labeled=True, return_image_ids=False, repeat=False, shuffle=True, batch_size=BATCH_SIZE * 2, aug=True)\n",
    "display_9_images_from_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fbce31",
   "metadata": {
    "id": "7df4c16c"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836e026",
   "metadata": {
    "id": "0d3c5afe"
   },
   "outputs": [],
   "source": [
    "oof_pred = []\n",
    "oof_target = []\n",
    "oof_ids = []\n",
    "\n",
    "files_train_all = np.array(fold_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5e007",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dae2e038",
    "outputId": "20993f91-1b6c-41ba-bede-d8c57e0e4050"
   },
   "outputs": [],
   "source": [
    "for fold in FOLDS:\n",
    "    all_fold = range(NUM_FOLDS)\n",
    "    files_train = list(files_train_all[(np.delete(all_fold, fold))].reshape(-1)) \n",
    "#    pseudo_files\n",
    "    files_valid = files_train_all[fold]\n",
    "\n",
    "    print(\"=\" * 120)\n",
    "    print(f\"Fold {fold}\")\n",
    "    print(\"=\" * 120)\n",
    "\n",
    "    train_image_count = count_data_items(files_train + pseudo_files) # check\n",
    "    valid_image_count = count_data_items(files_valid)\n",
    "    \n",
    "    print ('train files:', train_image_count, 'valid files:', valid_image_count)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    strategy, tpu_detected = auto_select_accelerator()\n",
    "    with strategy.scope():\n",
    "        model = build_model(\n",
    "            size=IMAGE_SIZE, \n",
    "            efficientnet_size=EFFICIENTNET_SIZE,\n",
    "            weights=WEIGHTS, \n",
    "            count=train_image_count // BATCH_SIZE // REPLICAS // 4)\n",
    "    \n",
    "    model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(SAVEDIR / f\"fold{fold}.h5\"), monitor=\"val_auc\", verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode=\"max\", save_freq=\"epoch\"\n",
    "    )\n",
    "\n",
    "    ds_train = get_dataset(files_train, batch_size=BATCH_SIZE, shuffle=True, repeat=True, aug=True)\n",
    "    ds_pseudo = get_soft_dataset(pseudo_files, batch_size=BATCH_SIZE, shuffle=True, repeat=True, aug=True)\n",
    "\n",
    "    w = [(train_image_count - valid_image_count) / train_image_count, valid_image_count / train_image_count ]\n",
    "\n",
    "    if len(pseudo_files) > 0:\n",
    "      print ('using pseudo labeling', w)  \n",
    "      ds_train = tf.data.experimental.sample_from_datasets([ds_train, ds_pseudo], w)\n",
    "\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[model_ckpt, get_lr_callback(BATCH_SIZE, REPLICAS)],\n",
    "        steps_per_epoch=train_image_count // BATCH_SIZE // REPLICAS // 4,\n",
    "        validation_data=get_dataset(files_valid, batch_size=BATCH_SIZE * 4, repeat=False, shuffle=False, aug=False),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Loading best model...\")\n",
    "    model.load_weights(str(SAVEDIR / f\"fold{fold}.h5\"))\n",
    "\n",
    "    ds_valid = get_dataset(files_valid, labeled=False, return_image_ids=False, repeat=True, shuffle=False, batch_size=BATCH_SIZE * 2, aug=False)\n",
    "    STEPS = valid_image_count / BATCH_SIZE / 2 / REPLICAS\n",
    "    \n",
    "    pred = model.predict(ds_valid, steps=STEPS, verbose=0)[:valid_image_count]\n",
    "    print (pred.shape)\n",
    "    oof_pred.append(np.mean(pred.reshape((valid_image_count, 1), order=\"F\"), axis=1))\n",
    "         \n",
    "    ds_valid = get_dataset(files_valid, batch_size=BATCH_SIZE * 2, repeat=False, labeled=True, return_image_ids=True, aug=False, shuffle=False)\n",
    "    oof_t = np.array([target.numpy() for _, target in iter(ds_valid.unbatch())])\n",
    "    oof_target.append(oof_t)\n",
    "\n",
    "    ds_valid = get_dataset(files_valid, batch_size=BATCH_SIZE * 2, repeat=False, shuffle=False, aug=False, labeled=False, return_image_ids=True)\n",
    "    file_ids = np.array([target.numpy() for _, target in iter(ds_valid.unbatch())])\n",
    "    oof_ids.append(file_ids)\n",
    "\n",
    "    print (pred.shape, oof_t.shape, file_ids.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.distplot(oof_pred[-1])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(\n",
    "        np.arange(len(history.history[\"auc\"])),\n",
    "        history.history[\"auc\"],\n",
    "        \"-o\",\n",
    "        label=\"Train auc\",\n",
    "        color=\"#ff7f0e\")\n",
    "    plt.plot(\n",
    "        np.arange(len(history.history[\"auc\"])),\n",
    "        history.history[\"val_auc\"],\n",
    "        \"-o\",\n",
    "        label=\"Val auc\",\n",
    "        color=\"#1f77b4\")\n",
    "    \n",
    "    x = np.argmax(history.history[\"val_auc\"])\n",
    "    y = np.max(history.history[\"val_auc\"])\n",
    "\n",
    "    xdist = plt.xlim()[1] - plt.xlim()[0]\n",
    "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "\n",
    "    plt.scatter(x, y, s=200, color=\"#1f77b4\")\n",
    "    plt.text(x - 0.03 * xdist, y - 0.13 * ydist, f\"max auc\\n{y}\", size=14)\n",
    "\n",
    "    plt.ylabel(\"auc\", size=14)\n",
    "    plt.xlabel(\"Epoch\", size=14)\n",
    "    plt.legend(loc=2)\n",
    "\n",
    "    plt2 = plt.gca().twinx()\n",
    "    plt2.plot(\n",
    "        np.arange(len(history.history[\"auc\"])),\n",
    "        history.history[\"loss\"],\n",
    "        \"-o\",\n",
    "        label=\"Train Loss\",\n",
    "        color=\"#2ca02c\")\n",
    "    plt2.plot(\n",
    "        np.arange(len(history.history[\"auc\"])),\n",
    "        history.history[\"val_loss\"],\n",
    "        \"-o\",\n",
    "        label=\"Val Loss\",\n",
    "        color=\"#d62728\")\n",
    "    \n",
    "    x = np.argmin(history.history[\"val_loss\"])\n",
    "    y = np.min(history.history[\"val_loss\"])\n",
    "    \n",
    "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "\n",
    "    plt.scatter(x, y, s=200, color=\"#d62728\")\n",
    "    plt.text(x - 0.03 * xdist, y + 0.05 * ydist, \"min loss\", size=14)\n",
    "\n",
    "    plt.ylabel(\"Loss\", size=14)\n",
    "    plt.title(f\"Fold {fold + 1} - Image Size {IMAGE_SIZE}, EfficientNetB{EFFICIENTNET_SIZE}\", size=18)\n",
    "\n",
    "    plt.legend(loc=3)\n",
    "    plt.savefig(OOFDIR / f\"fig{fold}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073376e5",
   "metadata": {
    "id": "a596d112"
   },
   "source": [
    "## OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf208f8d",
   "metadata": {
    "id": "c14a8b61"
   },
   "outputs": [],
   "source": [
    "oof = np.concatenate(oof_pred)\n",
    "oof_ids = np.concatenate(oof_ids)\n",
    "true = np.concatenate(oof_target)\n",
    "\n",
    "auc = roc_auc_score(y_true=true, y_score=oof)\n",
    "print(f\"AUC: {auc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66d3ad",
   "metadata": {
    "id": "d9c63a02"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\": [i.decode(\"UTF-8\") for i in oof_ids],\n",
    "    \"y_true\": true.reshape(-1),\n",
    "    \"y_pred\": oof.astype(float)\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f1c29",
   "metadata": {
    "id": "382aa5b8"
   },
   "outputs": [],
   "source": [
    "df.to_csv(OOFDIR / f\"oof.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3f2f5",
   "metadata": {
    "id": "4695252b"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(OOFDIR / f\"oof.csv\")\n",
    "\n",
    "auc = roc_auc_score(y_true=true, y_score=oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10caef3a",
   "metadata": {
    "id": "ZZt62MpMLhhl"
   },
   "outputs": [],
   "source": [
    "print ('oof auc=', auc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_efficientnet-b3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-19T11:19:39.156011",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
