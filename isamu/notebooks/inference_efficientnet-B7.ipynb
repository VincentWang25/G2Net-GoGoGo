{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68950bef",
   "metadata": {
    "id": "220c9988"
   },
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b314d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20392,
     "status": "ok",
     "timestamp": 1633596719188,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "b38ec6cd",
    "outputId": "f18fabc3-ce88-4915-8fba-a63dd4848a80"
   },
   "outputs": [],
   "source": [
    "# if you run this notebook in kaggle notebook or other platform, comment out the following codef\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91bd47",
   "metadata": {
    "id": "7ec05f3f"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13bb3a",
   "metadata": {
    "id": "be458d96"
   },
   "outputs": [],
   "source": [
    "root = '/content/drive/MyDrive/Colab Notebooks/g2net/' # set your root directory in your google drive. if you use Kaggle notebook, set this to '.'\n",
    "OUTPUT_DIR = root+'/test-B7/' # set your current folder to save model weights and outputs files\n",
    "DATA_DIR = root + '/data/' # put whiten profile data here to run whiten process on the fly\n",
    "MODEL_DIR = OUTPUT_DIR + '/models/' # put model weights to estimate here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a995e",
   "metadata": {
    "id": "e3b22e51"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# CQT params\n",
    "\n",
    "#FMIN=22 # 20\n",
    "#FMAX=None\n",
    "#WINDOW_TYPE='nuttall'\n",
    "#BINS=64\n",
    "HOP_LENGTH = 32\n",
    "#SCALE=1\n",
    "NORM=1\n",
    "#OCTAVE=12\n",
    "\n",
    "FMIN=20 # 20\n",
    "FMAX=512\n",
    "BINS=84\n",
    "OCTAVE=24\n",
    "SCALE=0.4\n",
    "WINDOW_TYPE=\"hann\"\n",
    "\n",
    "SMOOTHING=0.00\n",
    "ST=int(4096 / 16 * 7)\n",
    "EN=int(4096 / 16 * 15)\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "FOLDS=[0, 1, 2, 3, 4] \n",
    "\n",
    "LR=1e-4 # 1e-4\n",
    "IMAGE_SIZE = 512 #\n",
    "BATCH_SIZE = 16 # 32\n",
    "EFFICIENTNET_SIZE = 7\n",
    "WEIGHTS =  \"imagenet\" #\"noisy-student\"#\"imagenet\"\n",
    "\n",
    "#NORMALIZE=True\n",
    "\n",
    "MIXED=False # mixed precision does not work with tf models\n",
    "TFHUB_MODEL=None #'https://tfhub.dev/google/imagenet/inception_v3/feature_vector/5' # 0876\n",
    "\n",
    "MIXUP_PROB = 0.0\n",
    "EPOCHS = 20\n",
    "R_ANGLE = 0 / 180 * np.pi\n",
    "S_SHIFT = 0.0\n",
    "T_SHIFT = 0.0\n",
    "LABEL_POSITIVE_SHIFT = 1.0\n",
    "\n",
    "SEED = 2021\n",
    "\n",
    "# https://www.kaggle.com/yamsam/g2net-tf-on-the-fly-cqt-tpu-inference-path\n",
    "FILES =[\n",
    "# 'gs://kds-f56f84a6d403c2466d12eed4d4afaa1fe1464a9723336d38f33ca366',\n",
    "# 'gs://kds-d482711d73bef82b2ca8c1a0bd869a564992cdd7e6997df7b372ce8e'\n",
    "\n",
    "'gs://kds-9555cba0c858cb42c22a7a077204092f9007dff650116a7d4c173091',\n",
    "'gs://kds-02cc6d7b70e4ba9c2e7f31c15fb2dd7455ce6ad21c7afc2e08ada695'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92723845",
   "metadata": {
    "id": "614862b7"
   },
   "outputs": [],
   "source": [
    "!pip install efficientnet tensorflow_addons > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c1588",
   "metadata": {
    "id": "35fa1681"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import efficientnet.tfkeras as efn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "#from kaggle_datasets import KaggleDatasets\n",
    "from scipy.signal import get_window\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2f113",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1633596898799,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "d3fdee8e",
    "outputId": "1156755d-8fb8-4447-b166-e228dd633f20"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598552f",
   "metadata": {
    "id": "af03fa7e"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c208d",
   "metadata": {
    "id": "6106d929"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abbb7a",
   "metadata": {
    "id": "1576d594"
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    TPU_DETECTED = False\n",
    "    try:\n",
    "        if MIXED and TFHUB_MODEL is None:\n",
    "          tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "          tf.config.experimental_connect_to_cluster(tpu)\n",
    "          tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "          strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "          policy = mixed_precision.Policy('mixed_bfloat16')\n",
    "          mixed_precision.set_global_policy(policy)\n",
    "          tf.config.optimizer.set_jit(True)\n",
    "        else:\n",
    "          tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "          tf.config.experimental_connect_to_cluster(tpu)\n",
    "          tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "          strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        print(\"Running on TPU:\", tpu.master())\n",
    "        TPU_DETECTED = True\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "\n",
    "    return strategy, TPU_DETECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e207a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88806,
     "status": "ok",
     "timestamp": 1633596987587,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "7c97205f",
    "outputId": "39d53083-14a2-4dab-c1d7-c84a262bad91"
   },
   "outputs": [],
   "source": [
    "strategy, tpu_detected = auto_select_accelerator()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104273e",
   "metadata": {
    "id": "55e7f4c7"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c0984",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1633596987588,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "01a5f106",
    "outputId": "eb53f1f6-ef69-41c0-cd51-d91b48dfd681"
   },
   "outputs": [],
   "source": [
    "gcs_paths = []\n",
    "for file in FILES:\n",
    "    gcs_paths.append(file)\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda69237",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1633596988150,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "c0928c4f",
    "outputId": "1bbaaf4c-9386-4d75-eeee-631410c3c9da"
   },
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for path in gcs_paths:\n",
    "    all_files.extend(np.sort(np.array(tf.io.gfile.glob(path + f\"/*.tfrecords\")))) # !!!\n",
    "\n",
    "print('test_files: ', len(all_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8273e",
   "metadata": {
    "id": "804a8d17"
   },
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Here's the main contribution of this notebook - Tensorflow version of on-the-fly CQT computation. Note that some of the operations used in CQT computation are not supported by TPU, therefore the implementation is not a TF layer but a function that runs on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a1b5a",
   "metadata": {
    "id": "1d9416ad"
   },
   "outputs": [],
   "source": [
    "def create_cqt_kernels(\n",
    "    q: float,\n",
    "    fs: float,\n",
    "    fmin: float,\n",
    "    n_bins: int = 84,\n",
    "    bins_per_octave: int = 12,\n",
    "    norm: float = 1,\n",
    "    window: str = \"hann\",\n",
    "    fmax: Optional[float] = None,\n",
    "    topbin_check: bool = True\n",
    ") -> Tuple[np.ndarray, int, np.ndarray, float]:\n",
    "    fft_len = 2 ** _nextpow2(np.ceil(q * fs / fmin))\n",
    "    \n",
    "    if (fmax is not None) and (n_bins is None):\n",
    "        n_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "    elif (fmax is None) and (n_bins is not None):\n",
    "        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "    else:\n",
    "        warnings.warn(\"If nmax is given, n_bins will be ignored\", SyntaxWarning)\n",
    "        n_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.float(bins_per_octave))\n",
    "        \n",
    "    if np.max(freqs) > fs / 2 and topbin_check:\n",
    "        raise ValueError(f\"The top bin {np.max(freqs)} Hz has exceeded the Nyquist frequency, \\\n",
    "                           please reduce the `n_bins`\")\n",
    "    \n",
    "    kernel = np.zeros((int(n_bins), int(fft_len)), dtype=np.complex64)\n",
    "    \n",
    "    length = np.ceil(q * fs / freqs)\n",
    "    for k in range(0, int(n_bins)):\n",
    "        freq = freqs[k]\n",
    "        l = np.ceil(q * fs / freq)\n",
    "        \n",
    "        if l % 2 == 1:\n",
    "            start = int(np.ceil(fft_len / 2.0 - l / 2.0)) - 1\n",
    "        else:\n",
    "            start = int(np.ceil(fft_len / 2.0 - l / 2.0))\n",
    "\n",
    "        sig = get_window(window, int(l), fftbins=True) * np.exp(\n",
    "            np.r_[-l // 2:l // 2] * 1j * 2 * np.pi * freq / fs) / l\n",
    "        \n",
    "        if norm:\n",
    "            kernel[k, start:start + int(l)] = sig / np.linalg.norm(sig, norm)\n",
    "        else:\n",
    "            kernel[k, start:start + int(l)] = sig\n",
    "    return kernel, fft_len, length, freqs\n",
    "\n",
    "\n",
    "def _nextpow2(a: float) -> int:\n",
    "    return int(np.ceil(np.log2(a)))\n",
    "\n",
    "\n",
    "def prepare_cqt_kernel(\n",
    "    sr=22050,\n",
    "    hop_length=512,\n",
    "    fmin=32.70,\n",
    "    fmax=None,\n",
    "    n_bins=84,\n",
    "    bins_per_octave=12,\n",
    "    norm=1,\n",
    "    filter_scale=1,\n",
    "    window=\"hann\"\n",
    "):\n",
    "    q = float(filter_scale) / (2 ** (1 / bins_per_octave) - 1)\n",
    "    print(q)\n",
    "    return create_cqt_kernels(q, sr, fmin, n_bins, bins_per_octave, norm, window, fmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b9c28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1633596988153,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "c5a66284",
    "outputId": "ccdd2582-8eeb-448a-b8f3-e7a0a8614ff7"
   },
   "outputs": [],
   "source": [
    "\n",
    "cqt_kernels, KERNEL_WIDTH, lengths, _ = prepare_cqt_kernel(\n",
    "    sr=2048,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    fmin=FMIN,\n",
    "    fmax=FMAX,\n",
    "    n_bins=BINS,\n",
    "    norm=NORM,\n",
    "    window=WINDOW_TYPE,\n",
    "    bins_per_octave=OCTAVE,\n",
    "    filter_scale=SCALE)\n",
    "LENGTHS = tf.constant(lengths, dtype=tf.float32)\n",
    "CQT_KERNELS_REAL = tf.constant(np.swapaxes(cqt_kernels.real[:, np.newaxis, :], 0, 2))\n",
    "CQT_KERNELS_IMAG = tf.constant(np.swapaxes(cqt_kernels.imag[:, np.newaxis, :], 0, 2))\n",
    "PADDING = tf.constant([[0, 0],\n",
    "                        [KERNEL_WIDTH // 2, KERNEL_WIDTH // 2],\n",
    "                        [0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd1b69",
   "metadata": {
    "id": "f456b3bf"
   },
   "outputs": [],
   "source": [
    "ORDER=[0,0,0]\n",
    "def create_cqt_image(wave, hop_length=16):\n",
    "    CQTs = []\n",
    "    for i in ORDER:\n",
    "        x = wave[i][ST:EN]\n",
    "        x = tf.expand_dims(tf.expand_dims(x, 0), 2)\n",
    "        x = tf.pad(x, PADDING, \"REFLECT\")\n",
    "\n",
    "        CQT_real = tf.nn.conv1d(x, CQT_KERNELS_REAL, stride=hop_length, padding=\"VALID\")\n",
    "        CQT_imag = -tf.nn.conv1d(x, CQT_KERNELS_IMAG, stride=hop_length, padding=\"VALID\")\n",
    "        CQT_real *= tf.math.sqrt(LENGTHS)\n",
    "        CQT_imag *= tf.math.sqrt(LENGTHS)\n",
    "\n",
    "        CQT = tf.math.sqrt(tf.pow(CQT_real, 2) + tf.pow(CQT_imag, 2))\n",
    "        CQTs.append(CQT[0])\n",
    "    return tf.stack(CQTs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea103b95",
   "metadata": {
    "id": "ee17f353"
   },
   "outputs": [],
   "source": [
    "def read_labeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_image(example[\"wave\"], IMAGE_SIZE), tf.reshape(tf.cast(example[\"target\"], tf.float32), [1])\n",
    "\n",
    "\n",
    "def read_unlabeled_tfrecord(example, return_image_id):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_image(example[\"wave\"], IMAGE_SIZE), example[\"wave_id\"] if return_image_id else 0\n",
    "\n",
    "\n",
    "def count_data_items(filenames): \n",
    "    # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "def mixup(image, label, probability=0.5, aug_batch=64 * 8):\n",
    "    imgs = []\n",
    "    labs = []\n",
    "    for j in range(aug_batch):\n",
    "        p = tf.cast(tf.random.uniform([], 0, 1) <= probability, tf.float32)\n",
    "        k = tf.cast(tf.random.uniform([], 0, aug_batch), tf.int32)\n",
    "        a = tf.random.uniform([], 0, 1) * p\n",
    "\n",
    "        img1 = image[j]\n",
    "        img2 = image[k]\n",
    "        imgs.append((1 - a) * img1 + a * img2)\n",
    "        lab1 = label[j]\n",
    "        lab2 = label[k]\n",
    "        labs.append((1 - a) * lab1 + a * lab2)\n",
    "    image2 = tf.reshape(tf.stack(imgs), (aug_batch, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    label2 = tf.reshape(tf.stack(labs), (aug_batch,))\n",
    "    return image2, label2\n",
    "\n",
    "\n",
    "def time_shift(img, shift=T_SHIFT):\n",
    "    if shift > 0:\n",
    "        T = IMAGE_SIZE\n",
    "        P = tf.random.uniform([],0,1)\n",
    "        SHIFT = tf.cast(T * P, tf.int32)\n",
    "        return tf.concat([img[-SHIFT:], img[:-SHIFT]], axis=0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def rotate(img, angle=R_ANGLE):\n",
    "    if angle > 0:\n",
    "        P = tf.random.uniform([],0,1)\n",
    "        A = tf.cast(angle * P, tf.float32)\n",
    "        return tfa.image.rotate(img, A)\n",
    "    return img\n",
    "\n",
    "\n",
    "def spector_shift(img, shift=S_SHIFT):\n",
    "    if shift > 0:\n",
    "        T = IMAGE_SIZE\n",
    "        P = tf.random.uniform([],0,1)\n",
    "        SHIFT = tf.cast(T * P, tf.int32)\n",
    "        return tf.concat([img[:, -SHIFT:], img[:, :-SHIFT]], axis=1)\n",
    "    return img\n",
    "\n",
    "def img_aug_f(img):\n",
    "#    img = time_shift(img)\n",
    "#    img = spector_shift(img)\n",
    "    #img = tf.image.random_flip_left_right(img) \n",
    "#    img = tf.image.random_brightness(img, 0.2)\n",
    "#    img = AUGMENTATIONS_TRAIN(image=img)['image']\n",
    "    # img = rotate(img)\n",
    "    #print(img.shape)\n",
    "    img = swap_img(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def swap_img(img):\n",
    "   p = tf.random.uniform([],0,1)\n",
    "   if p < 0.2:\n",
    "     img = tf.stack([img[:,:,1], img[:,:,0], img[:,:,2]],axis=2)\n",
    "     return  img\n",
    "   else:\n",
    "     return img\n",
    "\n",
    "\n",
    "def imgs_aug_f(imgs, batch_size):\n",
    "    _imgs = []\n",
    "    DIM = IMAGE_SIZE\n",
    "    for j in range(batch_size):\n",
    "        _imgs.append(img_aug_f(imgs[j]))\n",
    "\n",
    "    return tf.reshape(tf.stack(_imgs),(batch_size,DIM,DIM,3))\n",
    "\n",
    "\n",
    "def label_positive_shift(labels):\n",
    "    return labels * LABEL_POSITIVE_SHIFT\n",
    "\n",
    "\n",
    "def aug_f(imgs, labels, batch_size):\n",
    "    #imgs, label = mixup(imgs, labels, MIXUP_PROB, batch_size)\n",
    "    imgs = imgs_aug_f(imgs, batch_size) \n",
    "    return imgs, labels\n",
    "\n",
    "# used for whitening\n",
    "window = tf.cast(np.load(DATA_DIR+'window.npy'), tf.float64)\n",
    "arv_w = tf.cast(np.load(DATA_DIR+'avr_w.npy'), tf.complex64)\n",
    "\n",
    "def whiten(c):\n",
    "  #print (c.shape)\n",
    "  c2 = tf.concat([tf.reverse(-c, axis=[1])[:,4096-2049:-1] + 2 *c[:,:1], c, tf.reverse(-c, axis=[1])[:,1:2049] + 2*c[:,-2:-1]],axis=1)\n",
    "  #print (c2.shape)\n",
    "  c3 = tf.math.real(tf.signal.ifft(tf.signal.fft(tf.cast(1e20*c2*window, tf.complex64))/arv_w))[:,2048:-2048]\n",
    "  #print (c3.shape)\n",
    "  return c3\n",
    "\n",
    "\n",
    "def prepare_image(wave, dim=256):\n",
    "    wave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n",
    "    #wave = tf.cast(wave, tf.float32)\n",
    "    wave = whiten(wave)\n",
    "    # normalized_waves = []\n",
    "    # for i in range(3):\n",
    "    #     normalized_wave = wave[i] - means[i]\n",
    "    #     normalized_wave = normalized_wave / stds[i]\n",
    "\n",
    "    #     normalized_waves.append(normalized_wave)\n",
    "    # wave = tf.stack(normalized_waves)\n",
    "    wave = tf.cast(wave, tf.float32)\n",
    "    image = create_cqt_image(wave, HOP_LENGTH)\n",
    "    #image = tf.keras.layers.Normalization()(image)\n",
    "    image = tf.image.resize(image, size=(dim, dim))\n",
    "    return tf.reshape(image, (dim, dim, 3))\n",
    "\n",
    "\n",
    "def get_dataset(files, batch_size=16, repeat=False, shuffle=False, aug=True, labeled=True, return_image_ids=True):\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n",
    "    ds = ds.cache()\n",
    "\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1024 * 2)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "\n",
    "    if labeled:\n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), num_parallel_calls=AUTO)\n",
    "\n",
    "    ds = ds.batch(batch_size * REPLICAS)\n",
    "    if aug:\n",
    "        ds = ds.map(lambda x, y: aug_f(x, y, batch_size * REPLICAS), num_parallel_calls=AUTO)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff79ba8",
   "metadata": {
    "id": "095d4682"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1743591",
   "metadata": {
    "id": "8f61151e"
   },
   "outputs": [],
   "source": [
    "def build_model(size=256, efficientnet_size=0, weights=\"imagenet\", count=0):\n",
    "    inputs = tf.keras.layers.Input(shape=(size, size, 3))\n",
    "    \n",
    "    if TFHUB_MODEL:\n",
    "      print ('using tf hubmodels')\n",
    "      load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "      loaded_model = hub.load(TFHUB_MODEL, options=load_options)\n",
    "      efn_layer = hub.KerasLayer(loaded_model, trainable=True) \n",
    "\n",
    "      x = efn_layer(inputs)\n",
    "    else:\n",
    "      efn_string= f\"EfficientNetB{efficientnet_size}\"\n",
    "      efn_layer = getattr(efn, efn_string)(input_shape=(size, size, 3), weights=weights, include_top=False)\n",
    "      x = efn_layer(inputs)\n",
    "      x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(1e-3, count)\n",
    "    opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate=LR)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=SMOOTHING)\n",
    "    #loss = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=[\"AUC\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6a569",
   "metadata": {
    "id": "49463f7a"
   },
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8, replicas=8):\n",
    "    lr_start   = 1e-4\n",
    "    lr_max     = 0.000015 * replicas * batch_size\n",
    "    lr_min     = 1e-7\n",
    "    lr_ramp_ep = 3\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.7\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d9b8d",
   "metadata": {
    "id": "c89ffc97"
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "def display_one_flower(image, title, subplot, red=False):\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis('off')\n",
    "    # for i in range(3):\n",
    "    #   image[i,:] -= image[i,:].min()\n",
    "    #   image[i,:] /= image[i,:].max()\n",
    "#    print (image.shape)\n",
    "    plt.imshow(image[:,:,0].transpose())\n",
    "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
    "    return subplot+1\n",
    "\n",
    "def dataset_to_numpy_util(dataset, N):\n",
    "    dataset = dataset.unbatch().batch(N)\n",
    "    for images, labels in dataset:\n",
    "        numpy_images = images.numpy()\n",
    "        numpy_labels = labels.numpy()\n",
    "        break;  \n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "def display_9_images_from_dataset(dataset):\n",
    "    subplot=331\n",
    "    plt.figure(figsize=(13,13))\n",
    "    images, labels = dataset_to_numpy_util(dataset, 9)\n",
    "    for i, image in enumerate(images):\n",
    "        title = labels[i]\n",
    "        subplot = display_one_flower(image, f'{title}', subplot)\n",
    "        if i >= 8:\n",
    "\n",
    "            break;\n",
    "              \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0f32e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "executionInfo": {
     "elapsed": 4369,
     "status": "ok",
     "timestamp": 1633596992717,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "a8b14217",
    "outputId": "76d16af9-3f2f-442b-d2dc-75d8dc9f5af5"
   },
   "outputs": [],
   "source": [
    "ds = get_dataset(all_files[0], labeled=False, return_image_ids=False, repeat=False, shuffle=True, batch_size=BATCH_SIZE * 2, aug=False)\n",
    "display_9_images_from_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ed777",
   "metadata": {
    "id": "b59bb832"
   },
   "outputs": [],
   "source": [
    "#display_9_images_from_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf947ca",
   "metadata": {
    "id": "57e1c48f"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd6918",
   "metadata": {
    "id": "76b33143"
   },
   "outputs": [],
   "source": [
    "files_test_all = np.array(all_files)\n",
    "all_test_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb284bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633596992721,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "c7e784ea",
    "outputId": "7ad67207-9d51-48a9-add6-bf3bc528017d"
   },
   "outputs": [],
   "source": [
    "# def count_data_items(filenames): \n",
    "#     # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "#     n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "#     return np.sum(n)\n",
    "\n",
    "# count_data_items_test = count_data_items\n",
    "\n",
    "# for arais dataset\n",
    "# def count_data_items(fileids):\n",
    "#     return len(fileids) * 5600 # 28000\n",
    "\n",
    "def count_data_items_test(fileids):\n",
    "    return len(fileids) * 22600\n",
    "\n",
    "print (count_data_items_test(files_test_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804290e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77929,
     "status": "ok",
     "timestamp": 1633597070640,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "e5643479",
    "outputId": "77fdf565-1449-467a-978b-ec3bbe8e711e"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = build_model(\n",
    "        size=IMAGE_SIZE,\n",
    "        efficientnet_size=EFFICIENTNET_SIZE,\n",
    "        weights=WEIGHTS,\n",
    "        count=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73368d4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2874269,
     "status": "ok",
     "timestamp": 1633599944901,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "92e51c6b",
    "outputId": "738062cb-fbc4-4d98-c1ff-f50aefbb75b1"
   },
   "outputs": [],
   "source": [
    "ORDER=[0,1,2]\n",
    "\n",
    "for i in FOLDS:\n",
    "    print(f\"Load weight for Fold {i + 1} model\")\n",
    "    model.load_weights(MODEL_DIR + f\"fold{i}.h5\")\n",
    "    \n",
    "    ds_test = get_dataset(files_test_all, batch_size=BATCH_SIZE * 2, repeat=True, shuffle=False, aug=False, labeled=False, return_image_ids=False)\n",
    "    STEPS = count_data_items_test(files_test_all) / BATCH_SIZE / 2 / REPLICAS\n",
    "    pred = model.predict(ds_test, verbose=1, steps=STEPS)[:count_data_items_test(files_test_all)]\n",
    "\n",
    "    all_test_preds.append(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69407c77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2220972,
     "status": "ok",
     "timestamp": 1633602713400,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "a736ae53",
    "outputId": "c54ba2c8-4eb5-44f8-bb5a-d1b33a2e6e8a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ORDER=[1,0,2]\n",
    "\n",
    "for i in FOLDS:\n",
    "    print(f\"Load weight for Fold {i + 1} model\")\n",
    "    model.load_weights(MODEL_DIR + f\"fold{i}.h5\")\n",
    "    \n",
    "    ds_test = get_dataset(files_test_all, batch_size=BATCH_SIZE * 2, repeat=True, shuffle=False, aug=False, labeled=False, return_image_ids=False)\n",
    "    STEPS = count_data_items_test(files_test_all) / BATCH_SIZE / 2 / REPLICAS\n",
    "    pred = model.predict(ds_test, verbose=1, steps=STEPS)[:count_data_items_test(files_test_all)]\n",
    "\n",
    "    all_test_preds.append(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2e78e",
   "metadata": {
    "executionInfo": {
     "elapsed": 393257,
     "status": "ok",
     "timestamp": 1633603106654,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "63d981c2"
   },
   "outputs": [],
   "source": [
    "ds_test = get_dataset(files_test_all, batch_size=BATCH_SIZE * 2, repeat=False, shuffle=False, aug=False, labeled=False, return_image_ids=True)\n",
    "file_ids = np.array([target.numpy() for img, target in iter(ds_test.unbatch())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4e3c7",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "71bf9f0b"
   },
   "outputs": [],
   "source": [
    "test_pred = np.zeros_like(all_test_preds[0])\n",
    "for i in range(len(all_test_preds)):\n",
    "    test_pred += all_test_preds[i] / len(all_test_preds)\n",
    "    \n",
    "test_df = pd.DataFrame({\n",
    "    \"id\": [i.decode(\"UTF-8\") for i in file_ids],\n",
    "    \"target\": test_pred.astype(float)\n",
    "})\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61009335",
   "metadata": {
    "executionInfo": {
     "elapsed": 1147,
     "status": "ok",
     "timestamp": 1633603107803,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "59f3d887"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv(OUTPUT_DIR + \"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8794808",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1186,
     "status": "ok",
     "timestamp": 1633603108988,
     "user": {
      "displayName": "136 yamashitan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17856296841449186565"
     },
     "user_tz": -540
    },
    "id": "61596893",
    "outputId": "03864a9f-c0d9-427f-c1aa-4ad9abcd3724"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(test_df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff2818",
   "metadata": {
    "id": "cfcefcc8"
   },
   "outputs": [],
   "source": [
    "for i in range(len(all_test_preds)):\n",
    "  test_df[f'pred{i}'] = all_test_preds[i].astype(float)\n",
    "\n",
    "test_df.to_csv(OUTPUT_DIR + \"all_pred_tta_float.csv\", index=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b66254",
   "metadata": {
    "id": "759c5997"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eaba40",
   "metadata": {
    "id": "5b92618e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "inference_B7.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-19T11:19:39.156011",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
