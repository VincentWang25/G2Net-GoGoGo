{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd22902b-841f-41c9-a66f-77395174ae4c",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bb6b65-2c6f-453d-8b6f-c6bf386f446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # !pip install -q nnAudio\n",
    "# !pip install -q --upgrade wandb\n",
    "# !pip install -q grad-cam\n",
    "# # !pip install -q ttach\n",
    "# # !pip install efficientnet_pytorch\n",
    "# # !pip install albumentations\n",
    "# !pip install line_profiler\n",
    "# !pip install transformers\n",
    "# !pip install audiomentations\n",
    "# !pip3 install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2025db-3b0f-43ab-a216-e9058852e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ipykernel<6\"\n",
    "# !pip install \"jupyterlab<3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171b030f-1a09-489c-8b3a-c574d306ade7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import collections\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "import sys\n",
    "import json\n",
    "import wandb\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "from glob import glob\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True) \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import IPython.display\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as torch_functional\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts,\n",
    "                    CosineAnnealingLR, ReduceLROnPlateau,_LRScheduler,CyclicLR)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import audiomentations as A\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, PolarityInversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd075088-f395-437c-a176-da7bf90a4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3e2de-86a7-4a6f-ad50-ef450f241250",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c74b277a-2cf0-4edc-8d36-75d33f89d081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G2Net-Model/133rd_V2SD_PL_4ep_2em3lr_32ch_vf_sc01_drop05/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "\n",
    "    #frequently changed \n",
    "    model_name = 'TCNN'\n",
    "    model_version = \"133rd_V2SD_PL_4ep_2em3lr_32ch_vf_sc01_drop05\" \n",
    "    model_module = 'V2StochasticDepth'#V2StochasticDepth,ModelIafossV2,V2SDCBAM,BoTCBAMV2SD,\n",
    "    use_pretrain = False\n",
    "    use_pseudo_label = True\n",
    "    up_thresh = 0.70\n",
    "    down_thresh = 0.15\n",
    "\n",
    "    debug = False\n",
    "    use_checkpoint = False\n",
    "    use_lr_finder = True\n",
    "    use_subset = False \n",
    "    subset_frac = 0.4\n",
    "\n",
    "    #preproc related\n",
    "    #augmentation\n",
    "    #proba for conservative, weight for aggressive\n",
    "    \n",
    "    #conservative\n",
    "    conservative_aug = []#'vflip','add_gaussian_noise',\n",
    "    #aggressive, OneOf \n",
    "    aggressive_aug_proba = 2.0/3.0\n",
    "    aggressive_aug = ['vflip','shuffle01']     #'reduce_SNR'\n",
    "    \n",
    "    \n",
    "    vflip = True\n",
    "    vflip_proba = 0.5\n",
    "    vflip_weight = 1.0 \n",
    "    add_gaussian_noise = False \n",
    "    add_gaussian_noise_proba = 0.5 \n",
    "    add_gaussian_noise_weight = 1.0    \n",
    "    timemask = False\n",
    "    timemask_proba = 0.35\n",
    "    timemask_weight = 0.8\n",
    "    shuffle01 = True\n",
    "    shuffle01_proba = 0.35\n",
    "    shuffle01_weight = 0.8\n",
    "    time_shift = False\n",
    "    time_shift_left = 96\n",
    "    time_shift_right = 96\n",
    "    time_shift_proba = 0.35\n",
    "    time_shift_weight = 0.4\n",
    "    \n",
    "    shift_channel = False\n",
    "    shift_channel_left = 16\n",
    "    shift_channel_right = 16\n",
    "    shift_channel_proba = 0.5\n",
    "    shift_channel_weight = 1.0\n",
    "    shift_two_channels = False #tba\n",
    "    shift_two_channels_proba = 0.5\n",
    "    shift_two_channels_weight= 1.0\n",
    "    reduce_SNR = False\n",
    "    reduce_SNR_ratio = 0.9998\n",
    "    reduce_SNR_proba = 0.5\n",
    "    reduce_SNR_weight = 1.0\n",
    "\n",
    "    time_stretch = False\n",
    "    divide_std = False \n",
    "    shuffle_channels = False    \n",
    "    pitch_shift = False\n",
    "    use_mixup = False\n",
    "    mixup_alpha = 0.1\n",
    "    cropping = False\n",
    "    \n",
    "    #logistic\n",
    "    seed = 48\n",
    "    target_size = 1\n",
    "    target_col = 'target'\n",
    "    n_fold = 5\n",
    "#     gdrive = './drive/MyDrive/Kaggle/G2Net/input/'\n",
    "    kaggle_json_path = 'kaggle/kaggle.json'\n",
    "    output_dir = \"G2Net-Model/\"\n",
    "    pseudo_label_folder = \"G2Net-Model/120th_V2_PL_6ep_1em3lr_32ch_vf_s01/\"#main_35th_GeM_vflip_shuffle01_5fold,#main_112th_V2SD_PL_6ep_5Fold\n",
    "\n",
    "    #logger\n",
    "    print_num_steps=350\n",
    "    \n",
    "    #training related\n",
    "    train_folds = [4]\n",
    "    epochs = 4\n",
    "    batch_size = 256\n",
    "    \n",
    "    lr=  2e-3 #2e-3#8e-3#1e-2#5e-3, 1e-2 # Optimizer  1e-2 channel8, 5e-3 or 2e-3 channel32, 7e-3 channel 16\n",
    "    weight_decay=0 #1e-4  # Optimizer, default value 0.01\n",
    "    gradient_accumulation_steps=1 # Optimizer\n",
    "    scheduler='cosineWithWarmUp' # warm up ratio 0.1 of total steps \n",
    "     \n",
    "    #speedup\n",
    "    num_workers=7\n",
    "    non_blocking=False\n",
    "    amp=True\n",
    "    use_cudnn = True \n",
    "    use_tpu = False\n",
    "    use_ram = False\n",
    "    continuous_exp = False\n",
    "    \n",
    "    #CNN structure\n",
    "    channels = 32\n",
    "    reduction = 1.0\n",
    "    stochastic_final_layer_proba = 0.50\n",
    "    CBAM_SG_kernel_size = 15\n",
    "\n",
    "# no need to change below\n",
    "Config.model_output_folder = Config.output_dir + Config.model_version + \"/\"\n",
    "if not os.path.exists(Config.output_dir):\n",
    "    os.mkdir(Config.output_dir)\n",
    "if not os.path.exists(Config.model_output_folder):\n",
    "    os.mkdir(Config.model_output_folder)\n",
    "\n",
    "torch.backends.cudnn.benchmark = Config.use_cudnn \n",
    "display(Config.model_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2fd5ddd-01f0-4e0c-a483-fe055e23bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run once for Fold 0, save it in RAM and then do experiments multiple times       \n",
    "# if Config.continuous_exp and Config.train_folds == [0]:\n",
    "#     start_time =time.time()  \n",
    "#     if Config.use_pseudo_label:\n",
    "#         with open('fold_0_data_PL.npy', 'rb') as f:\n",
    "#             fold_0_data_PL = np.load(f)\n",
    "#     else:\n",
    "#         with open('fold_0_data.npy', 'rb') as f:\n",
    "#             fold_0_data = np.load(f)\n",
    "#     print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d7181-da0e-4f49-b7dc-d1af56fa584c",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b1bd21-a513-4b67-9181-01e1468b1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "save_object(class2dict(Config), Config.model_output_folder + \"Config.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f52f10-a321-463e-be55-6f1267407999",
   "metadata": {},
   "source": [
    "# Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba86f1b-6607-4b9d-a251-f79d9cb7b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_2_path(file_id: str, train=True) -> str:\n",
    "    if train:\n",
    "        return \"./output/whiten-train-w0/{}.npy\".format(file_id)\n",
    "    else:\n",
    "        return \"./output/whiten-test-w0/{}.npy\".format(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93a20a41-a049-4ce9-aaf7-61856e1d1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training_labels.csv')\n",
    "test_df = pd.read_csv('sample_submission.csv')\n",
    "if Config.debug:\n",
    "    Config.epochs = 1\n",
    "    train_df = train_df.sample(n=50000, random_state=Config.seed).reset_index(drop=True)\n",
    "if Config.use_subset:\n",
    "    train_df = train_df.sample(frac=Config.subset_frac, random_state=Config.seed).reset_index(drop=True)\n",
    "train_df['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "test_df['file_path'] = test_df['id'].apply(lambda x :id_2_path(x,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcdac68-2ef7-46b7-91b4-bd52e2585fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking magnitude of waves\n",
    "num_files = 500\n",
    "input_file_paths = train_df['file_path'].values[:num_files]\n",
    "batch_waves=np.zeros((num_files,3,4096))\n",
    "for i,input_file_path in enumerate(input_file_paths[:num_files]):\n",
    "    file_name = input_file_path.split('/')[-1].split('.npy')[0]\n",
    "    waves = np.load(input_file_path)#.astype(np.float32) # (3, 4096)\n",
    "#     batch_waves[i,:] = np.array([waves.max(axis=1),np.abs(waves).max(axis=1),np.abs(waves).min(axis=1)])\n",
    "    whitened_waves = waves#whiten(waves)\n",
    "    print('std',whitened_waves[1].std())\n",
    "#     print(whitened_waves[2][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53554b42-bef6-4c2f-92bb-ae6c28715cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(whitened_waves[0]).plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e6d52c-c6fe-4ef7-bae2-e5dbfcffed6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold   \n",
       "0     0    0.500125\n",
       "      1    0.499875\n",
       "1     0    0.500125\n",
       "      1    0.499875\n",
       "2     0    0.500125\n",
       "      1    0.499875\n",
       "3     0    0.500125\n",
       "      1    0.499875\n",
       "4     0    0.500125\n",
       "      1    0.499875\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!\n",
    "skf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "splits = skf.split(train_df, train_df[\"target\"])\n",
    "train_df['fold'] = -1\n",
    "for fold, (train_index, valid_index) in enumerate(splits):\n",
    "    train_df.loc[valid_index,\"fold\"] = fold\n",
    "# train_df['fold_PL'] = train_df['fold']\n",
    "\n",
    "train_df.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7621283-4686-41fc-a66f-c99b7bfa6143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>file_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000e74ad</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/00000e74ad.npy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f4945</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/00001f4945.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000661522</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/0000661522.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00007a006a</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/00007a006a.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000a38978</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/0000a38978.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559995</th>\n",
       "      <td>ffff9a5645</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/ffff9a5645.npy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559996</th>\n",
       "      <td>ffffab0c27</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/ffffab0c27.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559997</th>\n",
       "      <td>ffffcf161a</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/ffffcf161a.npy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559998</th>\n",
       "      <td>ffffd2c403</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/ffffd2c403.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559999</th>\n",
       "      <td>fffff2180b</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/fffff2180b.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  target                                file_path  fold\n",
       "0       00000e74ad       1  ./output/whiten-train-w0/00000e74ad.npy     3\n",
       "1       00001f4945       0  ./output/whiten-train-w0/00001f4945.npy     0\n",
       "2       0000661522       0  ./output/whiten-train-w0/0000661522.npy     4\n",
       "3       00007a006a       0  ./output/whiten-train-w0/00007a006a.npy     0\n",
       "4       0000a38978       1  ./output/whiten-train-w0/0000a38978.npy     4\n",
       "...            ...     ...                                      ...   ...\n",
       "559995  ffff9a5645       1  ./output/whiten-train-w0/ffff9a5645.npy     3\n",
       "559996  ffffab0c27       0  ./output/whiten-train-w0/ffffab0c27.npy     1\n",
       "559997  ffffcf161a       1  ./output/whiten-train-w0/ffffcf161a.npy     2\n",
       "559998  ffffd2c403       0  ./output/whiten-train-w0/ffffd2c403.npy     1\n",
       "559999  fffff2180b       0  ./output/whiten-train-w0/fffff2180b.npy     4\n",
       "\n",
       "[560000 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1750c4-a631-4586-9f0a-400fbbae54a6",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b11e6-fe2c-4dcb-80c5-6f6e5acf3903",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "446a1dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservative transforms:  []\n",
      "aggressive transforms:  ['vflip', 'shuffle01']\n"
     ]
    }
   ],
   "source": [
    "conserv_transform_list = []\n",
    "aggressive_transform_list = []\n",
    "conserv_transform_list_strings = []\n",
    "aggressive_transform_list_strings = []\n",
    "\n",
    "#-------------------------vflip\n",
    "if Config.vflip:\n",
    "#     trans = lambda x:-x\n",
    "    def vflip_func(x,sample_rate=2048):\n",
    "        return -x\n",
    "    def vflip_func_random(x,sample_rate=2048):\n",
    "        if np.random.random()<Config.vflip_proba:\n",
    "            return -x\n",
    "        else:\n",
    "            return x\n",
    "    if 'vflip' in Config.aggressive_aug:\n",
    "        aggressive_transform_list.append(vflip_func)\n",
    "        aggressive_transform_list_strings.append('vflip')\n",
    "    else:\n",
    "        conserv_transform_list.append(vflip_func_random)\n",
    "        conserv_transform_list_strings.append('vflip')\n",
    "#----------------------add_gaussian_noise        \n",
    "if Config.add_gaussian_noise:\n",
    "    \n",
    "    if 'add_gaussian_noise' in Config.aggressive_aug:\n",
    "        trans = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude=0.015*0.015, p=1) #tbs #0.015 is the estimated std\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('add_gaussian_noise')\n",
    "    else:\n",
    "        trans = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude=0.015*0.015, p=Config.add_gaussian_noise_proba) #tbs #0.015 is the estimated std\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('add_gaussian_noise')\n",
    "\n",
    "#--------------------------timemask\n",
    "if Config.timemask:\n",
    "    \n",
    "    if 'timemask' in Config.aggressive_aug:\n",
    "        trans = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=1)\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('timemask')\n",
    "    else:\n",
    "        trans = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=Config.timemask_proba)\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('timemask')\n",
    "\n",
    "#--------------------------shuffle01        \n",
    "def shuffle01_func(x,sample_rate=2048):\n",
    "    return x[[1,0,2]]\n",
    "def shuffle01_func_random(x,sample_rate=2048):\n",
    "    if np.random.random()<Config.shuffle01_proba: \n",
    "        return x[[1,0,2]]\n",
    "    else:\n",
    "        return x\n",
    "if Config.shuffle01:\n",
    "#     trans = lambda x:x[[1,0,2]]\n",
    "\n",
    "    if 'shuffle01' in Config.aggressive_aug:\n",
    "        aggressive_transform_list.append(shuffle01_func)\n",
    "        aggressive_transform_list_strings.append('shuffle01')\n",
    "    else:\n",
    "        conserv_transform_list.append(shuffle01_func_random)\n",
    "        conserv_transform_list_strings.append('shuffle01')\n",
    "#---------------------------time_shift\n",
    "if Config.time_shift:\n",
    "    if 'time_shift' in Config.aggressive_aug:\n",
    "        trans = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096,\n",
    "                        max_fraction=Config.time_shift_right*1.0/4096, \n",
    "                        p=1,rollover=False)#<0 means shift towards left,  fraction of total sound length\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('time_shift')\n",
    "    else:\n",
    "        trans = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096,\n",
    "                                max_fraction=Config.time_shift_right*1.0/4096, \n",
    "                                p=Config.time_shift_proba,rollover=False)\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('time_shift')\n",
    "\n",
    "#-----------------shift_channel        \n",
    "def shift_channel_func(x,sample_rate=2048):\n",
    "    channel = np.random.choice(3)\n",
    "    trans = A.Shift(min_fraction=-Config.shift_channel_left*1.0/4096,\n",
    "                max_fraction=Config.shift_channel_right*1.0/4096, \n",
    "                p=1,rollover=False)\n",
    "    x[channel] = trans(x[channel],sample_rate=2048)\n",
    "    return x\n",
    "def shift_channel_func_random(x,sample_rate=2048):\n",
    "    channel = np.random.choice(3)\n",
    "    trans = A.Shift(min_fraction=-Config.shift_channel_left*1.0/4096,\n",
    "                max_fraction=Config.shift_channel_right*1.0/4096, \n",
    "                p=Config.shift_channel_proba,rollover=False)\n",
    "    x[channel] = trans(x[channel],sample_rate=2048)\n",
    "    return x\n",
    "if Config.shift_channel:\n",
    "    if 'shift_channel' in Config.aggressive_aug:\n",
    "        \n",
    "        aggressive_transform_list.append(shift_channel_func)\n",
    "        aggressive_transform_list_strings.append('shift_channel')\n",
    "    else:\n",
    "        \n",
    "        conserv_transform_list.append(shift_channel_func_random)\n",
    "        conserv_transform_list_strings.append('shift_channel')\n",
    "#-----------------reduce_SNR        \n",
    "def reduce_SNR_func(x,sample_rate=2048):\n",
    "    x = x * Config.reduce_SNR_ratio\n",
    "    trans = A.AddGaussianNoise(min_amplitude=multiplier, max_amplitude=multiplier, p=1)\n",
    "    x = trans(x,sample_rate=2048)\n",
    "    return x \n",
    "def reduce_SNR_func_random(x,sample_rate=2048):\n",
    "    if np.random.random() < Config.reduce_SNR_proba:\n",
    "        x = x * Config.reduce_SNR_ratio\n",
    "        trans = A.AddGaussianNoise(min_amplitude=multiplier, max_amplitude=multiplier, p=1)\n",
    "        x = trans(x,sample_rate=2048)\n",
    "    return x\n",
    "if Config.reduce_SNR:\n",
    "    multiplier = math.sqrt(1-Config.reduce_SNR_ratio**2)\n",
    "    if 'reduce_SNR' in Config.aggressive_aug:\n",
    "\n",
    "        aggressive_transform_list.append(reduce_SNR_func)\n",
    "        aggressive_transform_list_strings.append('reduce_SNR')\n",
    "    else:\n",
    "\n",
    "        conserv_transform_list.append(reduce_SNR_func_random)\n",
    "        conserv_transform_list_strings.append('reduce_SNR')\n",
    "        \n",
    "# if Config.time_stretch:\n",
    "#     trans = A.TimeStretch(min_rate=0.98, max_rate=1.02,leave_length_unchanged=True, p=0.5)\n",
    "#     if 'time_stretch' in aggressive_aug:\n",
    "#         aggressive_transform_list.append(trans)\n",
    "#         aggressive_transform_list_strings.append('time_stretch')\n",
    "#     else:\n",
    "#         conserv_transform_list.append(trans)\n",
    "#         conserv_transform_list_strings.append('time_stretch')\n",
    "# if Config.pitch_shift:\n",
    "#     trans = A.PitchShift(min_semitones=-1, max_semitones=1, p=0.5)\n",
    "#     if 'pitch_shift' in aggressive_aug:\n",
    "#         aggressive_transform_list.append(trans)\n",
    "#         aggressive_transform_list_strings.append('pitch_shift')\n",
    "#     else:\n",
    "#         conserv_transform_list.append(trans)\n",
    "#         conserv_transform_list_strings.append('pitch_shift')\n",
    "# if Config.shift_channel:\n",
    "#     pass\n",
    "\n",
    "print('conservative transforms: ',conserv_transform_list_strings)\n",
    "print('aggressive transforms: ',aggressive_transform_list_strings)\n",
    "train_transform = conserv_transform_list#A.Compose(conserv_transform_list)#,OneOf(aggressive_transform_list,p=0.5)) # no OneOf in audiomentation\n",
    "# \n",
    "\n",
    "test_transform = None #A.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a66e416-c7ac-4526-b32a-96d6aa50190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [getattr(Config(), f'{agg}_weight') for agg in aggressive_transform_list_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f6f8d25-feda-4576-9080-b2e40228ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRetriever(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "\n",
    "\n",
    "        #reading data for fold 0 for fast iteration\n",
    "        if Config.continuous_exp and Config.train_folds == [0]:\n",
    "            if Config.use_pseudo_label:\n",
    "                self.data = fold_0_data_PL\n",
    "            else:\n",
    "                self.data = fold_0_data\n",
    "        else:\n",
    "            if Config.use_ram:\n",
    "                start_time =time.time()\n",
    "                array_shape = (len(self.paths),3,4096)\n",
    "                self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "                for i,path in enumerate(self.paths):\n",
    "                    waves = np.load(path)\n",
    "                    self.data[i,:] = waves            \n",
    "                print(time.time()-start_time)\n",
    "\n",
    "                \n",
    "            # saving Fold 0 data for later use\n",
    "#         with open('fold_0_data_PL.npy', 'wb') as f:\n",
    "#             np.save(f, self.data)\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if Config.use_ram:\n",
    "            waves = self.data[index]\n",
    "        else:\n",
    "            path = self.paths[index] \n",
    "            waves = np.load(path)\n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015 #causing NaN?\n",
    "\n",
    "#         if Config.shuffle_channels:#nn.ChannelShuffle\n",
    "#             if np.random.random()<0.5:\n",
    "#                 np.random.shuffle(waves)\n",
    "                \n",
    "#         if Config.vflip:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves = -waves\n",
    "            \n",
    "        if self.transforms is not None:\n",
    "            for i,_ in enumerate(self.transforms):\n",
    "                transform = conserv_transform_list[i]\n",
    "                waves= transform(waves,sample_rate=2048)\n",
    "            \n",
    "        if aggressive_transform_list_strings:\n",
    "            if np.random.random()<Config.aggressive_aug_proba:\n",
    "                n = len(aggressive_transform_list_strings)\n",
    "                probas = np.array([getattr(Config(), f'{agg}_weight') for agg in aggressive_transform_list_strings])\n",
    "                probas /= probas.sum()\n",
    "                trans_idx = np.random.choice(n,p=probas)\n",
    "                trans = aggressive_transform_list[trans_idx]\n",
    "                waves = trans(waves,sample_rate=2048)\n",
    "\n",
    "\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverTest(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "        if Config.use_ram:\n",
    "            array_shape = (len(self.paths),3,4096)\n",
    "            self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "            for i,path in enumerate(self.paths):\n",
    "                waves = np.load(path)\n",
    "                self.data[i,:] = waves  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if Config.use_ram:\n",
    "            waves = self.data[index]\n",
    "        else:\n",
    "            path = self.paths[index] \n",
    "            waves = np.load(path)\n",
    "            \n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "            \n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverLRFinder(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms     \n",
    "#         start_time =time.time()\n",
    "#         array_shape = (len(self.paths),3,4096)\n",
    "#         self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "#         for i,path in enumerate(self.paths):\n",
    "#             waves = np.load(path)\n",
    "#             self.data[i,:] = waves\n",
    "#         print(time.time()-start_time)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "        \n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "#         if Config.shuffle_channels:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 np.random.shuffle(waves)\n",
    "#         if Config.shuffle01:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves[[0,1]]=waves[[1,0]]\n",
    "#         if Config.vflip:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves = -waves\n",
    "              \n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b86ab7a-b88e-4a0f-9cb7-3a023ae12408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(aggressive_transform_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6f55460-98fb-47e8-b4b6-3d898ff751da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.choice(5,p=[0.1, 0, 0.3, 0.6, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d3d1abe-3f31-4b01-bf9a-15b300461b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    '''\n",
    "    Code modified from the 2d code in\n",
    "    https://amaarora.github.io/2020/08/30/gempool.html\n",
    "    '''\n",
    "    def __init__(self, kernel_size=8, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        with torch.cuda.amp.autocast(enabled=False):#to avoid NaN issue for fp16\n",
    "            return torch_functional.avg_pool1d(x.clamp(min=eps).pow(p), self.kernel_size).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45273271-348f-453b-a905-01e846d7bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/iafoss/mish-activation\n",
    "import torch.nn.functional as F\n",
    "class MishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        sigmoid = torch.sigmoid(x)\n",
    "        tanh_sp = torch.tanh(F.softplus(x)) \n",
    "        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return MishFunction.apply(x)\n",
    "\n",
    "def to_Mish(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, Mish())\n",
    "        else:\n",
    "            to_Mish(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd2ba5-d403-4abf-b971-2303a92ea8ea",
   "metadata": {},
   "source": [
    "## neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de5c4e68-068e-451e-aa4c-ac1e28ed5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCNN_Dilations(nn.Module):\n",
    "    \"\"\"1D convolutional neural network with dilations. Classifier of the gravitaitonal waves\n",
    "    Inspired by the https://arxiv.org/pdf/1904.08693.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.init_conv = nn.Sequential(nn.Conv1d(3, 256, kernel_size=1), nn.ReLU())\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(256, 256, kernel_size=2, dilation=2 ** i),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for i in range(11)\n",
    "            ]\n",
    "        )\n",
    "        self.out_conv = nn.Sequential(nn.Conv1d(256, 1, kernel_size=1), nn.ReLU())\n",
    "        self.fc = nn.Linear(2049, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = self.fc(x)\n",
    "        x.squeeze_(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model1DCNN(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            nn.MaxPool1d(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Model1DCNNGEM(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            GeM(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            GeM(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            GeM(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x    \n",
    "\n",
    "#--------------------------------------------------------------------------- V0\n",
    "class ExtractorMaxPool(nn.Sequential):\n",
    "    def __init__(self, in_c=8, out_c=8, kernel_size=64, maxpool=8, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_c), act,\n",
    "            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.MaxPool1d(kernel_size=maxpool),\n",
    "        )\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes))\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class ModelIafoss(nn.Module):\n",
    "    def __init__(self, n=8, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(ExtractorMaxPool(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),),\n",
    "            nn.Sequential(ExtractorMaxPool(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),)\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlock(3*n,2*n,kernel_size=31,stride=4),\n",
    "            ResBlock(2*n,2*n,kernel_size=31),\n",
    "            ResBlock(2*n,4*n,kernel_size=15,stride=4),\n",
    "            ResBlock(4*n,4*n,kernel_size=15),\n",
    "            ResBlock(4*n,8*n,kernel_size=7,stride=4),\n",
    "            ResBlock(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Flatten(),\n",
    "            nn.Linear(n*8*8,256),nn.BatchNorm1d(256),nn.Dropout(ps), act,\n",
    "            nn.Linear(256, 256),nn.BatchNorm1d(256),nn.Dropout(ps), act,\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "\n",
    "#----------------------------------------------V1    \n",
    "    \n",
    "class AdaptiveConcatPool1d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`\"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "\n",
    "# using GeM\n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, in_c=8, out_c=8, kernel_size=64, maxpool=8, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_c), act,\n",
    "            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "#             nn.MaxPool1d(kernel_size=maxpool),\n",
    "            GeM(kernel_size=maxpool),\n",
    "        )\n",
    "    \n",
    "class ModelIafossV1(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            ResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            ResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            ResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#for SE-----------------------------------------------------------------------------\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, int(channel // reduction), bias=False),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(int(channel // reduction), channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class SEResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True),reduction=Config.reduction):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            SELayer(out_planes, reduction)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1SE(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            SEResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            SEResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            SEResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            SEResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            SEResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            SEResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[1](x[:,1].unsqueeze(1)),\n",
    "            self.ex[2](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#for CBAM-----------------------------------------------------------------------\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, silu=True):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm1d(out_planes,eps=1e-5, momentum=0.01, affine=True) #0.01,default momentum 0.1\n",
    "        self.silu = nn.SiLU(inplace=True) if silu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.silu is not None:\n",
    "            x = self.silu(x)\n",
    "        return x\n",
    "    \n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self,kernel_size=15):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = kernel_size\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, silu=True)#silu False\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = torch.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "    \n",
    "class CBAMResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True),reduction=Config.reduction):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            SELayer(out_planes, reduction),\n",
    "            SpatialGate(),\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "    \n",
    "class ModelIafossV1CBAM(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),CBAMResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          CBAMResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),CBAMResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          CBAMResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            CBAMResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            CBAMResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            CBAMResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            CBAMResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            CBAMResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            CBAMResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))    \n",
    "\n",
    "#---------------------------------------------------------------------------------------------------  \n",
    "    \n",
    "    \n",
    "class BasicBlockPool(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.MaxPool1d(downsample,ceil_mode=True), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    nn.MaxPool1d(downsample,ceil_mode=True),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                     nn.MaxPool1d(2,ceil_mode=True),  # downsampling \n",
    "#                 )\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "    #             self.shortcut = nn.Sequential(\n",
    "    #                     nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "    #                     nn.BatchNorm1d(out_channels),\n",
    "    #                 )#skip layers in residual_function, can try identity, i.e., nn.Sequential()\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1Pool(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicBlockPool(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            BasicBlockPool(3*n,3*n,kernel_size=31), #128\n",
    "            BasicBlockPool(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            BasicBlockPool(4*n,4*n,kernel_size=15), #32\n",
    "            BasicBlockPool(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            BasicBlockPool(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------  \n",
    "    \n",
    "    \n",
    "class ResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1GeM(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "#-----------------------------------------------------------------------------\n",
    "class ModelIafossV1GeMAll(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#-----------------------------------------------------------------------------    \n",
    "class AdaptiveConcatPool1dx3(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d`,`AdaptiveMaxPool1d` and 'GeM' \"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "        self.gemp = GeM(kernel_size=8)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x),self.gemp(x)], 1)\n",
    "    \n",
    "class ModelGeMx3(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1dx3(),nn.Flatten(),\n",
    "            nn.Linear(n*8*3,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "#-----------------------------------------------------------------------------\n",
    "class ModelIafossV1GeMAllDeep(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "    \n",
    "class StochasticDepthResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=False),p=1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.act = act\n",
    "\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple Pooling\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "            \n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:#attribute inherited\n",
    "            if self.survival():\n",
    "                x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.act(self.shortcut(x))\n",
    "        else:\n",
    "            x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "        return x\n",
    "    \n",
    "   \n",
    "    \n",
    "class DeepStochastic(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 11\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.conv = nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,p=self.survival_proba[0]), #512\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[1]), #128\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[2]), \n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[3]), \n",
    "            StochasticDepthResBlockGeM(3*n,4*n,kernel_size=15,downsample=4,p=self.survival_proba[4]), #128\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[5]), #32\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[6]),\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[7]),\n",
    "            StochasticDepthResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,p=self.survival_proba[8]), #32\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,p=self.survival_proba[9]), #8\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,p=self.survival_proba[10]),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#-----------------------------------------------------------------------------\n",
    "class Deeper(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), #128\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3), #32\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3),\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "class Deeper2(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=2), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=2), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=2), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15,downsample=2),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=2),\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7,downsample=2),\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),#8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#-------------------------------------------------------------------V2    \n",
    "\n",
    "class ModelIafossV2(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,act=act)),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15,act=act),#128\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7,act=act), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "    \n",
    "#-----------------------------------\n",
    "class V2StochasticDepth(nn.Module):#stocnot on ex\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        \n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 10\n",
    "#         self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "#         self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(1,num_block+1)]\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[0]), #512\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[1])),\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[2]), #512\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[3])),\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[4]), #512\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,act=act,p=self.survival_proba[5])),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act,p=self.survival_proba[6]),\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,act=act,p=self.survival_proba[7]),#128\n",
    "            StochasticDepthResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act,p=self.survival_proba[8]), #32\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,act=act,p=self.survival_proba[9]), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "    \n",
    "#for StochasticCBAM-----------------------------------------------------------------------\n",
    "    \n",
    "class StochasticCBAMResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, \n",
    "                 downsample=1, act=nn.SiLU(inplace=False),p=1,reduction=1.0):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.act = act\n",
    "\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                SELayer(out_channels, reduction),\n",
    "                SpatialGate(Config.CBAM_SG_kernel_size),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple Pooling\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                SELayer(out_channels, reduction),\n",
    "                SpatialGate(Config.CBAM_SG_kernel_size),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "            \n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:#attribute inherited\n",
    "            if self.survival():\n",
    "                x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.act(self.shortcut(x))\n",
    "        else:\n",
    "            x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "        return x \n",
    "\n",
    "    \n",
    "class V2SDCBAM(nn.Module):#stocnot on ex\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        \n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 10\n",
    "#         self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "#         self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(1,num_block+1)]\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            StochasticCBAMResBlock(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[0],reduction=Config.reduction), #512\n",
    "            StochasticCBAMResBlock(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[1],reduction=Config.reduction)),\n",
    "            nn.Sequential(\n",
    "            StochasticCBAMResBlock(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[2],reduction=Config.reduction), #512\n",
    "            StochasticCBAMResBlock(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[3],reduction=Config.reduction)),\n",
    "            nn.Sequential(\n",
    "            StochasticCBAMResBlock(3*n,3*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[4],reduction=Config.reduction), #512\n",
    "            StochasticCBAMResBlock(3*n,3*n,kernel_size=31,act=act,p=self.survival_proba[5],reduction=Config.reduction)),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            StochasticCBAMResBlock(6*n,4*n,kernel_size=15,downsample=4,act=act,p=self.survival_proba[6],reduction=Config.reduction),\n",
    "            StochasticCBAMResBlock(4*n,4*n,kernel_size=15,act=act,p=self.survival_proba[7],reduction=Config.reduction),#128\n",
    "            StochasticCBAMResBlock(4*n,8*n,kernel_size=7,downsample=4,act=act,p=self.survival_proba[8],reduction=Config.reduction), #32\n",
    "            StochasticCBAMResBlock(8*n,8*n,kernel_size=7,act=act,p=self.survival_proba[9],reduction=Config.reduction), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "    \n",
    "#BoT---------------------------------------------------------------------------------------------    \n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, n_dims, length, heads=4):\n",
    "        super(MHSA, self).__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "        self.query = nn.Conv1d(n_dims, n_dims, kernel_size=1)\n",
    "        self.key = nn.Conv1d(n_dims, n_dims, kernel_size=1)\n",
    "        self.value = nn.Conv1d(n_dims, n_dims, kernel_size=1)\n",
    "        self.rel_pos = nn.Parameter(torch.randn([1, heads, n_dims // heads, length]), requires_grad=True)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_batch, C, length = x.size()\n",
    "        q = self.query(x).view(n_batch, self.heads, C // self.heads, -1)\n",
    "        k = self.key(x).view(n_batch, self.heads, C // self.heads, -1)\n",
    "        v = self.value(x).view(n_batch, self.heads, C // self.heads, -1)\n",
    "\n",
    "        content_content = torch.matmul(q.permute(0, 1, 3, 2 ), k)\n",
    "\n",
    "        content_position = self.rel_pos.view(1, self.heads, C // self.heads, -1).permute(0, 1, 3, 2)\n",
    "        content_position = torch.matmul(content_position, q)\n",
    "\n",
    "        energy = content_content + content_position\n",
    "        attention = self.softmax(energy)\n",
    "\n",
    "        out = torch.matmul(v, attention.permute(0, 1, 3, 2))\n",
    "        out = out.view(n_batch, C, length)\n",
    "\n",
    "        return out\n",
    "\n",
    "# class BoTCBAMSDResBlockGeM(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=False),p=1,mhsa=False,heads=4,length=None):\n",
    "#         super().__init__()\n",
    "#         self.p = p\n",
    "#         self.act = act\n",
    "        \n",
    "        \n",
    "#         layers = nn.ModuleList()\n",
    "#         layers.append(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False))\n",
    "#         layers.append(nn.BatchNorm1d(out_channels))\n",
    "#         layers.append(act)\n",
    "#         if mhsa:\n",
    "#             layers.append(MHSA(out_channels, length=length, heads=heads))\n",
    "#         else:\n",
    "#             layers.append(nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False))\n",
    "#         layers.append(nn.BatchNorm1d(out_channels))\n",
    "#         layers.append(SELayer(out_channels, reduction))\n",
    "#         layers.append(SpatialGate(Config.CBAM_SG_kernel_size))\n",
    "#         if downsample != 1 or in_channels != out_channels:\n",
    "#             layers.append(GeM(kernel_size=downsample))\n",
    "#         self.residual_function = nn.Sequential(*layers)\n",
    "        \n",
    "#         sc_layers = nn.ModuleList()\n",
    "#         if downsample != 1 or in_channels != out_channels:\n",
    "#             sc_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False))\n",
    "#             sc_layers.append(nn.BatchNorm1d(out_channels))\n",
    "#             sc_layers.append(GeM(kernel_size=downsample))\n",
    "#         self.shortcut = nn.Sequential(*sc_layers)\n",
    "        \n",
    "            \n",
    "#     def survival(self):\n",
    "#         var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "#         return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.training:#attribute inherited\n",
    "#             if self.survival():\n",
    "#                 x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "#             else:\n",
    "#                 x = self.act(self.shortcut(x))\n",
    "#         else:\n",
    "#             x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "#         return x\n",
    "\n",
    "    \n",
    "# class BoTCBAMV2SD(nn.Module):#stocnot on ex\n",
    "#     def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "#         super().__init__()\n",
    "#         self.ex = nn.ModuleList([\n",
    "#             nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "#                           ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "#             nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "#                           ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "#         ])\n",
    "#         self.length = 4096/8#tbs, bad code style\n",
    "#         proba_final_layer = Config.stochastic_final_layer_proba \n",
    "#         num_block = 10\n",
    "# #         self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "# #         self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "#         self.proba_step = (1-proba_final_layer)/(num_block)\n",
    "#         self.survival_proba = [1-i*self.proba_step for i in range(1,num_block+1)]\n",
    "        \n",
    "#         self.conv1 = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#             BoTCBAMSDResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[0],reduction=Config.reduction), #128\n",
    "#             BoTCBAMSDResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[1],reduction=Config.reduction)),\n",
    "#             nn.Sequential(\n",
    "#             BoTCBAMSDResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[2],reduction=Config.reduction), #128\n",
    "#             BoTCBAMSDResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[3],reduction=Config.reduction)),\n",
    "#             nn.Sequential(\n",
    "#             BoTCBAMSDResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[4],reduction=Config.reduction), #128\n",
    "#             BoTCBAMSDResBlockGeM(3*n,3*n,kernel_size=31,act=act,p=self.survival_proba[5],reduction=Config.reduction)),#128\n",
    "#             ])\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             BoTCBAMSDResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act,p=self.survival_proba[6],reduction=Config.reduction,length=self.length/16),#32\n",
    "#             BoTCBAMSDResBlockGeM(4*n,4*n,kernel_size=15,act=act,p=self.survival_proba[7],reduction=Config.reduction,length=self.length/16),#32\n",
    "#             BoTCBAMSDResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act,p=self.survival_proba[8],reduction=Config.reduction,mhsa=True,length=self.length/64), #8 #mhsa for last stage\n",
    "#             BoTCBAMSDResBlockGeM(8*n,8*n,kernel_size=7,act=act,p=self.survival_proba[9],reduction=Config.reduction,mhsa=True,length=self.length/64), #8 #mhsa for last stage\n",
    "#         )\n",
    "#         self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "#             nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "#             nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "#             nn.Linear(nh, 1),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "#               self.ex[1](x[:,2].unsqueeze(1))]\n",
    "#         x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "#               self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "#         x2 = torch.cat(x1,1)\n",
    "#         return self.head(self.conv2(x2))\n",
    "    \n",
    "#-------------------------------\n",
    "class BoTSDResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=False),p=1,mhsa=False,heads=4,length=None):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.act = act\n",
    "        \n",
    "        \n",
    "        layers = nn.ModuleList()\n",
    "        layers.append(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False))\n",
    "        layers.append(nn.BatchNorm1d(out_channels))\n",
    "        layers.append(act)\n",
    "        if mhsa:\n",
    "            layers.append(MHSA(out_channels, length=length, heads=heads))\n",
    "        else:\n",
    "            layers.append(nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False))\n",
    "        layers.append(nn.BatchNorm1d(out_channels))\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            layers.append(GeM(kernel_size=downsample))\n",
    "        self.residual_function = nn.Sequential(*layers)\n",
    "        \n",
    "        sc_layers = nn.ModuleList()\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            sc_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False))\n",
    "            sc_layers.append(nn.BatchNorm1d(out_channels))\n",
    "            sc_layers.append(GeM(kernel_size=downsample))\n",
    "        self.shortcut = nn.Sequential(*sc_layers)\n",
    "        \n",
    "            \n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:#attribute inherited\n",
    "            if self.survival():\n",
    "                x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.act(self.shortcut(x))\n",
    "        else:\n",
    "            x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "        return x\n",
    "\n",
    "    \n",
    "class BoTV2SD(nn.Module):#stocnot on ex\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        self.length = 4096//8#tbs, bad code style\n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 10\n",
    "#         self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "#         self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(1,num_block+1)]\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            BoTSDResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[0]), #128\n",
    "            BoTSDResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[1])),\n",
    "            nn.Sequential(\n",
    "            BoTSDResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[2]), #128\n",
    "            BoTSDResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[3])),\n",
    "            nn.Sequential(\n",
    "            BoTSDResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[4]), #128\n",
    "            BoTSDResBlockGeM(3*n,3*n,kernel_size=31,act=act,p=self.survival_proba[5])),#128\n",
    "                ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            BoTSDResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act,p=self.survival_proba[6],length=self.length//4),#128\n",
    "            BoTSDResBlockGeM(4*n,4*n,kernel_size=15,act=act,p=self.survival_proba[7],length=self.length//16),#32\n",
    "            BoTSDResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act,p=self.survival_proba[8],mhsa=True,heads=16,length=self.length//16), #32 #mhsa for last stage\n",
    "            BoTSDResBlockGeM(8*n,8*n,kernel_size=7,act=act,p=self.survival_proba[9],mhsa=True,heads=16,length=self.length//64), #8 #mhsa for last stage\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e1528ce-06af-4d53-84d9-bbbcaf4991b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    model_name = Config.model_module \n",
    "    if model_name == 'Model1DCNN':\n",
    "        model = Model1DCNN(Config.channels)\n",
    "    elif model_name == 'Model1DCNNGEM':\n",
    "        model = Model1DCNNGEM(Config.channels)\n",
    "    elif model_name == 'ModelIafoss':\n",
    "        model = ModelIafoss(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1':\n",
    "        model = ModelIafossV1(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1SE':\n",
    "        model = ModelIafossV1SE(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1CBAM':\n",
    "        model = ModelIafossV1CBAM(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1Pool':\n",
    "        model = ModelIafossV1Pool(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeM':\n",
    "        model = ModelIafossV1GeM(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeMAll':\n",
    "        model = ModelIafossV1GeMAll(Config.channels)\n",
    "    elif model_name == 'ModelGeMx3':\n",
    "        model = ModelGeMx3(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeMAllDeep':\n",
    "        model = ModelIafossV1GeMAllDeep(Config.channels)\n",
    "    elif model_name == 'DeepStochastic':\n",
    "        model = DeepStochastic(Config.channels)\n",
    "    elif model_name == 'Deeper':\n",
    "        model = Deeper(Config.channels)\n",
    "    elif model_name == 'Deeper2':\n",
    "        model = Deeper2(Config.channels)\n",
    "    elif model_name == 'ModelIafossV2':\n",
    "        model = ModelIafossV2(Config.channels)\n",
    "    elif model_name == 'ModelIafossV2Mish':\n",
    "        model = ModelIafossV2(Config.channels,act=Mish())\n",
    "    elif model_name == 'ModelIafossV2Elu':\n",
    "        model = ModelIafossV2(Config.channels,act=torch.nn.ELU())\n",
    "    elif model_name == 'V2StochasticDepth':\n",
    "        model = V2StochasticDepth(Config.channels)\n",
    "    elif model_name == 'V2SDCBAM':\n",
    "        model = V2SDCBAM(Config.channels)\n",
    "    elif model_name == 'BoTCBAMV2SD':\n",
    "        model = BoTCBAMV2SD(Config.channels)\n",
    "    elif model_name == 'BoTV2SD':\n",
    "        model = BoTV2SD(Config.channels)\n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "    print(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ce418f8-18dc-40c4-ac7b-fe4384f44245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2StochasticDepth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5845905"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "model = Model()#can possibly call random\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c9f2c-fc5f-41ee-afa5-2dfbec2ef82f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b76b69f-3f6d-40dd-92e9-b04be3232814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(seed=Config.seed)    \n",
    "\n",
    "def get_scheduler(optimizer, train_size):\n",
    "    if Config.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=Config.factor, \n",
    "                                      patience=Config.patience, verbose=True, eps=Config.eps)\n",
    "    elif Config.scheduler=='CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, \n",
    "                                      T_max=Config.T_max, \n",
    "                                      eta_min=Config.min_lr, last_epoch=-1)\n",
    "    elif Config.scheduler=='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                T_0=Config.T_0, \n",
    "                                                T_mult=1, \n",
    "                                                eta_min=Config.min_lr, \n",
    "                                                last_epoch=-1)\n",
    "    elif Config.scheduler=='CyclicLR':\n",
    "        iter_per_ep = train_size/Config.batch_size\n",
    "        step_size_up = int(iter_per_ep*Config.step_up_epochs)\n",
    "        step_size_down=int(iter_per_ep*Config.step_down_epochs)\n",
    "        scheduler = CyclicLR(optimizer, \n",
    "                             base_lr=Config.base_lr, \n",
    "                             max_lr=Config.max_lr,\n",
    "                             step_size_up=step_size_up,\n",
    "                             step_size_down=step_size_down,\n",
    "                             mode=Config.mode,\n",
    "                             gamma=Config.cycle_decay**(1/(step_size_up+step_size_down)),\n",
    "                             cycle_momentum=False)\n",
    "        \n",
    "    elif Config.scheduler == 'cosineWithWarmUp':\n",
    "        epoch_step = train_size/Config.batch_size\n",
    "        num_warmup_steps = int(0.1 * epoch_step * Config.epochs)\n",
    "        num_training_steps = int(epoch_step * Config.epochs)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps=num_warmup_steps, \n",
    "                                                    num_training_steps=num_training_steps)      \n",
    "    return scheduler\n",
    "def mixed_criterion(loss_fn, pred, y_a, y_b, lam):\n",
    "    return lam * loss_fn(pred, y_a) + (1 - lam) * loss_fn(pred, y_b)\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size, requires_grad=False).to(x.device,non_blocking=Config.non_blocking)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "498ba49a-52a8-4e90-9b51-4fb267a03350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-PCIE-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Reserved:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "if Config.use_tpu:\n",
    "    device = xm.xla_device()\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')#for debug, tb see\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "# watch nvidia-smi\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Reserved:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588c2fb-6f2f-4a71-ae80-af4e7facbf40",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1791f02a-cd63-448f-ad07-a216643c196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        torch.save(model.state_dict(), f'{Config.model_output_folder}/init_params.pt')\n",
    "\n",
    "    def range_test(self, loader, end_lr = 10, num_iter = 100, \n",
    "                   smooth_f = 0.05, diverge_th = 5):\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        for step, batch in enumerate(loader):\n",
    "            if step == num_iter:\n",
    "                break\n",
    "            loss = self._train_batch(batch)\n",
    "            lrs.append(lr_scheduler.get_last_lr()[0])\n",
    "            #update lr\n",
    "            lr_scheduler.step()\n",
    "            if step > 0:\n",
    "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "            losses.append(loss)\n",
    "            if loss > diverge_th * best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "        #reset model to initial parameters\n",
    "        model.load_state_dict(torch.load(f'{Config.model_output_folder}/init_params.pt'))\n",
    "        return lrs, losses\n",
    "\n",
    "    def _train_batch(self, batch):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        scaler = GradScaler()\n",
    "        X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "        targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "        \n",
    "        if Config.use_mixup:\n",
    "            (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                X, targets, Config.mixup_alpha\n",
    "            )\n",
    "            with autocast(enabled=False):\n",
    "                outputs = self.model(X_mix).squeeze()\n",
    "                loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "        else:\n",
    "            with autocast(enabled=False):\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "        #loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if Config.use_tpu:\n",
    "            xm.optimizer_step(self.optimizer, barrier=True)  # Note: TPU-specific code! \n",
    "        else:\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "#             self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "                    \n",
    "class ExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "def plot_lr_finder(lrs, losses, skip_start = 0, skip_end = 0):\n",
    "    if skip_end == 0:\n",
    "        lrs = lrs[skip_start:]\n",
    "        losses = losses[skip_start:]\n",
    "    else:\n",
    "        lrs = lrs[skip_start:-skip_end]\n",
    "        losses = losses[skip_start:-skip_end]\n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Learning rate')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.grid(True, 'both', 'x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0acca149-21fd-4ee7-9195-6e9c4ab3343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2StochasticDepth\n"
     ]
    }
   ],
   "source": [
    "if Config.use_lr_finder:\n",
    "    START_LR = 1e-7\n",
    "    model = Model()\n",
    "    model.to(device,non_blocking=Config.non_blocking)\n",
    "    optimizer = AdamW(model.parameters(), lr=START_LR, weight_decay=Config.weight_decay, amsgrad=False)\n",
    "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    train_data_retriever = DataRetrieverLRFinder(train_df['file_path'], train_df[\"target\"].values)\n",
    "    train_loader = DataLoader(train_data_retriever,\n",
    "                                batch_size=Config.batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=Config.num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a155a5-34a6-4f13-bca8-ea8efaade09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13a2e828-f63d-4564-874b-b56924fb62e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "CPU times: user 28.6 s, sys: 4.23 s, total: 32.8 s\n",
      "Wall time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if Config.use_lr_finder:\n",
    "    try:\n",
    "        END_LR = 10\n",
    "        NUM_ITER = 150\n",
    "        lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "        lrs, losses = lr_finder.range_test(train_loader, END_LR, NUM_ITER)\n",
    "    except RuntimeError as e:\n",
    "        del model, optimizer, criterion, train_data_retriever, train_loader, lr_finder\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c0570b0-2648-4761-993f-66b8aa6e5118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if Config.use_lr_finder:\n",
    "    plot_lr_finder(lrs[:-18], losses[:-18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfb7f1-6e67-4dbe-9244-2d08a880b108",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70036313-ffa6-4249-a4b8-faadad8bafa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        scheduler,\n",
    "        valid_labels,\n",
    "        best_valid_score,\n",
    "        fold,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.best_valid_score = best_valid_score\n",
    "        self.valid_labels = valid_labels\n",
    "        self.fold = fold\n",
    "\n",
    "    \n",
    "    def fit(self, epochs, train_loader, valid_loader, save_path): \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "#         global N_EPOCH_EXPLICIT  #tbs later\n",
    "        for n_epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch: ', n_epoch)\n",
    "            N_EPOCH_EXPLICIT = n_epoch\n",
    "            train_loss, train_preds = self.train_epoch(train_loader)\n",
    "            valid_loss, valid_preds = self.valid_epoch(valid_loader)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            if isinstance(self.scheduler, ReduceLROnPlateau):\n",
    "                self.scheduler.step(valid_loss)\n",
    "            valid_score = get_score(self.valid_labels, valid_preds)\n",
    "\n",
    "            numbers = valid_score\n",
    "            filename = Config.model_output_folder+f'score_epoch_{n_epoch}.json'          \n",
    "            with open(filename, 'w') as file_object: \n",
    "                json.dump(numbers, file_object) \n",
    "            \n",
    "\n",
    "            if self.best_valid_score < valid_score:\n",
    "                self.best_valid_score = valid_score\n",
    "                self.save_model(n_epoch, save_path+f'best_model.pth', train_preds, valid_preds)\n",
    "\n",
    "            print('train_loss: ',train_loss)\n",
    "            print('valid_loss: ',valid_loss)\n",
    "            print('valid_score: ',valid_score)\n",
    "            print('best_valid_score: ',self.best_valid_score)\n",
    "            print('time used: ', time.time()-start_time)\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] epoch\": n_epoch+1, \n",
    "                      f\"[fold{self.fold}] avg_train_loss\": train_loss, \n",
    "                      f\"[fold{self.fold}] avg_val_loss\": valid_loss,\n",
    "                      f\"[fold{self.fold}] val_score\": valid_score})        \n",
    "\n",
    "        # fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "        # ax.plot(list(range(epochs)), train_losses, label=\"train_loss\")\n",
    "        # ax.plot(list(range(epochs)), valid_losses, label=\"val_loss\")\n",
    "        # fig.legend()\n",
    "        # plt.show()            \n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        if Config.amp:\n",
    "            scaler = GradScaler()\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        train_loss = 0\n",
    "        # preds = []\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            self.optimizer.zero_grad()\n",
    "            X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "            targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "            \n",
    "            if Config.use_mixup:\n",
    "                (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                    X, targets, Config.mixup_alpha\n",
    "                )\n",
    "                with autocast(enabled=False):\n",
    "                    outputs = self.model(X_mix).squeeze()\n",
    "                    loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                with autocast(enabled=False):\n",
    "                    outputs = self.model(X).squeeze()\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "\n",
    "                \n",
    "            if Config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / Config.gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "          \n",
    "            if (step) % Config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "\n",
    "            if (not isinstance(self.scheduler, ReduceLROnPlateau)):\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # preds.append(outputs.sigmoid().to('cpu').detach().numpy())\n",
    "            loss2 = loss.detach()\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] loss\": loss2,\n",
    "                       f\"[fold{self.fold}] lr\": self.scheduler.get_last_lr()[0]})            \n",
    "\n",
    "            # losses.append(loss2.item())\n",
    "            losses.append(loss2)\n",
    "            train_loss += loss2\n",
    "\n",
    "            if (step) % Config.print_num_steps == 0:\n",
    "                train_loss = train_loss.item() #synch once per print_num_steps instead of once per batch\n",
    "                print(f'[{step}/{len(train_loader)}] ', \n",
    "                      f'avg loss: ',train_loss/step,\n",
    "                      f'inst loss: ', loss2.item())\n",
    "                \n",
    "        # predictions = np.concatenate(preds)\n",
    "\n",
    "#         losses_avg = []\n",
    "#         for i, loss in enumerate(losses):\n",
    "#             if i == 0 :\n",
    "#                 losses_avg.append(loss)\n",
    "#             else:\n",
    "#                 losses_avg.append(losses_avg[-1] * 0.6 + loss * 0.4)\n",
    "#         losses = torch.stack(losses)\n",
    "#         losses_avg = torch.stack(losses_avg)\n",
    "#         fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "#         ax.plot(list(range(step)), losses, label=\"train_loss per step\")\n",
    "#         ax.plot(list(range(step)), losses_avg, label=\"train_loss_avg per step\")\n",
    "#         fig.legend()\n",
    "#         plt.show()            \n",
    "        \n",
    "        return train_loss / step, None#, predictions\n",
    "\n",
    "    def valid_epoch(self, valid_loader):\n",
    "        self.model.eval()      \n",
    "        valid_loss = []\n",
    "        preds = []\n",
    "        for step, batch in enumerate(valid_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "                targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                if Config.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / Config.gradient_accumulation_steps\n",
    "                valid_loss.append(loss.detach().item())\n",
    "                preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "#                 valid_loss.append(loss.detach())#.item())\n",
    "#                 preds.append(outputs.sigmoid())#.to('cpu').numpy())\n",
    "#         valid_loss = torch.cat(valid_loss).to('cpu').numpy()\n",
    "#         predictions = torch.cat(preds).to('cpu').numpy()\n",
    "        predictions = np.concatenate(preds)\n",
    "        return np.mean(valid_loss), predictions\n",
    "\n",
    "    def save_model(self, n_epoch, save_path, train_preds, valid_preds):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"best_valid_score\": self.best_valid_score,\n",
    "                \"n_epoch\": n_epoch,\n",
    "                'scheduler': self.scheduler.state_dict(),\n",
    "                'train_preds': train_preds,\n",
    "                'valid_preds': valid_preds,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0dfd0b-83ea-48f9-aca0-7e84b3346689",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f26270c-7a97-4d8a-80bd-db5ec6ad345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(seed=Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cce08b8-36a9-4fe2-b764-b612927f3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_PL(fold):\n",
    "#     up_thresh = Config.up_thresh\n",
    "#     down_thresh = Config.down_thresh\n",
    "#     pseudo_label_df = pd.read_csv(Config.pseudo_label_folder + f\"test_Fold_{fold}.csv\") \n",
    "#     pseudo_label_df.head()\n",
    "#     pseudo_label_df[\"target\"] = pseudo_label_df[f'preds_Fold_{fold}']#or adding tta\n",
    "#     num_test = pseudo_label_df.shape[0]\n",
    "#     num_yes = (pseudo_label_df[\"target\"] >= up_thresh).sum()\n",
    "#     num_no = (pseudo_label_df[\"target\"] <= down_thresh).sum()\n",
    "#     num_all = num_yes+num_no\n",
    "#     print(\"{:.2%} ratio, {:.2%} 1, {:.2%} 0\".format(num_all/num_test, num_yes/num_test, num_no/num_test))\n",
    "#     print(num_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae6c66a1-0cda-4447-b5c8-017af0d03874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Config.use_pseudo_label:\n",
    "#     for fold in Config.train_folds:\n",
    "#         check_PL(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aed9b-d467-49b5-9611-8a170be6d00a",
   "metadata": {},
   "source": [
    "## non-leaky PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14d354a8-d7e3-4bbb-85ee-f5e6a0126652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_PL(fold,up_thresh,down_thresh,train_df,test_df):\n",
    "    pseudo_label_df = pd.read_csv(Config.pseudo_label_folder + f\"test_Fold_{fold}.csv\") \n",
    "    \n",
    "    #soft labels\n",
    "    pseudo_label_df[\"target\"] = pseudo_label_df[f'preds_Fold_{fold}']\n",
    "    \n",
    "    #harden labels\n",
    "#     test_df_2 = pseudo_label_df[(pseudo_label_df[\"target\"] >= up_thresh) | (pseudo_label_df[\"target\"] <= down_thresh)].copy()\n",
    "#     test_df_2[\"target\"] = (test_df_2[\"target\"] >= up_thresh).astype(int)\n",
    "#     test_df_2 = test_df_2.merge(test_df[[\"id\",\"file_path\"]],on=\"id\",how=\"left\") #no need for this line if already has path\n",
    "    test_df_2 = pseudo_label_df.copy()\n",
    "    test_df_2['fold'] = Config.n_fold\n",
    "    PL_train_df = pd.concat([train_df, test_df_2]).reset_index(drop=True)\n",
    "    PL_train_df.reset_index(inplace=True, drop=True)\n",
    "#         display(train_df_PL.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True)))\n",
    "#         display(train_df_PL.shape)\n",
    "#         display(train_df_PL)\n",
    "    return PL_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdaf0da2-d079-4982-bb12-8ff2ea2751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_PL(fold,Config.up_thresh,Config.down_thresh,train_df.copy(),test_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1aff774-130a-4ba1-9132-5a7a60553992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_df, use_checkpoint=Config.use_checkpoint):\n",
    "    kf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "    avg_best_valid_score = 0\n",
    "    folds_val_score = []\n",
    "    original_train_df = train_df.copy()#for PL\n",
    "    for fold in range(Config.n_fold): \n",
    "        if Config.use_pseudo_label:\n",
    "            PL_train_df = generate_PL(fold,Config.up_thresh,Config.down_thresh,original_train_df.copy(),test_df)   \n",
    "            train_df = PL_train_df\n",
    "        train_index, valid_index = train_df.query(f\"fold!={fold}\").index, train_df.query(f\"fold=={fold}\").index #fold means fold_valid \n",
    "        print('Fold: ', fold)\n",
    "        if fold not in Config.train_folds:\n",
    "            print(\"skip\")\n",
    "            continue\n",
    "        train_X, valid_X = train_df.loc[train_index], train_df.loc[valid_index]\n",
    "        valid_labels = train_df.loc[valid_index,Config.target_col].values\n",
    "#         fold_indices = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv')#saved fold ids\n",
    "        oof = pd.DataFrame()\n",
    "        oof['id'] = train_df.loc[valid_index,'id']\n",
    "        oof['id'] = valid_X['id'].values.copy()\n",
    "        oof = oof.reset_index()\n",
    "        # assert oof['id'].eq(fold_indices['id']).all()\n",
    "#         if not Config.use_subset:\n",
    "#             assert oof['id'].eq(fold_indices['id']).sum()==112000\n",
    "        oof['target'] = valid_labels\n",
    "        \n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        # continue # uncomment this is to check oof ids\n",
    "\n",
    "        print('training data samples, val data samples: ', len(train_X) ,len(valid_X))\n",
    "        train_data_retriever = DataRetriever(train_X[\"file_path\"].values, train_X[\"target\"].values, transforms=train_transform)#how to run this only once and use for next experiment?\n",
    "        valid_data_retriever = DataRetrieverTest(valid_X[\"file_path\"].values, valid_X[\"target\"].values, transforms=test_transform)        \n",
    "        train_loader = DataLoader(train_data_retriever,\n",
    "                                  batch_size=Config.batch_size, \n",
    "                                  shuffle=True, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "        valid_loader = DataLoader(valid_data_retriever, \n",
    "                                  batch_size=Config.batch_size * 2, \n",
    "                                  shuffle=False, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        model = Model()\n",
    "        model.to(device,non_blocking=Config.non_blocking)\n",
    "        optimizer = AdamW(model.parameters(), lr=Config.lr,eps=1e-04, weight_decay=Config.weight_decay, amsgrad=False) #eps to avoid NaN/Inf in training loss\n",
    "        scheduler = get_scheduler(optimizer, len(train_X))\n",
    "        best_valid_score = -np.inf\n",
    "        if use_checkpoint:\n",
    "            print(\"Load Checkpoint, epo\")\n",
    "            checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_valid_score = float(checkpoint['best_valid_score'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        \n",
    "        \n",
    "        criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "        \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model, \n",
    "            device, \n",
    "            optimizer, \n",
    "            criterion,\n",
    "            scheduler,\n",
    "            valid_labels,\n",
    "            best_valid_score,\n",
    "            fold\n",
    "        )\n",
    "\n",
    "        history = trainer.fit(\n",
    "            epochs=Config.epochs, \n",
    "            train_loader=train_loader, \n",
    "            valid_loader=valid_loader,\n",
    "            save_path=f'{Config.model_output_folder}/Fold_{fold}_',\n",
    "        )\n",
    "        folds_val_score.append(trainer.best_valid_score)\n",
    "        del train_data_retriever\n",
    "    wandb.finish()\n",
    "    print('folds score:', folds_val_score)\n",
    "    print(\"Avg: {:.5f}\".format(np.mean(folds_val_score)))\n",
    "    print(\"Std: {:.5f}\".format(np.std(folds_val_score)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36681e6-4f7d-4d6c-b54f-57d800d63016",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d285326b-438d-40af-a1db-eb618cf145c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaggle_go\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"1b0833b15e81d54fad9cfbbe3d923f57562a6f89\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d12bd58-7094-43db-85b9-9071b0fecfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">133rd_V2SD_PL_4ep_2em3lr_32ch_vf_sc01_drop05</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kaggle_go/G2Net\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/3ew6uv28\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/3ew6uv28</a><br/>\n",
       "                Run data is saved locally in <code>/home/wandb/run-20210928_225704-3ew6uv28</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_type= \"debug\" if Config.debug else \"train\"\n",
    "# run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=job_type)\n",
    "run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=Config.model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f8adc-4368-48be-8015-cf9e1e9719ab",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3560abc6-3e87-4021-84e8-abddfd72130a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "skip\n",
      "Fold:  1\n",
      "skip\n",
      "Fold:  2\n",
      "skip\n",
      "Fold:  3\n",
      "skip\n",
      "Fold:  4\n",
      "training data samples, val data samples:  674000 112000\n",
      "V2StochasticDepth\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.5061109706333705 inst loss:  0.4887094795703888\n",
      "[700/2633]  avg loss:  0.4687864467075893 inst loss:  0.42209526896476746\n",
      "[1050/2633]  avg loss:  0.45118012927827383 inst loss:  0.38421565294265747\n",
      "[1400/2633]  avg loss:  0.44104217529296874 inst loss:  0.4238743782043457\n",
      "[1750/2633]  avg loss:  0.43334054129464283 inst loss:  0.41216927766799927\n",
      "[2100/2633]  avg loss:  0.42799630301339286 inst loss:  0.38816559314727783\n",
      "[2450/2633]  avg loss:  0.4238196049904337 inst loss:  0.39668476581573486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.4218, device='cuda:0')\n",
      "valid_loss:  0.42271838612752416\n",
      "valid_score:  0.8725797657926129\n",
      "best_valid_score:  0.8725797657926129\n",
      "time used:  531.1158275604248\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3952197701590402 inst loss:  0.4057198762893677\n",
      "[700/2633]  avg loss:  0.393405020577567 inst loss:  0.37889790534973145\n",
      "[1050/2633]  avg loss:  0.39208048502604165 inst loss:  0.44389837980270386\n",
      "[1400/2633]  avg loss:  0.3909822300502232 inst loss:  0.3987402319908142\n",
      "[1750/2633]  avg loss:  0.39037503487723213 inst loss:  0.38108572363853455\n",
      "[2100/2633]  avg loss:  0.3899168468656994 inst loss:  0.43498921394348145\n",
      "[2450/2633]  avg loss:  0.3893067402742347 inst loss:  0.4171134829521179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3890, device='cuda:0')\n",
      "valid_loss:  0.4034751067150673\n",
      "valid_score:  0.876248427533384\n",
      "best_valid_score:  0.876248427533384\n",
      "time used:  528.7872107028961\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.38259334019252234 inst loss:  0.37643665075302124\n",
      "[700/2633]  avg loss:  0.3808040509905134 inst loss:  0.33189329504966736\n",
      "[1050/2633]  avg loss:  0.3802412923177083 inst loss:  0.38014882802963257\n",
      "[1400/2633]  avg loss:  0.38038194928850444 inst loss:  0.3871503472328186\n",
      "[1750/2633]  avg loss:  0.37988134765625 inst loss:  0.3428307771682739\n",
      "[2100/2633]  avg loss:  0.38000281924293156 inst loss:  0.3387196660041809\n",
      "[2450/2633]  avg loss:  0.37990353954081635 inst loss:  0.36380326747894287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3797, device='cuda:0')\n",
      "valid_loss:  0.3994392514500988\n",
      "valid_score:  0.8794204323553587\n",
      "best_valid_score:  0.8794204323553587\n",
      "time used:  528.2954144477844\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3756317138671875 inst loss:  0.3608918786048889\n",
      "[700/2633]  avg loss:  0.3750270298549107 inst loss:  0.37270820140838623\n",
      "[1050/2633]  avg loss:  0.3745194498697917 inst loss:  0.37450486421585083\n",
      "[1400/2633]  avg loss:  0.3739540318080357 inst loss:  0.36932963132858276\n",
      "[1750/2633]  avg loss:  0.3734417201450893 inst loss:  0.38301485776901245\n",
      "[2100/2633]  avg loss:  0.3732060023716518 inst loss:  0.463192343711853\n",
      "[2450/2633]  avg loss:  0.3730364616549745 inst loss:  0.37684184312820435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3727, device='cuda:0')\n",
      "valid_loss:  0.3997876913580176\n",
      "valid_score:  0.8803630947229484\n",
      "best_valid_score:  0.8803630947229484\n",
      "time used:  528.4762632846832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 930<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/wandb/run-20210928_225704-3ew6uv28/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/wandb/run-20210928_225704-3ew6uv28/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold4] avg_train_loss</td><td>0.37267</td></tr><tr><td>[fold4] avg_val_loss</td><td>0.39979</td></tr><tr><td>[fold4] epoch</td><td>4</td></tr><tr><td>[fold4] loss</td><td>0.38515</td></tr><tr><td>[fold4] lr</td><td>0.0</td></tr><tr><td>[fold4] val_score</td><td>0.88036</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold4] avg_train_loss</td><td>█▃▂▁</td></tr><tr><td>[fold4] avg_val_loss</td><td>█▂▁▁</td></tr><tr><td>[fold4] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold4] loss</td><td>█▆▆▃▄▄▃▆▆▃▃▄▅▃▂▄▆▃▃▆▄▄▄▂▂▃▅▁▅▃▃▄▃▁▁▁▄▃▂▂</td></tr><tr><td>[fold4] lr</td><td>▂▄▅▇███████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold4] val_score</td><td>▁▄▇█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">133rd_V2SD_PL_4ep_2em3lr_32ch_vf_sc01_drop05</strong>: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/3ew6uv28\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/3ew6uv28</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folds score: [0.8803630947229484]\n",
      "Avg: 0.88036\n",
      "Std: 0.00000\n",
      "CPU times: user 34min 4s, sys: 2min 11s, total: 36min 15s\n",
      "Wall time: 35min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "#     %lprun -f DataRetriever.__getitem__ -f Trainer.train_epoch -f Trainer.fit -f Trainer.valid_epoch training_loop() \n",
    "    training_loop(train_df,Config.use_checkpoint)\n",
    "except RuntimeError as e:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()   \n",
    "    print(e)# saving oof predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a03bfac-e977-4cc1-908f-fd91ad857903",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eec60e-00c2-43aa-9768-8fb74482de2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d39375ae-356c-4742-8f68-f020b8d71fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "print(Config.train_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce12376c-30df-443b-b057-3a54f4c75230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# import Ipython\n",
    "# IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ccdc76b4-47ca-4622-a4c1-0763f96aaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80731b-2b25-48c4-83bc-491572148207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarviscloud import jarviscloud\n",
    "jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b00fb94-6a9f-43f0-b73d-5738a7e48439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "successfully saved oof predictions for Fold:  0\n",
      "1\n",
      "successfully saved oof predictions for Fold:  1\n",
      "2\n",
      "successfully saved oof predictions for Fold:  2\n",
      "3\n",
      "successfully saved oof predictions for Fold:  3\n",
      "4\n",
      "successfully saved oof predictions for Fold:  4\n"
     ]
    }
   ],
   "source": [
    "for fold in Config.train_folds:\n",
    "    print(fold)\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    # print(checkpoint['valid_preds'])\n",
    "    try:\n",
    "        # oof = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv') also works, used in replacement of next statement for previously not generated Fold_{fold}_oof_pred.csv\n",
    "        oof = pd.read_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        oof['pred'] = checkpoint['valid_preds']\n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv') \n",
    "        print('successfully saved oof predictions for Fold: ', fold)   \n",
    "    except:\n",
    "        raise RuntimeError('failure in saving predictions for Fold: ', fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb59e1-9caf-4636-bb62-18c237d5c716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6afe6c-0be0-485b-a2e2-a899024b8a80",
   "metadata": {},
   "source": [
    "# add TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dafe5ee8-3a67-43fd-94f8-4f04984c9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e70a9794-653f-4c95-9108-547bf07f2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tbs need pythonic way\n",
    "class TTA(Dataset):\n",
    "    def __init__(self, paths, targets, vflip=False, shuffle_channels=False, time_shift=False, \n",
    "                 add_gaussian_noise = False,  time_stretch=False,shuffle01=False,timemask=False,\n",
    "                 shift_channel=False,reduce_SNR=False, ):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.vflip = vflip\n",
    "        self.shuffle_channels = shuffle_channels\n",
    "        self.time_shift = time_shift\n",
    "        self.add_gaussian_noise = add_gaussian_noise\n",
    "        self.time_stretch = time_stretch\n",
    "        self.shuffle01 = shuffle01\n",
    "        self.timemask = timemask\n",
    "        self.shift_channel = shift_channel\n",
    "        self.reduce_SNR = reduce_SNR\n",
    "        if time_shift:\n",
    "            self.time_shift = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096, \n",
    "                                      max_fraction=Config.time_shift_right*1.0/4096, p=1,rollover=False)\n",
    "        if add_gaussian_noise:\n",
    "            self.add_gaussian_noise = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude= 0.015*0.015, p=1)\n",
    "        if time_stretch:\n",
    "            self.time_stretch = A.TimeStretch(min_rate=0.9, max_rate=1.111,leave_length_unchanged=True, p=1)\n",
    "        if timemask:\n",
    "            self.timemask = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=1.0)\n",
    "\n",
    "              \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "        if self.vflip:\n",
    "            waves = -waves\n",
    "        if self.shuffle_channels:\n",
    "            np.random.shuffle(waves)\n",
    "        if self.time_shift:\n",
    "            waves = self.time_shift(waves, sample_rate=2048)\n",
    "        if self.add_gaussian_noise:\n",
    "            waves = self.add_gaussian_noise(waves, sample_rate=2048)\n",
    "        if self.time_stretch:\n",
    "            waves = self.time_stretch(waves, sample_rate=2048)\n",
    "        if self.shuffle01:\n",
    "            waves[[0,1]] = waves[[1,0]]\n",
    "        if self.timemask:\n",
    "            waves = self.timemask(waves, sample_rate=2048)\n",
    "        if self.shift_channel:\n",
    "            waves = shift_channel_func(waves, sample_rate=2048)\n",
    "        if self.reduce_SNR:\n",
    "            waves = reduce_SNR_func(waves, sample_rate=2048)\n",
    "        #snr, shift_channel tba\n",
    "        \n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device,             \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30c18af1-daaf-409d-bbe8-180ca6db1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e95c39f-c917-4d9b-afe8-dc89d5d8f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(loader,model):\n",
    "    preds = []\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        if step % Config.print_num_steps == 0:\n",
    "            print(\"step {}/{}\".format(step, len(loader)))\n",
    "        with torch.no_grad():\n",
    "            X = batch[0].to(device,non_blocking=Config.non_blocking)\n",
    "            outputs = model(X).squeeze()\n",
    "            preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "def get_tta_pred(df,model,**transforms):\n",
    "    data_retriever = TTA(df['file_path'].values, df['target'].values, **transforms)\n",
    "    loader = DataLoader(data_retriever, \n",
    "                            batch_size=Config.batch_size * 2, \n",
    "                            shuffle=False, \n",
    "                            num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "    return get_pred(loader,model)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f838287-0fa6-442d-b39d-f700b6ea483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['vflip', 'add_gaussian_noise', 'timemask', 'shuffle01', 'time_shift']\n"
     ]
    }
   ],
   "source": [
    "##TTA for oof\n",
    "print(conserv_transform_list_strings)\n",
    "print(aggressive_transform_list_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "709164c2-28c6-4eed-954e-3fc524bc9fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[()]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "conserv_transform_powerset = list(powerset(conserv_transform_list_strings))\n",
    "conserv_transform_powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7343696-bcac-4160-8a76-0d79c0dee4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "for transformations in conserv_transform_powerset:\n",
    "    print({transformation:True for transformation in transformations})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf5e8f7-1349-45fe-b646-42e0febed1c3",
   "metadata": {},
   "source": [
    "## generate oof tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c09fa2a-7afc-4c7d-9fbe-fa02890d73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n",
      "Fold  0\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__add_gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__time_shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__add_gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__time_shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  2\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__add_gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__time_shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  3\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__add_gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__time_shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  4\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__add_gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__time_shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Model()\n",
    "\n",
    "for fold in Config.train_folds:\n",
    "    print('Fold ',fold)\n",
    "    oof = train_df.query(f\"fold=={fold}\").copy()\n",
    "    oof['preds'] = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')['valid_preds']\n",
    "    oof['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "    # display(oof)    \n",
    "\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "    \n",
    "    for transformations in conserv_transform_powerset:\n",
    "#         print(transformations)\n",
    "        if transformations:#to avoid double count original\n",
    "            print(\"tta_\"+('_').join(transformations))\n",
    "            oof[\"tta_\"+('_').join(transformations)] = get_tta_pred(oof,model,**{transformation:True for transformation in transformations})\n",
    "        for aggr_transformation in aggressive_transform_list_strings:#tbs combination of conservative and aggressive\n",
    "            print(\"tta_\"+('_').join(transformations)+'_'+aggr_transformation)\n",
    "            oof[\"tta_\"+('_').join(transformations)+'_'+aggr_transformation] = get_tta_pred(oof,model,**{transformation:True for transformation in transformations}, **{aggr_transformation:True})\n",
    "               \n",
    "\n",
    "    oof.to_csv(Config.model_output_folder + f\"/oof_Fold_{fold}.csv\", index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd3fa0d4-3e28-48ea-bf7a-fa6c2df077f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_all = pd.DataFrame()\n",
    "for fold in Config.train_folds:\n",
    "    oof = pd.read_csv(Config.model_output_folder + f\"/oof_Fold_{fold}.csv\")\n",
    "    oof_all = pd.concat([oof_all,oof])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ca818-92aa-4734-8a20-874db80b6c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed70f5fd-00b8-4eb9-a003-6fec6de253d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('_').join(transformations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da1a3065-5238-4efa-8851-cb60c17bc878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 0.8801739879573027\n",
      "tta__vflip 0.8802155890695967\n",
      "tta__add_gaussian_noise 0.8801488065845556\n",
      "tta__timemask 0.8779151828990358\n",
      "tta__shuffle01 0.8796113994017328\n",
      "tta__time_shift 0.8787075903592754\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",roc_auc_score(oof_all['target'], oof_all['preds']))\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    if \"tta\" in col:\n",
    "        print(col,roc_auc_score(oof_all['target'], oof_all[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2757f84-5698-4982-8d06-c3058bc0f410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.880162177937432"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_sample = oof_all[oof_all['fold']==2]\n",
    "roc_auc_score(oof_sample['target'], oof_sample['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b324aba-a293-43a2-86e5-9fae454daa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f727009f8b0>, {'preds': 0.19999999999999996, 'tta__vflip': 0.20000000000000007, 'tta__add_gaussian_noise': 0.20000000000000007, 'tta__timemask': 0.16000000000000003, 'tta__shuffle01': 0.16000000000000003, 'tta__time_shift': 0.08000000000000002})\n",
      "preds 0.19999999999999996\n",
      "tta__vflip 0.20000000000000007\n",
      "tta__add_gaussian_noise 0.20000000000000007\n",
      "tta__timemask 0.16000000000000003\n",
      "tta__shuffle01 0.16000000000000003\n",
      "tta__time_shift 0.08000000000000002\n",
      "preds\n",
      "tta__vflip\n",
      "tta__add_gaussian_noise\n",
      "tta__timemask\n",
      "tta__shuffle01\n",
      "tta__time_shift\n",
      "preds_tta_avg: 0.880417961697041\n"
     ]
    }
   ],
   "source": [
    "oof_all['avg']=0\n",
    "total_weight = 0\n",
    "#weights leaky? not fine tuned\n",
    "\n",
    "oof_weight  = defaultdict(lambda :1)\n",
    "aggr_total_weight = 0\n",
    "for trans in aggressive_transform_list_strings:\n",
    "    aggr_total_weight += getattr(Config(),trans+'_weight')\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    \n",
    "    if 'tta_' in col or 'preds' in col: \n",
    "        for trans in conserv_transform_list_strings:\n",
    "            \n",
    "            if trans in col:\n",
    "                oof_weight[col] *= getattr(Config(),trans+'_proba')\n",
    "            else:\n",
    "                oof_weight[col] *= 1-getattr(Config(),trans+'_proba')\n",
    "            \n",
    "        flag = False\n",
    "        for trans in aggressive_transform_list_strings:\n",
    "            \n",
    "            if trans in col:\n",
    "                oof_weight[col] *= getattr(Config(),trans+'_weight')/aggr_total_weight*Config.aggressive_aug_proba\n",
    "                \n",
    "                flag = True\n",
    "        if not flag:\n",
    "            oof_weight[col] *= (1-Config.aggressive_aug_proba)\n",
    "        \n",
    "print(oof_weight)\n",
    "for key,value in oof_weight.items():\n",
    "    print(key,value)\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    if ('tta_' in col or 'preds' in col): # and 'time_shift' not in col and 'timemask' not in col\n",
    "        print(col)\n",
    "        total_weight+=oof_weight[col]\n",
    "        oof_all['avg'] += oof_all[col]*oof_weight[col]\n",
    "oof_all['avg'] /= total_weight\n",
    "\n",
    "print(\"preds_tta_avg:\",roc_auc_score(oof_all['target'], oof_all['avg']))\n",
    "\n",
    "oof_all.to_csv(Config.model_output_folder + \"/oof_all.csv\", index=False)\n",
    "oof_all[['id','fold','avg']].rename(columns={'id':'id','fold':'fold','avg':'prediction'}).to_csv(Config.model_output_folder + \"/oof_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a7c77-63b3-4efd-82e3-588ff224c583",
   "metadata": {},
   "source": [
    "## generate TTA for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a1c70-e352-435b-8767-3c397d4bd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__vflip_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__timemask_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__shuffle01_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__vflip_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__timemask_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__shuffle01_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__vflip_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__vflip_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__shuffle01_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__vflip_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__timemask_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "test_df['target'] = 0  \n",
    "model = Model()\n",
    "\n",
    "for fold in Config.train_folds:\n",
    "    test_df2 = test_df.copy()\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "\n",
    "    test_df2['preds'+f'_Fold_{fold}'] = get_tta_pred(test_df2,model)\n",
    "\n",
    "    for transformations in conserv_transform_powerset:\n",
    "#         print(transformations)\n",
    "        if transformations:#to avoid double count original\n",
    "            print(\"tta_\"+('_').join(transformations)+f'_Fold_{fold}')\n",
    "            test_df2[\"tta_\"+('_').join(transformations)+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,**{transformation:True for transformation in transformations})\n",
    "        for transformation in aggressive_transform_list_strings:#tbs combination of conservative and aggressive\n",
    "            print(\"tta_\"+('_').join(transformations)+'_'+transformation+f'_Fold_{fold}')\n",
    "            test_df2[\"tta_\"+('_').join(transformations)+'_'+transformation+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,**{transformation:True for transformation in transformations}, **{transformation:True})\n",
    "               \n",
    "    test_df2.to_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab6a61-d4eb-447e-ae1f-33b1af2958ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg = test_df[['id', 'target']].copy()\n",
    "test_avg['target'] = 0\n",
    "# print(test_avg.describe())\n",
    "\n",
    "total_weight = 0\n",
    "for fold in Config.train_folds:\n",
    "#     test_weight = {key+f'_Fold_{fold}':value for key,value in oof_weight.items()}\n",
    "    test_weight = oof_weight #defaultdict(lambda:1)\n",
    "    test_df2 = pd.read_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\")\n",
    "#     print(test_df2.describe())\n",
    "    for col in test_df2.columns:\n",
    "        col_weight = col.split('_Fold_')[0]\n",
    "        if ('tta_' in col or 'preds' in col): \n",
    "#             print(col)\n",
    "#             print(test_weight[col_weight])\n",
    "            total_weight+=test_weight[col_weight]\n",
    "            test_avg['target'] += test_df2[col]*test_weight[col_weight]\n",
    "test_avg['target'] /= total_weight\n",
    "print(test_avg.describe())\n",
    "print(test_avg[\"target\"].hist(bins=100))\n",
    "print(test_avg)\n",
    "# print(total_weight)\n",
    "test_avg.to_csv(Config.model_output_folder + \"/test_avg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd406d-d7fd-4958-8e73-9d2caa624c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a2c45-3493-49b5-a750-d4bb4b33303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg[['id', 'target']].to_csv(\"./submission.csv\", index=False)\n",
    "\n",
    "test_avg[['id', 'target']].to_csv(Config.model_output_folder + \"/submission.csv\", index=False)\n",
    "\n",
    "!mkdir -p ~/.kaggle/ && cp $Config.kaggle_json_path ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f74d3-2783-4014-9b37-a0d6997797ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c g2net-gravitational-wave-detection -f ./submission.csv -m $Config.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6966b-8273-46fa-bc42-ec36bf4b2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarviscloud import jarviscloud\n",
    "jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea4e9c-95c5-4aba-9930-e13842e2c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -cf  $Config.model_version  -C  $Config.model_output_folder ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f3ae8-bce5-49f7-82a7-fdd49ef16eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "340px",
    "width": "209.2px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
