{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd22902b-841f-41c9-a66f-77395174ae4c",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bb6b65-2c6f-453d-8b6f-c6bf386f446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # !pip install -q nnAudio\n",
    "# !pip install -q --upgrade wandb\n",
    "# !pip install -q grad-cam\n",
    "# # !pip install -q ttach\n",
    "# # !pip install efficientnet_pytorch\n",
    "# # !pip install albumentations\n",
    "# !pip install line_profiler\n",
    "# !pip install transformers\n",
    "# !pip install audiomentations\n",
    "# !pip3 install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2025db-3b0f-43ab-a216-e9058852e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ipykernel<6\"\n",
    "# !pip install \"jupyterlab<3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171b030f-1a09-489c-8b3a-c574d306ade7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import collections\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "import sys\n",
    "import json\n",
    "import wandb\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "from glob import glob\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True) \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import IPython.display\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as torch_functional\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts,\n",
    "                    CosineAnnealingLR, ReduceLROnPlateau,_LRScheduler,CyclicLR)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import audiomentations as A\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, PolarityInversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd075088-f395-437c-a176-da7bf90a4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3e2de-86a7-4a6f-ad50-ef450f241250",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c74b277a-2cf0-4edc-8d36-75d33f89d081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G2Net-Model/122nd_V2_PL_6ep_2em3lr_32ch_vf+gn+sc01+tm+ts/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "\n",
    "    #frequently changed \n",
    "    model_name = 'TCNN'\n",
    "    model_version = \"121st_V2SD_PL_6ep_2em3lr_32ch_vf+gn+sc01+tm+ts\" \n",
    "    model_module = 'V2StochasticDepth'#V2StochasticDepth,ModelIafossV2\n",
    "    use_pretrain = False\n",
    "    use_pseudo_label = True\n",
    "    up_thresh = 0.70\n",
    "    down_thresh = 0.15\n",
    "\n",
    "    debug = False\n",
    "    use_checkpoint = False\n",
    "    use_lr_finder = True\n",
    "    use_subset = False \n",
    "    subset_frac = 0.4\n",
    "\n",
    "    #preproc related\n",
    "    #augmentation\n",
    "    #proba for conservative, weight for aggressive\n",
    "    \n",
    "    #conservative\n",
    "    conservative_aug = []#'vflip','add_gaussian_noise',\n",
    "    #aggressive, OneOf \n",
    "    aggressive_aug_proba = 0.80\n",
    "    aggressive_aug = ['vflip','add_gaussian_noise','shuffle01','timemask','time_shift',]     #'reduce_SNR'\n",
    "    \n",
    "    \n",
    "    vflip = True\n",
    "    vflip_proba = 0.5\n",
    "    vflip_weight = 1.0 \n",
    "    add_gaussian_noise = True \n",
    "    add_gaussian_noise_proba = 0.5 \n",
    "    add_gaussian_noise_weight = 1.0    \n",
    "    timemask = True\n",
    "    timemask_proba = 0.35\n",
    "    timemask_weight = 0.8\n",
    "    shuffle01 = True\n",
    "    shuffle01_proba = 0.35\n",
    "    shuffle01_weight = 0.8\n",
    "    time_shift = True\n",
    "    time_shift_left = 96\n",
    "    time_shift_right = 96\n",
    "    time_shift_proba = 0.35\n",
    "    time_shift_weight = 0.4\n",
    "    \n",
    "    shift_channel = False\n",
    "    shift_channel_left = 16\n",
    "    shift_channel_right = 16\n",
    "    shift_channel_proba = 0.5\n",
    "    shift_channel_weight = 1.0\n",
    "    shift_two_channels = False #tba\n",
    "    shift_two_channels_proba = 0.5\n",
    "    shift_two_channels_weight= 1.0\n",
    "    reduce_SNR = False\n",
    "    reduce_SNR_ratio = 0.9998\n",
    "    reduce_SNR_proba = 0.5\n",
    "    reduce_SNR_weight = 1.0\n",
    "\n",
    "    time_stretch = False\n",
    "    divide_std = False \n",
    "    shuffle_channels = False    \n",
    "    pitch_shift = False\n",
    "    use_mixup = False\n",
    "    mixup_alpha = 0.1\n",
    "    cropping = False\n",
    "    \n",
    "    #logistic\n",
    "    seed = 48\n",
    "    target_size = 1\n",
    "    target_col = 'target'\n",
    "    n_fold = 5\n",
    "#     gdrive = './drive/MyDrive/Kaggle/G2Net/input/'\n",
    "    kaggle_json_path = 'kaggle/kaggle.json'\n",
    "    output_dir = \"G2Net-Model/\"\n",
    "    pseudo_label_folder = \"G2Net-Model/main_112th_V2SD_PL_6ep_5Fold/\"#main_35th_GeM_vflip_shuffle01_5fold,#main_112th_V2SD_PL_6ep_5Fold\n",
    "\n",
    "    #logger\n",
    "    print_num_steps=350\n",
    "    \n",
    "    #training related\n",
    "    train_folds = [0,1,2,3,4]\n",
    "    epochs = 6\n",
    "    batch_size = 256\n",
    "    \n",
    "    lr=  2e-3 #2e-3#8e-3#1e-2#5e-3, 1e-2 # Optimizer  1e-2 channel8, 5e-3 or 2e-3 channel32, 7e-3 channel 16\n",
    "    weight_decay=0 #1e-4  # Optimizer, default value 0.01\n",
    "    gradient_accumulation_steps=1 # Optimizer\n",
    "    scheduler='cosineWithWarmUp' # warm up ratio 0.1 of total steps \n",
    "     \n",
    "    #speedup\n",
    "    num_workers=7\n",
    "    non_blocking=False\n",
    "    amp=True\n",
    "    use_cudnn = True \n",
    "    use_tpu = False\n",
    "    use_ram = False\n",
    "    continuous_exp = False\n",
    "    \n",
    "    #CNN structure\n",
    "    channels = 32\n",
    "    reduction = 4.0\n",
    "    stochastic_final_layer_proba = 0.8\n",
    "\n",
    "# no need to change below\n",
    "Config.model_output_folder = Config.output_dir + Config.model_version + \"/\"\n",
    "if not os.path.exists(Config.output_dir):\n",
    "    os.mkdir(Config.output_dir)\n",
    "if not os.path.exists(Config.model_output_folder):\n",
    "    os.mkdir(Config.model_output_folder)\n",
    "\n",
    "torch.backends.cudnn.benchmark = Config.use_cudnn \n",
    "display(Config.model_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2fd5ddd-01f0-4e0c-a483-fe055e23bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run once for Fold 0, save it in RAM and then do experiments multiple times       \n",
    "# if Config.continuous_exp and Config.train_folds == [0]:\n",
    "#     start_time =time.time()  \n",
    "#     if Config.use_pseudo_label:\n",
    "#         with open('fold_0_data_PL.npy', 'rb') as f:\n",
    "#             fold_0_data_PL = np.load(f)\n",
    "#     else:\n",
    "#         with open('fold_0_data.npy', 'rb') as f:\n",
    "#             fold_0_data = np.load(f)\n",
    "#     print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d7181-da0e-4f49-b7dc-d1af56fa584c",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b1bd21-a513-4b67-9181-01e1468b1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "save_object(class2dict(Config), Config.model_output_folder + \"Config.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f52f10-a321-463e-be55-6f1267407999",
   "metadata": {},
   "source": [
    "# Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba86f1b-6607-4b9d-a251-f79d9cb7b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_2_path(file_id: str, train=True) -> str:\n",
    "    if train:\n",
    "        return \"./output/whiten-train-w0/{}.npy\".format(file_id)\n",
    "    else:\n",
    "        return \"./output/whiten-test-w0/{}.npy\".format(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93a20a41-a049-4ce9-aaf7-61856e1d1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training_labels.csv')\n",
    "test_df = pd.read_csv('sample_submission.csv')\n",
    "if Config.debug:\n",
    "    Config.epochs = 1\n",
    "    train_df = train_df.sample(n=50000, random_state=Config.seed).reset_index(drop=True)\n",
    "if Config.use_subset:\n",
    "    train_df = train_df.sample(frac=Config.subset_frac, random_state=Config.seed).reset_index(drop=True)\n",
    "train_df['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "test_df['file_path'] = test_df['id'].apply(lambda x :id_2_path(x,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcdac68-2ef7-46b7-91b4-bd52e2585fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.013057893\n",
      "0.01869043\n",
      "0.005618966\n",
      "-0.01463365\n",
      "0.010516807\n"
     ]
    }
   ],
   "source": [
    "# checking magnitude of waves\n",
    "num_files = 5\n",
    "input_file_paths = train_df['file_path'].values[:num_files]\n",
    "batch_waves=np.zeros((num_files,3,4096))\n",
    "for i,input_file_path in enumerate(input_file_paths[:num_files]):\n",
    "    file_name = input_file_path.split('/')[-1].split('.npy')[0]\n",
    "    waves = np.load(input_file_path)#.astype(np.float32) # (3, 4096)\n",
    "#     batch_waves[i,:] = np.array([waves.max(axis=1),np.abs(waves).max(axis=1),np.abs(waves).min(axis=1)])\n",
    "    whitened_waves = waves#whiten(waves)\n",
    "    print(whitened_waves[2][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e6d52c-c6fe-4ef7-bae2-e5dbfcffed6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold   \n",
       "0     0    0.500125\n",
       "      1    0.499875\n",
       "1     0    0.500125\n",
       "      1    0.499875\n",
       "2     0    0.500125\n",
       "      1    0.499875\n",
       "3     0    0.500125\n",
       "      1    0.499875\n",
       "4     0    0.500125\n",
       "      1    0.499875\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!\n",
    "skf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "splits = skf.split(train_df, train_df[\"target\"])\n",
    "train_df['fold'] = -1\n",
    "for fold, (train_index, valid_index) in enumerate(splits):\n",
    "    train_df.loc[valid_index,\"fold\"] = fold\n",
    "# train_df['fold_PL'] = train_df['fold']\n",
    "\n",
    "train_df.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7621283-4686-41fc-a66f-c99b7bfa6143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>file_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000e74ad</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/00000e74ad.npy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f4945</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/00001f4945.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000661522</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/0000661522.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00007a006a</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/00007a006a.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000a38978</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/0000a38978.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559995</th>\n",
       "      <td>ffff9a5645</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/ffff9a5645.npy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559996</th>\n",
       "      <td>ffffab0c27</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/ffffab0c27.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559997</th>\n",
       "      <td>ffffcf161a</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/ffffcf161a.npy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559998</th>\n",
       "      <td>ffffd2c403</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/ffffd2c403.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559999</th>\n",
       "      <td>fffff2180b</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/fffff2180b.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  target                                file_path  fold\n",
       "0       00000e74ad       1  ./output/whiten-train-w0/00000e74ad.npy     3\n",
       "1       00001f4945       0  ./output/whiten-train-w0/00001f4945.npy     0\n",
       "2       0000661522       0  ./output/whiten-train-w0/0000661522.npy     4\n",
       "3       00007a006a       0  ./output/whiten-train-w0/00007a006a.npy     0\n",
       "4       0000a38978       1  ./output/whiten-train-w0/0000a38978.npy     4\n",
       "...            ...     ...                                      ...   ...\n",
       "559995  ffff9a5645       1  ./output/whiten-train-w0/ffff9a5645.npy     3\n",
       "559996  ffffab0c27       0  ./output/whiten-train-w0/ffffab0c27.npy     1\n",
       "559997  ffffcf161a       1  ./output/whiten-train-w0/ffffcf161a.npy     2\n",
       "559998  ffffd2c403       0  ./output/whiten-train-w0/ffffd2c403.npy     1\n",
       "559999  fffff2180b       0  ./output/whiten-train-w0/fffff2180b.npy     4\n",
       "\n",
       "[560000 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1750c4-a631-4586-9f0a-400fbbae54a6",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b11e6-fe2c-4dcb-80c5-6f6e5acf3903",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "446a1dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservative transforms:  []\n",
      "aggressive transforms:  ['vflip', 'add_gaussian_noise', 'timemask', 'shuffle01', 'time_shift']\n"
     ]
    }
   ],
   "source": [
    "conserv_transform_list = []\n",
    "aggressive_transform_list = []\n",
    "conserv_transform_list_strings = []\n",
    "aggressive_transform_list_strings = []\n",
    "\n",
    "#-------------------------vflip\n",
    "if Config.vflip:\n",
    "#     trans = lambda x:-x\n",
    "    def vflip_func(x,sample_rate=2048):\n",
    "        return -x\n",
    "    def vflip_func_random(x,sample_rate=2048):\n",
    "        if np.random.random()<Config.vflip_proba:\n",
    "            return -x\n",
    "        else:\n",
    "            return x\n",
    "    if 'vflip' in Config.aggressive_aug:\n",
    "        aggressive_transform_list.append(vflip_func)\n",
    "        aggressive_transform_list_strings.append('vflip')\n",
    "    else:\n",
    "        conserv_transform_list.append(vflip_func_random)\n",
    "        conserv_transform_list_strings.append('vflip')\n",
    "#----------------------add_gaussian_noise        \n",
    "if Config.add_gaussian_noise:\n",
    "    \n",
    "    if 'add_gaussian_noise' in Config.aggressive_aug:\n",
    "        trans = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude=0.015*0.015, p=1) #tbs #0.015 is the estimated std\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('add_gaussian_noise')\n",
    "    else:\n",
    "        trans = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude=0.015*0.015, p=Config.add_gaussian_noise_proba) #tbs #0.015 is the estimated std\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('add_gaussian_noise')\n",
    "\n",
    "#--------------------------timemask\n",
    "if Config.timemask:\n",
    "    \n",
    "    if 'timemask' in Config.aggressive_aug:\n",
    "        trans = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=1)\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('timemask')\n",
    "    else:\n",
    "        trans = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=Config.timemask_proba)\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('timemask')\n",
    "\n",
    "#--------------------------shuffle01        \n",
    "def shuffle01_func(x,sample_rate=2048):\n",
    "    return x[[1,0,2]]\n",
    "def shuffle01_func_random(x,sample_rate=2048):\n",
    "    if np.random.random()<Config.shuffle01_proba: \n",
    "        return x[[1,0,2]]\n",
    "    else:\n",
    "        return x\n",
    "if Config.shuffle01:\n",
    "#     trans = lambda x:x[[1,0,2]]\n",
    "\n",
    "    if 'shuffle01' in Config.aggressive_aug:\n",
    "        aggressive_transform_list.append(shuffle01_func)\n",
    "        aggressive_transform_list_strings.append('shuffle01')\n",
    "    else:\n",
    "        conserv_transform_list.append(shuffle01_func_random)\n",
    "        conserv_transform_list_strings.append('shuffle01')\n",
    "#---------------------------time_shift\n",
    "if Config.time_shift:\n",
    "    if 'time_shift' in Config.aggressive_aug:\n",
    "        trans = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096,\n",
    "                        max_fraction=Config.time_shift_right*1.0/4096, \n",
    "                        p=1,rollover=False)#<0 means shift towards left,  fraction of total sound length\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('time_shift')\n",
    "    else:\n",
    "        trans = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096,\n",
    "                                max_fraction=Config.time_shift_right*1.0/4096, \n",
    "                                p=Config.time_shift_proba,rollover=False)\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('time_shift')\n",
    "\n",
    "#-----------------shift_channel        \n",
    "def shift_channel_func(x,sample_rate=2048):\n",
    "    channel = np.random.choice(3)\n",
    "    trans = A.Shift(min_fraction=-Config.shift_channel_left*1.0/4096,\n",
    "                max_fraction=Config.shift_channel_right*1.0/4096, \n",
    "                p=1,rollover=False)\n",
    "    x[channel] = trans(x[channel],sample_rate=2048)\n",
    "    return x\n",
    "def shift_channel_func_random(x,sample_rate=2048):\n",
    "    channel = np.random.choice(3)\n",
    "    trans = A.Shift(min_fraction=-Config.shift_channel_left*1.0/4096,\n",
    "                max_fraction=Config.shift_channel_right*1.0/4096, \n",
    "                p=Config.shift_channel_proba,rollover=False)\n",
    "    x[channel] = trans(x[channel],sample_rate=2048)\n",
    "    return x\n",
    "if Config.shift_channel:\n",
    "    if 'shift_channel' in Config.aggressive_aug:\n",
    "        \n",
    "        aggressive_transform_list.append(shift_channel_func)\n",
    "        aggressive_transform_list_strings.append('shift_channel')\n",
    "    else:\n",
    "        \n",
    "        conserv_transform_list.append(shift_channel_func_random)\n",
    "        conserv_transform_list_strings.append('shift_channel')\n",
    "#-----------------reduce_SNR        \n",
    "def reduce_SNR_func(x,sample_rate=2048):\n",
    "    x = x * Config.reduce_SNR_ratio\n",
    "    trans = A.AddGaussianNoise(min_amplitude=multiplier, max_amplitude=multiplier, p=1)\n",
    "    x = trans(x,sample_rate=2048)\n",
    "    return x \n",
    "def reduce_SNR_func_random(x,sample_rate=2048):\n",
    "    if np.random.random() < Config.reduce_SNR_proba:\n",
    "        x = x * Config.reduce_SNR_ratio\n",
    "        trans = A.AddGaussianNoise(min_amplitude=multiplier, max_amplitude=multiplier, p=1)\n",
    "        x = trans(x,sample_rate=2048)\n",
    "    return x\n",
    "if Config.reduce_SNR:\n",
    "    multiplier = math.sqrt(1-Config.reduce_SNR_ratio**2)\n",
    "    if 'reduce_SNR' in Config.aggressive_aug:\n",
    "\n",
    "        aggressive_transform_list.append(reduce_SNR_func)\n",
    "        aggressive_transform_list_strings.append('reduce_SNR')\n",
    "    else:\n",
    "\n",
    "        conserv_transform_list.append(reduce_SNR_func_random)\n",
    "        conserv_transform_list_strings.append('reduce_SNR')\n",
    "        \n",
    "# if Config.time_stretch:\n",
    "#     trans = A.TimeStretch(min_rate=0.98, max_rate=1.02,leave_length_unchanged=True, p=0.5)\n",
    "#     if 'time_stretch' in aggressive_aug:\n",
    "#         aggressive_transform_list.append(trans)\n",
    "#         aggressive_transform_list_strings.append('time_stretch')\n",
    "#     else:\n",
    "#         conserv_transform_list.append(trans)\n",
    "#         conserv_transform_list_strings.append('time_stretch')\n",
    "# if Config.pitch_shift:\n",
    "#     trans = A.PitchShift(min_semitones=-1, max_semitones=1, p=0.5)\n",
    "#     if 'pitch_shift' in aggressive_aug:\n",
    "#         aggressive_transform_list.append(trans)\n",
    "#         aggressive_transform_list_strings.append('pitch_shift')\n",
    "#     else:\n",
    "#         conserv_transform_list.append(trans)\n",
    "#         conserv_transform_list_strings.append('pitch_shift')\n",
    "# if Config.shift_channel:\n",
    "#     pass\n",
    "\n",
    "print('conservative transforms: ',conserv_transform_list_strings)\n",
    "print('aggressive transforms: ',aggressive_transform_list_strings)\n",
    "train_transform = conserv_transform_list#A.Compose(conserv_transform_list)#,OneOf(aggressive_transform_list,p=0.5)) # no OneOf in audiomentation\n",
    "# \n",
    "\n",
    "test_transform = None #A.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a66e416-c7ac-4526-b32a-96d6aa50190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [getattr(Config(), f'{agg}_weight') for agg in aggressive_transform_list_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f6f8d25-feda-4576-9080-b2e40228ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRetriever(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "\n",
    "\n",
    "        #reading data for fold 0 for fast iteration\n",
    "        if Config.continuous_exp and Config.train_folds == [0]:\n",
    "            if Config.use_pseudo_label:\n",
    "                self.data = fold_0_data_PL\n",
    "            else:\n",
    "                self.data = fold_0_data\n",
    "        else:\n",
    "            if Config.use_ram:\n",
    "                start_time =time.time()\n",
    "                array_shape = (len(self.paths),3,4096)\n",
    "                self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "                for i,path in enumerate(self.paths):\n",
    "                    waves = np.load(path)\n",
    "                    self.data[i,:] = waves            \n",
    "                print(time.time()-start_time)\n",
    "\n",
    "                \n",
    "            # saving Fold 0 data for later use\n",
    "#         with open('fold_0_data_PL.npy', 'wb') as f:\n",
    "#             np.save(f, self.data)\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if Config.use_ram:\n",
    "            waves = self.data[index]\n",
    "        else:\n",
    "            path = self.paths[index] \n",
    "            waves = np.load(path)\n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015 #causing NaN?\n",
    "\n",
    "#         if Config.shuffle_channels:#nn.ChannelShuffle\n",
    "#             if np.random.random()<0.5:\n",
    "#                 np.random.shuffle(waves)\n",
    "                \n",
    "#         if Config.vflip:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves = -waves\n",
    "            \n",
    "        if self.transforms is not None:\n",
    "            for i,_ in enumerate(self.transforms):\n",
    "                transform = conserv_transform_list[i]\n",
    "                waves= transform(waves,sample_rate=2048)\n",
    "            \n",
    "        if aggressive_transform_list_strings:\n",
    "            if np.random.random()<Config.aggressive_aug_proba:\n",
    "                n = len(aggressive_transform_list_strings)\n",
    "                probas = np.array([getattr(Config(), f'{agg}_weight') for agg in aggressive_transform_list_strings])\n",
    "                probas /= probas.sum()\n",
    "                trans_idx = np.random.choice(n,p=probas)\n",
    "                trans = aggressive_transform_list[trans_idx]\n",
    "                waves = trans(waves,sample_rate=2048)\n",
    "\n",
    "\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverTest(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "        if Config.use_ram:\n",
    "            array_shape = (len(self.paths),3,4096)\n",
    "            self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "            for i,path in enumerate(self.paths):\n",
    "                waves = np.load(path)\n",
    "                self.data[i,:] = waves  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if Config.use_ram:\n",
    "            waves = self.data[index]\n",
    "        else:\n",
    "            path = self.paths[index] \n",
    "            waves = np.load(path)\n",
    "            \n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "            \n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverLRFinder(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms     \n",
    "#         start_time =time.time()\n",
    "#         array_shape = (len(self.paths),3,4096)\n",
    "#         self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "#         for i,path in enumerate(self.paths):\n",
    "#             waves = np.load(path)\n",
    "#             self.data[i,:] = waves\n",
    "#         print(time.time()-start_time)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "        \n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "#         if Config.shuffle_channels:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 np.random.shuffle(waves)\n",
    "#         if Config.shuffle01:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves[[0,1]]=waves[[1,0]]\n",
    "#         if Config.vflip:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves = -waves\n",
    "              \n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b86ab7a-b88e-4a0f-9cb7-3a023ae12408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(aggressive_transform_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6f55460-98fb-47e8-b4b6-3d898ff751da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.choice(5,p=[0.1, 0, 0.3, 0.6, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d3d1abe-3f31-4b01-bf9a-15b300461b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    '''\n",
    "    Code modified from the 2d code in\n",
    "    https://amaarora.github.io/2020/08/30/gempool.html\n",
    "    '''\n",
    "    def __init__(self, kernel_size=8, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        with torch.cuda.amp.autocast(enabled=False):#to avoid NaN issue for fp16\n",
    "            return torch_functional.avg_pool1d(x.clamp(min=eps).pow(p), self.kernel_size).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45273271-348f-453b-a905-01e846d7bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/iafoss/mish-activation\n",
    "import torch.nn.functional as F\n",
    "class MishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        sigmoid = torch.sigmoid(x)\n",
    "        tanh_sp = torch.tanh(F.softplus(x)) \n",
    "        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return MishFunction.apply(x)\n",
    "\n",
    "def to_Mish(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, Mish())\n",
    "        else:\n",
    "            to_Mish(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd2ba5-d403-4abf-b971-2303a92ea8ea",
   "metadata": {},
   "source": [
    "## neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de5c4e68-068e-451e-aa4c-ac1e28ed5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCNN_Dilations(nn.Module):\n",
    "    \"\"\"1D convolutional neural network with dilations. Classifier of the gravitaitonal waves\n",
    "    Inspired by the https://arxiv.org/pdf/1904.08693.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.init_conv = nn.Sequential(nn.Conv1d(3, 256, kernel_size=1), nn.ReLU())\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(256, 256, kernel_size=2, dilation=2 ** i),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for i in range(11)\n",
    "            ]\n",
    "        )\n",
    "        self.out_conv = nn.Sequential(nn.Conv1d(256, 1, kernel_size=1), nn.ReLU())\n",
    "        self.fc = nn.Linear(2049, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = self.fc(x)\n",
    "        x.squeeze_(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model1DCNN(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            nn.MaxPool1d(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Model1DCNNGEM(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            GeM(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            GeM(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            GeM(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x    \n",
    "\n",
    "#--------------------------------------------------------------------------- V0\n",
    "class ExtractorMaxPool(nn.Sequential):\n",
    "    def __init__(self, in_c=8, out_c=8, kernel_size=64, maxpool=8, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_c), act,\n",
    "            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.MaxPool1d(kernel_size=maxpool),\n",
    "        )\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes))\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class ModelIafoss(nn.Module):\n",
    "    def __init__(self, n=8, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(ExtractorMaxPool(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),),\n",
    "            nn.Sequential(ExtractorMaxPool(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),)\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlock(3*n,2*n,kernel_size=31,stride=4),\n",
    "            ResBlock(2*n,2*n,kernel_size=31),\n",
    "            ResBlock(2*n,4*n,kernel_size=15,stride=4),\n",
    "            ResBlock(4*n,4*n,kernel_size=15),\n",
    "            ResBlock(4*n,8*n,kernel_size=7,stride=4),\n",
    "            ResBlock(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Flatten(),\n",
    "            nn.Linear(n*8*8,256),nn.BatchNorm1d(256),nn.Dropout(ps), act,\n",
    "            nn.Linear(256, 256),nn.BatchNorm1d(256),nn.Dropout(ps), act,\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "\n",
    "#----------------------------------------------V1    \n",
    "    \n",
    "class AdaptiveConcatPool1d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`\"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "\n",
    "# using GeM\n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, in_c=8, out_c=8, kernel_size=64, maxpool=8, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_c), act,\n",
    "            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "#             nn.MaxPool1d(kernel_size=maxpool),\n",
    "            GeM(kernel_size=maxpool),\n",
    "        )\n",
    "    \n",
    "class ModelIafossV1(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            ResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            ResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            ResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#for SE-----------------------------------------------------------------------------\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, int(channel // reduction), bias=False),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(int(channel // reduction), channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class SEResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True),reduction=Config.reduction):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            SELayer(out_planes, reduction)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1SE(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            SEResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            SEResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            SEResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            SEResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            SEResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            SEResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[1](x[:,1].unsqueeze(1)),\n",
    "            self.ex[2](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#for CBAM-----------------------------------------------------------------------\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, silu=True):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm1d(out_planes,eps=1e-5, momentum=0.01, affine=True) #0.01,default momentum 0.1\n",
    "        self.silu = nn.SiLU(inplace=True) if silu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.silu is not None:\n",
    "            x = self.silu(x)\n",
    "        return x\n",
    "    \n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 15\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, silu=True)#silu False\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = torch.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "    \n",
    "class CBAMResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True),reduction=Config.reduction):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            SELayer(out_planes, reduction),\n",
    "            SpatialGate(),\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "    \n",
    "class ModelIafossV1CBAM(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),CBAMResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          CBAMResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),CBAMResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          CBAMResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            CBAMResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            CBAMResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            CBAMResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            CBAMResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            CBAMResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            CBAMResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))    \n",
    "\n",
    "#---------------------------------------------------------------------------------------------------  \n",
    "    \n",
    "    \n",
    "class BasicBlockPool(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.MaxPool1d(downsample,ceil_mode=True), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    nn.MaxPool1d(downsample,ceil_mode=True),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                     nn.MaxPool1d(2,ceil_mode=True),  # downsampling \n",
    "#                 )\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "    #             self.shortcut = nn.Sequential(\n",
    "    #                     nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "    #                     nn.BatchNorm1d(out_channels),\n",
    "    #                 )#skip layers in residual_function, can try identity, i.e., nn.Sequential()\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1Pool(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicBlockPool(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            BasicBlockPool(3*n,3*n,kernel_size=31), #128\n",
    "            BasicBlockPool(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            BasicBlockPool(4*n,4*n,kernel_size=15), #32\n",
    "            BasicBlockPool(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            BasicBlockPool(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------  \n",
    "    \n",
    "    \n",
    "class ResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1GeM(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "#-----------------------------------------------------------------------------\n",
    "class ModelIafossV1GeMAll(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#-----------------------------------------------------------------------------    \n",
    "class AdaptiveConcatPool1dx3(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d`,`AdaptiveMaxPool1d` and 'GeM' \"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "        self.gemp = GeM(kernel_size=8)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x),self.gemp(x)], 1)\n",
    "    \n",
    "class ModelGeMx3(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1dx3(),nn.Flatten(),\n",
    "            nn.Linear(n*8*3,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "#-----------------------------------------------------------------------------\n",
    "class ModelIafossV1GeMAllDeep(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "    \n",
    "class StochasticDepthResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=False),p=1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.act = act\n",
    "\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple Pooling\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "            \n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:#attribute inherited\n",
    "            if self.survival():\n",
    "                x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.act(self.shortcut(x))\n",
    "        else:\n",
    "            x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "        return x\n",
    "    \n",
    "   \n",
    "    \n",
    "class DeepStochastic(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 11\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.conv = nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,p=self.survival_proba[0]), #512\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[1]), #128\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[2]), \n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[3]), \n",
    "            StochasticDepthResBlockGeM(3*n,4*n,kernel_size=15,downsample=4,p=self.survival_proba[4]), #128\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[5]), #32\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[6]),\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[7]),\n",
    "            StochasticDepthResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,p=self.survival_proba[8]), #32\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,p=self.survival_proba[9]), #8\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,p=self.survival_proba[10]),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#-----------------------------------------------------------------------------\n",
    "class Deeper(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), #128\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3), #32\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3),\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "class Deeper2(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=2), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=2), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=2), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15,downsample=2),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=2),\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7,downsample=2),\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),#8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#-------------------------------------------------------------------V2    \n",
    "\n",
    "class ModelIafossV2(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,act=act)),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15,act=act),#128\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7,act=act), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "    \n",
    "#-----------------------------------\n",
    "class V2StochasticDepth(nn.Module):#stocnot on ex\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        \n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 10\n",
    "#         self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "#         self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(1,num_block+1)]\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[0]), #512\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[1])),\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[2]), #512\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[3])),\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[4]), #512\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,act=act,p=self.survival_proba[5])),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act,p=self.survival_proba[6]),\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,act=act,p=self.survival_proba[7]),#128\n",
    "            StochasticDepthResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act,p=self.survival_proba[8]), #32\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,act=act,p=self.survival_proba[9]), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "    \n",
    "    \n",
    "    \n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, n_dims, length, heads=4):\n",
    "        super(MHSA, self).__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "        self.query = nn.Conv1d(n_dims, n_dims, kernel_size=1)\n",
    "        self.key = nn.Conv1d(n_dims, n_dims, kernel_size=1)\n",
    "        self.value = nn.Conv1d(n_dims, n_dims, kernel_size=1)\n",
    "        self.rel_pos = nn.Parameter(torch.randn([1, heads, n_dims // heads, length]), requires_grad=True)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_batch, C, width, height = x.size()\n",
    "        q = self.query(x).view(n_batch, self.heads, C // self.heads, -1)\n",
    "        k = self.key(x).view(n_batch, self.heads, C // self.heads, -1)\n",
    "        v = self.value(x).view(n_batch, self.heads, C // self.heads, -1)\n",
    "\n",
    "        content_content = torch.matmul(q.permute(0, 2, 1), k)\n",
    "\n",
    "        content_position = self.rel_pos.view(1, self.heads, C // self.heads, -1).permute(0, 2, 1)\n",
    "        content_position = torch.matmul(content_position, q)\n",
    "\n",
    "        energy = content_content + content_position\n",
    "        attention = self.softmax(energy)\n",
    "\n",
    "        out = torch.matmul(v, attention.permute(0, 2, 1))\n",
    "        out = out.view(n_batch, C, width, height)\n",
    "\n",
    "        return out\n",
    "#tbd    \n",
    "class BoTStochasticDepthResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=False),p=1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.act = act\n",
    "\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple Pooling\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "            \n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:#attribute inherited\n",
    "            if self.survival():\n",
    "                x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.act(self.shortcut(x))\n",
    "        else:\n",
    "            x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e1528ce-06af-4d53-84d9-bbbcaf4991b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    model_name = Config.model_module \n",
    "    if model_name == 'Model1DCNN':\n",
    "        model = Model1DCNN(Config.channels)\n",
    "    elif model_name == 'Model1DCNNGEM':\n",
    "        model = Model1DCNNGEM(Config.channels)\n",
    "    elif model_name == 'ModelIafoss':\n",
    "        model = ModelIafoss(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1':\n",
    "        model = ModelIafossV1(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1SE':\n",
    "        model = ModelIafossV1SE(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1CBAM':\n",
    "        model = ModelIafossV1CBAM(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1Pool':\n",
    "        model = ModelIafossV1Pool(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeM':\n",
    "        model = ModelIafossV1GeM(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeMAll':\n",
    "        model = ModelIafossV1GeMAll(Config.channels)\n",
    "    elif model_name == 'ModelGeMx3':\n",
    "        model = ModelGeMx3(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeMAllDeep':\n",
    "        model = ModelIafossV1GeMAllDeep(Config.channels)\n",
    "    elif model_name == 'DeepStochastic':\n",
    "        model = DeepStochastic(Config.channels)\n",
    "    elif model_name == 'Deeper':\n",
    "        model = Deeper(Config.channels)\n",
    "    elif model_name == 'Deeper2':\n",
    "        model = Deeper2(Config.channels)\n",
    "    elif model_name == 'ModelIafossV2':\n",
    "        model = ModelIafossV2(Config.channels)\n",
    "    elif model_name == 'ModelIafossV2Mish':\n",
    "        model = ModelIafossV2(Config.channels,act=Mish())\n",
    "    elif model_name == 'ModelIafossV2Elu':\n",
    "        model = ModelIafossV2(Config.channels,act=torch.nn.ELU())\n",
    "    elif model_name == 'V2StochasticDepth':\n",
    "        model = V2StochasticDepth(Config.channels)\n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "    print(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ce418f8-18dc-40c4-ac7b-fe4384f44245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5845905"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "model = Model()#can possibly call random\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c9f2c-fc5f-41ee-afa5-2dfbec2ef82f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b76b69f-3f6d-40dd-92e9-b04be3232814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(seed=Config.seed)    \n",
    "\n",
    "def get_scheduler(optimizer, train_size):\n",
    "    if Config.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=Config.factor, \n",
    "                                      patience=Config.patience, verbose=True, eps=Config.eps)\n",
    "    elif Config.scheduler=='CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, \n",
    "                                      T_max=Config.T_max, \n",
    "                                      eta_min=Config.min_lr, last_epoch=-1)\n",
    "    elif Config.scheduler=='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                T_0=Config.T_0, \n",
    "                                                T_mult=1, \n",
    "                                                eta_min=Config.min_lr, \n",
    "                                                last_epoch=-1)\n",
    "    elif Config.scheduler=='CyclicLR':\n",
    "        iter_per_ep = train_size/Config.batch_size\n",
    "        step_size_up = int(iter_per_ep*Config.step_up_epochs)\n",
    "        step_size_down=int(iter_per_ep*Config.step_down_epochs)\n",
    "        scheduler = CyclicLR(optimizer, \n",
    "                             base_lr=Config.base_lr, \n",
    "                             max_lr=Config.max_lr,\n",
    "                             step_size_up=step_size_up,\n",
    "                             step_size_down=step_size_down,\n",
    "                             mode=Config.mode,\n",
    "                             gamma=Config.cycle_decay**(1/(step_size_up+step_size_down)),\n",
    "                             cycle_momentum=False)\n",
    "        \n",
    "    elif Config.scheduler == 'cosineWithWarmUp':\n",
    "        epoch_step = train_size/Config.batch_size\n",
    "        num_warmup_steps = int(0.1 * epoch_step * Config.epochs)\n",
    "        num_training_steps = int(epoch_step * Config.epochs)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps=num_warmup_steps, \n",
    "                                                    num_training_steps=num_training_steps)      \n",
    "    return scheduler\n",
    "def mixed_criterion(loss_fn, pred, y_a, y_b, lam):\n",
    "    return lam * loss_fn(pred, y_a) + (1 - lam) * loss_fn(pred, y_b)\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size, requires_grad=False).to(x.device,non_blocking=Config.non_blocking)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "498ba49a-52a8-4e90-9b51-4fb267a03350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-PCIE-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Reserved:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "if Config.use_tpu:\n",
    "    device = xm.xla_device()\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')#for debug, tb see\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "# watch nvidia-smi\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Reserved:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588c2fb-6f2f-4a71-ae80-af4e7facbf40",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1791f02a-cd63-448f-ad07-a216643c196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        torch.save(model.state_dict(), f'{Config.model_output_folder}/init_params.pt')\n",
    "\n",
    "    def range_test(self, loader, end_lr = 10, num_iter = 100, \n",
    "                   smooth_f = 0.05, diverge_th = 5):\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        for step, batch in enumerate(loader):\n",
    "            if step == num_iter:\n",
    "                break\n",
    "            loss = self._train_batch(batch)\n",
    "            lrs.append(lr_scheduler.get_last_lr()[0])\n",
    "            #update lr\n",
    "            lr_scheduler.step()\n",
    "            if step > 0:\n",
    "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "            losses.append(loss)\n",
    "            if loss > diverge_th * best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "        #reset model to initial parameters\n",
    "        model.load_state_dict(torch.load(f'{Config.model_output_folder}/init_params.pt'))\n",
    "        return lrs, losses\n",
    "\n",
    "    def _train_batch(self, batch):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        scaler = GradScaler()\n",
    "        X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "        targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "        \n",
    "        if Config.use_mixup:\n",
    "            (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                X, targets, Config.mixup_alpha\n",
    "            )\n",
    "            with autocast(enabled=False):\n",
    "                outputs = self.model(X_mix).squeeze()\n",
    "                loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "        else:\n",
    "            with autocast(enabled=False):\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "        #loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if Config.use_tpu:\n",
    "            xm.optimizer_step(self.optimizer, barrier=True)  # Note: TPU-specific code! \n",
    "        else:\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "#             self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "                    \n",
    "class ExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "def plot_lr_finder(lrs, losses, skip_start = 0, skip_end = 0):\n",
    "    if skip_end == 0:\n",
    "        lrs = lrs[skip_start:]\n",
    "        losses = losses[skip_start:]\n",
    "    else:\n",
    "        lrs = lrs[skip_start:-skip_end]\n",
    "        losses = losses[skip_start:-skip_end]\n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Learning rate')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.grid(True, 'both', 'x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0acca149-21fd-4ee7-9195-6e9c4ab3343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n"
     ]
    }
   ],
   "source": [
    "if Config.use_lr_finder:\n",
    "    START_LR = 1e-7\n",
    "    model = Model()\n",
    "    model.to(device,non_blocking=Config.non_blocking)\n",
    "    optimizer = AdamW(model.parameters(), lr=START_LR, weight_decay=Config.weight_decay, amsgrad=False)\n",
    "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    train_data_retriever = DataRetrieverLRFinder(train_df['file_path'], train_df[\"target\"].values)\n",
    "    train_loader = DataLoader(train_data_retriever,\n",
    "                                batch_size=Config.batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=Config.num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a155a5-34a6-4f13-bca8-ea8efaade09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13a2e828-f63d-4564-874b-b56924fb62e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "CPU times: user 29 s, sys: 4.32 s, total: 33.3 s\n",
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if Config.use_lr_finder:\n",
    "    try:\n",
    "        END_LR = 10\n",
    "        NUM_ITER = 150\n",
    "        lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "        lrs, losses = lr_finder.range_test(train_loader, END_LR, NUM_ITER)\n",
    "    except RuntimeError as e:\n",
    "        del model, optimizer, criterion, train_data_retriever, train_loader, lr_finder\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c0570b0-2648-4761-993f-66b8aa6e5118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if Config.use_lr_finder:\n",
    "    plot_lr_finder(lrs[:-18], losses[:-18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfb7f1-6e67-4dbe-9244-2d08a880b108",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70036313-ffa6-4249-a4b8-faadad8bafa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        scheduler,\n",
    "        valid_labels,\n",
    "        best_valid_score,\n",
    "        fold,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.best_valid_score = best_valid_score\n",
    "        self.valid_labels = valid_labels\n",
    "        self.fold = fold\n",
    "\n",
    "    \n",
    "    def fit(self, epochs, train_loader, valid_loader, save_path): \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "#         global N_EPOCH_EXPLICIT  #tbs later\n",
    "        for n_epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch: ', n_epoch)\n",
    "            N_EPOCH_EXPLICIT = n_epoch\n",
    "            train_loss, train_preds = self.train_epoch(train_loader)\n",
    "            valid_loss, valid_preds = self.valid_epoch(valid_loader)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            if isinstance(self.scheduler, ReduceLROnPlateau):\n",
    "                self.scheduler.step(valid_loss)\n",
    "            valid_score = get_score(self.valid_labels, valid_preds)\n",
    "\n",
    "            numbers = valid_score\n",
    "            filename = Config.model_output_folder+f'score_epoch_{n_epoch}.json'          \n",
    "            with open(filename, 'w') as file_object: \n",
    "                json.dump(numbers, file_object) \n",
    "            \n",
    "\n",
    "            if self.best_valid_score < valid_score:\n",
    "                self.best_valid_score = valid_score\n",
    "                self.save_model(n_epoch, save_path+f'best_model.pth', train_preds, valid_preds)\n",
    "\n",
    "            print('train_loss: ',train_loss)\n",
    "            print('valid_loss: ',valid_loss)\n",
    "            print('valid_score: ',valid_score)\n",
    "            print('best_valid_score: ',self.best_valid_score)\n",
    "            print('time used: ', time.time()-start_time)\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] epoch\": n_epoch+1, \n",
    "                      f\"[fold{self.fold}] avg_train_loss\": train_loss, \n",
    "                      f\"[fold{self.fold}] avg_val_loss\": valid_loss,\n",
    "                      f\"[fold{self.fold}] val_score\": valid_score})        \n",
    "\n",
    "        # fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "        # ax.plot(list(range(epochs)), train_losses, label=\"train_loss\")\n",
    "        # ax.plot(list(range(epochs)), valid_losses, label=\"val_loss\")\n",
    "        # fig.legend()\n",
    "        # plt.show()            \n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        if Config.amp:\n",
    "            scaler = GradScaler()\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        train_loss = 0\n",
    "        # preds = []\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            self.optimizer.zero_grad()\n",
    "            X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "            targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "            \n",
    "            if Config.use_mixup:\n",
    "                (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                    X, targets, Config.mixup_alpha\n",
    "                )\n",
    "                with autocast(enabled=False):\n",
    "                    outputs = self.model(X_mix).squeeze()\n",
    "                    loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                with autocast(enabled=False):\n",
    "                    outputs = self.model(X).squeeze()\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "\n",
    "                \n",
    "            if Config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / Config.gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "          \n",
    "            if (step) % Config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "\n",
    "            if (not isinstance(self.scheduler, ReduceLROnPlateau)):\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # preds.append(outputs.sigmoid().to('cpu').detach().numpy())\n",
    "            loss2 = loss.detach()\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] loss\": loss2,\n",
    "                       f\"[fold{self.fold}] lr\": self.scheduler.get_last_lr()[0]})            \n",
    "\n",
    "            # losses.append(loss2.item())\n",
    "            losses.append(loss2)\n",
    "            train_loss += loss2\n",
    "\n",
    "            if (step) % Config.print_num_steps == 0:\n",
    "                train_loss = train_loss.item() #synch once per print_num_steps instead of once per batch\n",
    "                print(f'[{step}/{len(train_loader)}] ', \n",
    "                      f'avg loss: ',train_loss/step,\n",
    "                      f'inst loss: ', loss2.item())\n",
    "                \n",
    "        # predictions = np.concatenate(preds)\n",
    "\n",
    "#         losses_avg = []\n",
    "#         for i, loss in enumerate(losses):\n",
    "#             if i == 0 :\n",
    "#                 losses_avg.append(loss)\n",
    "#             else:\n",
    "#                 losses_avg.append(losses_avg[-1] * 0.6 + loss * 0.4)\n",
    "#         losses = torch.stack(losses)\n",
    "#         losses_avg = torch.stack(losses_avg)\n",
    "#         fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "#         ax.plot(list(range(step)), losses, label=\"train_loss per step\")\n",
    "#         ax.plot(list(range(step)), losses_avg, label=\"train_loss_avg per step\")\n",
    "#         fig.legend()\n",
    "#         plt.show()            \n",
    "        \n",
    "        return train_loss / step, None#, predictions\n",
    "\n",
    "    def valid_epoch(self, valid_loader):\n",
    "        self.model.eval()      \n",
    "        valid_loss = []\n",
    "        preds = []\n",
    "        for step, batch in enumerate(valid_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "                targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                if Config.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / Config.gradient_accumulation_steps\n",
    "                valid_loss.append(loss.detach().item())\n",
    "                preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "#                 valid_loss.append(loss.detach())#.item())\n",
    "#                 preds.append(outputs.sigmoid())#.to('cpu').numpy())\n",
    "#         valid_loss = torch.cat(valid_loss).to('cpu').numpy()\n",
    "#         predictions = torch.cat(preds).to('cpu').numpy()\n",
    "        predictions = np.concatenate(preds)\n",
    "        return np.mean(valid_loss), predictions\n",
    "\n",
    "    def save_model(self, n_epoch, save_path, train_preds, valid_preds):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"best_valid_score\": self.best_valid_score,\n",
    "                \"n_epoch\": n_epoch,\n",
    "                'scheduler': self.scheduler.state_dict(),\n",
    "                'train_preds': train_preds,\n",
    "                'valid_preds': valid_preds,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0dfd0b-83ea-48f9-aca0-7e84b3346689",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f26270c-7a97-4d8a-80bd-db5ec6ad345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(seed=Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cce08b8-36a9-4fe2-b764-b612927f3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_PL(fold):\n",
    "#     up_thresh = Config.up_thresh\n",
    "#     down_thresh = Config.down_thresh\n",
    "#     pseudo_label_df = pd.read_csv(Config.pseudo_label_folder + f\"test_Fold_{fold}.csv\") \n",
    "#     pseudo_label_df.head()\n",
    "#     pseudo_label_df[\"target\"] = pseudo_label_df[f'preds_Fold_{fold}']#or adding tta\n",
    "#     num_test = pseudo_label_df.shape[0]\n",
    "#     num_yes = (pseudo_label_df[\"target\"] >= up_thresh).sum()\n",
    "#     num_no = (pseudo_label_df[\"target\"] <= down_thresh).sum()\n",
    "#     num_all = num_yes+num_no\n",
    "#     print(\"{:.2%} ratio, {:.2%} 1, {:.2%} 0\".format(num_all/num_test, num_yes/num_test, num_no/num_test))\n",
    "#     print(num_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae6c66a1-0cda-4447-b5c8-017af0d03874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Config.use_pseudo_label:\n",
    "#     for fold in Config.train_folds:\n",
    "#         check_PL(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aed9b-d467-49b5-9611-8a170be6d00a",
   "metadata": {},
   "source": [
    "## non-leaky PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14d354a8-d7e3-4bbb-85ee-f5e6a0126652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_PL(fold,up_thresh,down_thresh,train_df,test_df):\n",
    "    pseudo_label_df = pd.read_csv(Config.pseudo_label_folder + f\"test_Fold_{fold}.csv\") \n",
    "    \n",
    "    #soft labels\n",
    "    pseudo_label_df[\"target\"] = pseudo_label_df[f'preds_Fold_{fold}']\n",
    "    \n",
    "    #harden labels\n",
    "#     test_df_2 = pseudo_label_df[(pseudo_label_df[\"target\"] >= up_thresh) | (pseudo_label_df[\"target\"] <= down_thresh)].copy()\n",
    "#     test_df_2[\"target\"] = (test_df_2[\"target\"] >= up_thresh).astype(int)\n",
    "#     test_df_2 = test_df_2.merge(test_df[[\"id\",\"file_path\"]],on=\"id\",how=\"left\") #no need for this line if already has path\n",
    "    test_df_2 = pseudo_label_df.copy()\n",
    "    test_df_2['fold'] = Config.n_fold\n",
    "    PL_train_df = pd.concat([train_df, test_df_2]).reset_index(drop=True)\n",
    "    PL_train_df.reset_index(inplace=True, drop=True)\n",
    "#         display(train_df_PL.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True)))\n",
    "#         display(train_df_PL.shape)\n",
    "#         display(train_df_PL)\n",
    "    return PL_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdaf0da2-d079-4982-bb12-8ff2ea2751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_PL(fold,Config.up_thresh,Config.down_thresh,train_df.copy(),test_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1aff774-130a-4ba1-9132-5a7a60553992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_df, use_checkpoint=Config.use_checkpoint):\n",
    "    kf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "    avg_best_valid_score = 0\n",
    "    folds_val_score = []\n",
    "    original_train_df = train_df.copy()#for PL\n",
    "    for fold in range(Config.n_fold): \n",
    "        if Config.use_pseudo_label:\n",
    "            PL_train_df = generate_PL(fold,Config.up_thresh,Config.down_thresh,original_train_df.copy(),test_df)   \n",
    "            train_df = PL_train_df\n",
    "        train_index, valid_index = train_df.query(f\"fold!={fold}\").index, train_df.query(f\"fold=={fold}\").index #fold means fold_valid \n",
    "        print('Fold: ', fold)\n",
    "        if fold not in Config.train_folds:\n",
    "            print(\"skip\")\n",
    "            continue\n",
    "        train_X, valid_X = train_df.loc[train_index], train_df.loc[valid_index]\n",
    "        valid_labels = train_df.loc[valid_index,Config.target_col].values\n",
    "#         fold_indices = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv')#saved fold ids\n",
    "        oof = pd.DataFrame()\n",
    "        oof['id'] = train_df.loc[valid_index,'id']\n",
    "        oof['id'] = valid_X['id'].values.copy()\n",
    "        oof = oof.reset_index()\n",
    "        # assert oof['id'].eq(fold_indices['id']).all()\n",
    "#         if not Config.use_subset:\n",
    "#             assert oof['id'].eq(fold_indices['id']).sum()==112000\n",
    "        oof['target'] = valid_labels\n",
    "        \n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        # continue # uncomment this is to check oof ids\n",
    "\n",
    "        print('training data samples, val data samples: ', len(train_X) ,len(valid_X))\n",
    "        train_data_retriever = DataRetriever(train_X[\"file_path\"].values, train_X[\"target\"].values, transforms=train_transform)#how to run this only once and use for next experiment?\n",
    "        valid_data_retriever = DataRetrieverTest(valid_X[\"file_path\"].values, valid_X[\"target\"].values, transforms=test_transform)        \n",
    "        train_loader = DataLoader(train_data_retriever,\n",
    "                                  batch_size=Config.batch_size, \n",
    "                                  shuffle=True, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "        valid_loader = DataLoader(valid_data_retriever, \n",
    "                                  batch_size=Config.batch_size * 2, \n",
    "                                  shuffle=False, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        model = Model()\n",
    "        model.to(device,non_blocking=Config.non_blocking)\n",
    "        optimizer = AdamW(model.parameters(), lr=Config.lr,eps=1e-04, weight_decay=Config.weight_decay, amsgrad=False) #eps to avoid NaN/Inf in training loss\n",
    "        scheduler = get_scheduler(optimizer, len(train_X))\n",
    "        best_valid_score = -np.inf\n",
    "        if use_checkpoint:\n",
    "            print(\"Load Checkpoint, epo\")\n",
    "            checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_valid_score = float(checkpoint['best_valid_score'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        \n",
    "        \n",
    "        criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "        \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model, \n",
    "            device, \n",
    "            optimizer, \n",
    "            criterion,\n",
    "            scheduler,\n",
    "            valid_labels,\n",
    "            best_valid_score,\n",
    "            fold\n",
    "        )\n",
    "\n",
    "        history = trainer.fit(\n",
    "            epochs=Config.epochs, \n",
    "            train_loader=train_loader, \n",
    "            valid_loader=valid_loader,\n",
    "            save_path=f'{Config.model_output_folder}/Fold_{fold}_',\n",
    "        )\n",
    "        folds_val_score.append(trainer.best_valid_score)\n",
    "        del train_data_retriever\n",
    "    wandb.finish()\n",
    "    print('folds score:', folds_val_score)\n",
    "    print(\"Avg: {:.5f}\".format(np.mean(folds_val_score)))\n",
    "    print(\"Std: {:.5f}\".format(np.std(folds_val_score)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36681e6-4f7d-4d6c-b54f-57d800d63016",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d285326b-438d-40af-a1db-eb618cf145c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaggle_go\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"1b0833b15e81d54fad9cfbbe3d923f57562a6f89\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d12bd58-7094-43db-85b9-9071b0fecfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">122nd_V2_PL_6ep_2em3lr_32ch_vf+gn+sc01+tm+ts</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kaggle_go/G2Net\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/1hwi2klo\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/1hwi2klo</a><br/>\n",
       "                Run data is saved locally in <code>/home/wandb/run-20210926_094722-1hwi2klo</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_type= \"debug\" if Config.debug else \"train\"\n",
    "# run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=job_type)\n",
    "run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=Config.model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f8adc-4368-48be-8015-cf9e1e9719ab",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3560abc6-3e87-4021-84e8-abddfd72130a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "training data samples, val data samples:  674000 112000\n",
      "ModelIafossV2\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.5003572736467634 inst loss:  0.3867655098438263\n",
      "[700/2633]  avg loss:  0.46284493582589287 inst loss:  0.4021603763103485\n",
      "[1050/2633]  avg loss:  0.44633480980282736 inst loss:  0.39791756868362427\n",
      "[1400/2633]  avg loss:  0.43650111607142855 inst loss:  0.4234142303466797\n",
      "[1750/2633]  avg loss:  0.42990244838169644 inst loss:  0.3675483465194702\n",
      "[2100/2633]  avg loss:  0.4246531749906994 inst loss:  0.3821924924850464\n",
      "[2450/2633]  avg loss:  0.4208280353156888 inst loss:  0.40816813707351685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.4192, device='cuda:0')\n",
      "valid_loss:  0.4210255884960906\n",
      "valid_score:  0.8726379921993132\n",
      "best_valid_score:  0.8726379921993132\n",
      "time used:  555.6921803951263\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3915500749860491 inst loss:  0.40063267946243286\n",
      "[700/2633]  avg loss:  0.3910455758231027 inst loss:  0.3839007019996643\n",
      "[1050/2633]  avg loss:  0.3899938092912946 inst loss:  0.39141494035720825\n",
      "[1400/2633]  avg loss:  0.38938995361328127 inst loss:  0.3739362359046936\n",
      "[1750/2633]  avg loss:  0.38967229352678573 inst loss:  0.4345611333847046\n",
      "[2100/2633]  avg loss:  0.3891989571707589 inst loss:  0.3616589903831482\n",
      "[2450/2633]  avg loss:  0.38900206273915816 inst loss:  0.364124596118927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3889, device='cuda:0')\n",
      "valid_loss:  0.4072287877400716\n",
      "valid_score:  0.877717749053788\n",
      "best_valid_score:  0.877717749053788\n",
      "time used:  551.9084038734436\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3835544695172991 inst loss:  0.41967469453811646\n",
      "[700/2633]  avg loss:  0.3832276262555804 inst loss:  0.3853771388530731\n",
      "[1050/2633]  avg loss:  0.38184517996651784 inst loss:  0.33456578850746155\n",
      "[1400/2633]  avg loss:  0.3816934640066964 inst loss:  0.33687546849250793\n",
      "[1750/2633]  avg loss:  0.38211056082589284 inst loss:  0.4150581955909729\n",
      "[2100/2633]  avg loss:  0.38181876046316965 inst loss:  0.38667622208595276\n",
      "[2450/2633]  avg loss:  0.3820782346141582 inst loss:  0.3668712079524994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3818, device='cuda:0')\n",
      "valid_loss:  0.41970706190148444\n",
      "valid_score:  0.8782191559728809\n",
      "best_valid_score:  0.8782191559728809\n",
      "time used:  551.9757137298584\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3770186070033482 inst loss:  0.32136213779449463\n",
      "[700/2633]  avg loss:  0.376459481375558 inst loss:  0.3548157811164856\n",
      "[1050/2633]  avg loss:  0.3765399460565476 inst loss:  0.4043453335762024\n",
      "[1400/2633]  avg loss:  0.37698272705078123 inst loss:  0.35492783784866333\n",
      "[1750/2633]  avg loss:  0.37637649972098214 inst loss:  0.36927664279937744\n",
      "[2100/2633]  avg loss:  0.375896490187872 inst loss:  0.3647564947605133\n",
      "[2450/2633]  avg loss:  0.3756258221061862 inst loss:  0.3936469554901123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3756, device='cuda:0')\n",
      "valid_loss:  0.39936323239378735\n",
      "valid_score:  0.8805719918979944\n",
      "best_valid_score:  0.8805719918979944\n",
      "time used:  551.9347636699677\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3707480294363839 inst loss:  0.38554972410202026\n",
      "[700/2633]  avg loss:  0.3718447004045759 inst loss:  0.41254034638404846\n",
      "[1050/2633]  avg loss:  0.37057471865699404 inst loss:  0.40058332681655884\n",
      "[1400/2633]  avg loss:  0.37071602957589284 inst loss:  0.37858814001083374\n",
      "[1750/2633]  avg loss:  0.3707045200892857 inst loss:  0.3524106442928314\n",
      "[2100/2633]  avg loss:  0.3706926037016369 inst loss:  0.38949307799339294\n",
      "[2450/2633]  avg loss:  0.37049396125637757 inst loss:  0.375670850276947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3703, device='cuda:0')\n",
      "valid_loss:  0.4090315822052629\n",
      "valid_score:  0.8812905609798948\n",
      "best_valid_score:  0.8812905609798948\n",
      "time used:  551.5274102687836\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3664559064592634 inst loss:  0.35711562633514404\n",
      "[700/2633]  avg loss:  0.36617793491908485 inst loss:  0.3314417600631714\n",
      "[1050/2633]  avg loss:  0.36582754952566965 inst loss:  0.4140022397041321\n",
      "[1400/2633]  avg loss:  0.36547326224190846 inst loss:  0.3363601565361023\n",
      "[1750/2633]  avg loss:  0.36611356026785713 inst loss:  0.34666863083839417\n",
      "[2100/2633]  avg loss:  0.36613871256510416 inst loss:  0.38644540309906006\n",
      "[2450/2633]  avg loss:  0.36595237264827807 inst loss:  0.3590589165687561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3661, device='cuda:0')\n",
      "valid_loss:  0.4188992764851818\n",
      "valid_score:  0.8813972652276353\n",
      "best_valid_score:  0.8813972652276353\n",
      "time used:  551.6686499118805\n",
      "Fold:  1\n",
      "training data samples, val data samples:  674000 112000\n",
      "ModelIafossV2\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.5008421543666295 inst loss:  0.4362630248069763\n",
      "[700/2633]  avg loss:  0.4611658150809152 inst loss:  0.40896642208099365\n",
      "[1050/2633]  avg loss:  0.4445051792689732 inst loss:  0.40314722061157227\n",
      "[1400/2633]  avg loss:  0.433914315359933 inst loss:  0.3973780572414398\n",
      "[1750/2633]  avg loss:  0.42751143973214284 inst loss:  0.447491854429245\n",
      "[2100/2633]  avg loss:  0.42189482189360117 inst loss:  0.37994492053985596\n",
      "[2450/2633]  avg loss:  0.41788434709821426 inst loss:  0.3529663681983948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.4160, device='cuda:0')\n",
      "valid_loss:  0.4304323652291407\n",
      "valid_score:  0.8717763291990308\n",
      "best_valid_score:  0.8717763291990308\n",
      "time used:  551.9198005199432\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.39001469203404016 inst loss:  0.42764195799827576\n",
      "[700/2633]  avg loss:  0.3878819056919643 inst loss:  0.3941894769668579\n",
      "[1050/2633]  avg loss:  0.3870373825799851 inst loss:  0.37149572372436523\n",
      "[1400/2633]  avg loss:  0.38661433628627234 inst loss:  0.38721713423728943\n",
      "[1750/2633]  avg loss:  0.38637949916294645 inst loss:  0.42434296011924744\n",
      "[2100/2633]  avg loss:  0.3859879847935268 inst loss:  0.3697793781757355\n",
      "[2450/2633]  avg loss:  0.3857521524234694 inst loss:  0.37732404470443726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3855, device='cuda:0')\n",
      "valid_loss:  0.42271281744791495\n",
      "valid_score:  0.8755655681157052\n",
      "best_valid_score:  0.8755655681157052\n",
      "time used:  551.8290421962738\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3769676862444196 inst loss:  0.3692851960659027\n",
      "[700/2633]  avg loss:  0.3794281877790179 inst loss:  0.34652501344680786\n",
      "[1050/2633]  avg loss:  0.37930914015997025 inst loss:  0.3981020450592041\n",
      "[1400/2633]  avg loss:  0.37918116978236605 inst loss:  0.3789381980895996\n",
      "[1750/2633]  avg loss:  0.37890053013392855 inst loss:  0.41765761375427246\n",
      "[2100/2633]  avg loss:  0.37893330891927085 inst loss:  0.37240177392959595\n",
      "[2450/2633]  avg loss:  0.3790084153778699 inst loss:  0.41850441694259644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3789, device='cuda:0')\n",
      "valid_loss:  0.41091725393517375\n",
      "valid_score:  0.8780997100151605\n",
      "best_valid_score:  0.8780997100151605\n",
      "time used:  551.9366595745087\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3741966029575893 inst loss:  0.3893216848373413\n",
      "[700/2633]  avg loss:  0.37417977469308034 inst loss:  0.37590712308883667\n",
      "[1050/2633]  avg loss:  0.37366678873697917 inst loss:  0.35213667154312134\n",
      "[1400/2633]  avg loss:  0.37305511474609376 inst loss:  0.37422770261764526\n",
      "[1750/2633]  avg loss:  0.3729835030691964 inst loss:  0.39322662353515625\n",
      "[2100/2633]  avg loss:  0.3729292515345982 inst loss:  0.3615248501300812\n",
      "[2450/2633]  avg loss:  0.37311782525510206 inst loss:  0.36144834756851196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3730, device='cuda:0')\n",
      "valid_loss:  0.40574361118551805\n",
      "valid_score:  0.8797872727800718\n",
      "best_valid_score:  0.8797872727800718\n",
      "time used:  551.903558254242\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.36708657400948663 inst loss:  0.3822900056838989\n",
      "[700/2633]  avg loss:  0.367993643624442 inst loss:  0.4197021722793579\n",
      "[1050/2633]  avg loss:  0.36853838239397324 inst loss:  0.4079897999763489\n",
      "[1400/2633]  avg loss:  0.36815992082868304 inst loss:  0.34302210807800293\n",
      "[1750/2633]  avg loss:  0.36776708984375 inst loss:  0.3374071717262268\n",
      "[2100/2633]  avg loss:  0.36754586356026786 inst loss:  0.36652278900146484\n",
      "[2450/2633]  avg loss:  0.3676153938137755 inst loss:  0.36171936988830566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3678, device='cuda:0')\n",
      "valid_loss:  0.4177906952764346\n",
      "valid_score:  0.8798625478166644\n",
      "best_valid_score:  0.8798625478166644\n",
      "time used:  551.7076375484467\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.36376085553850446 inst loss:  0.35493358969688416\n",
      "[700/2633]  avg loss:  0.36294767107282366 inst loss:  0.37041395902633667\n",
      "[1050/2633]  avg loss:  0.3633499581473214 inst loss:  0.3282294273376465\n",
      "[1400/2633]  avg loss:  0.3639602225167411 inst loss:  0.31495606899261475\n",
      "[1750/2633]  avg loss:  0.3640554896763393 inst loss:  0.36666715145111084\n",
      "[2100/2633]  avg loss:  0.36399512881324403 inst loss:  0.35110026597976685\n",
      "[2450/2633]  avg loss:  0.3637737663424745 inst loss:  0.32748928666114807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3638, device='cuda:0')\n",
      "valid_loss:  0.4183265915200046\n",
      "valid_score:  0.8801770258337681\n",
      "best_valid_score:  0.8801770258337681\n",
      "time used:  551.8627302646637\n",
      "Fold:  2\n",
      "training data samples, val data samples:  674000 112000\n",
      "ModelIafossV2\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.5009562901088169 inst loss:  0.4281560182571411\n",
      "[700/2633]  avg loss:  0.46416285923549105 inst loss:  0.3850356638431549\n",
      "[1050/2633]  avg loss:  0.44866725376674105 inst loss:  0.39652740955352783\n",
      "[1400/2633]  avg loss:  0.43864946637834823 inst loss:  0.3731676936149597\n",
      "[1750/2633]  avg loss:  0.4321004813058036 inst loss:  0.43238964676856995\n",
      "[2100/2633]  avg loss:  0.4267306954520089 inst loss:  0.3767877221107483\n",
      "[2450/2633]  avg loss:  0.42243986168686226 inst loss:  0.42954570055007935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.4205, device='cuda:0')\n",
      "valid_loss:  0.41036402675659145\n",
      "valid_score:  0.8726640899369138\n",
      "best_valid_score:  0.8726640899369138\n",
      "time used:  551.7233927249908\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.39173025948660717 inst loss:  0.4278798997402191\n",
      "[700/2633]  avg loss:  0.3903182111467634 inst loss:  0.38231784105300903\n",
      "[1050/2633]  avg loss:  0.39004972912016367 inst loss:  0.38689467310905457\n",
      "[1400/2633]  avg loss:  0.38889007568359374 inst loss:  0.3615129590034485\n",
      "[1750/2633]  avg loss:  0.3884808872767857 inst loss:  0.46252351999282837\n",
      "[2100/2633]  avg loss:  0.38827831449962796 inst loss:  0.4021753668785095\n",
      "[2450/2633]  avg loss:  0.38818730568399235 inst loss:  0.40438705682754517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3882, device='cuda:0')\n",
      "valid_loss:  0.4271347192052293\n",
      "valid_score:  0.8761379772713787\n",
      "best_valid_score:  0.8761379772713787\n",
      "time used:  551.9645791053772\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.38340868268694195 inst loss:  0.37794455885887146\n",
      "[700/2633]  avg loss:  0.38157906668526786 inst loss:  0.3909333050251007\n",
      "[1050/2633]  avg loss:  0.38169628324962795 inst loss:  0.4115670621395111\n",
      "[1400/2633]  avg loss:  0.3814861624581473 inst loss:  0.38982653617858887\n",
      "[1750/2633]  avg loss:  0.380893310546875 inst loss:  0.3455107808113098\n",
      "[2100/2633]  avg loss:  0.3809172712053571 inst loss:  0.4252614974975586\n",
      "[2450/2633]  avg loss:  0.3804091348453444 inst loss:  0.36253058910369873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3807, device='cuda:0')\n",
      "valid_loss:  0.41090325174266346\n",
      "valid_score:  0.8785511362232216\n",
      "best_valid_score:  0.8785511362232216\n",
      "time used:  551.7301068305969\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3774117170061384 inst loss:  0.3745355010032654\n",
      "[700/2633]  avg loss:  0.3761586216517857 inst loss:  0.38149741291999817\n",
      "[1050/2633]  avg loss:  0.3756930687313988 inst loss:  0.36314475536346436\n",
      "[1400/2633]  avg loss:  0.37542149135044645 inst loss:  0.32907742261886597\n",
      "[1750/2633]  avg loss:  0.37511962890625 inst loss:  0.4269590973854065\n",
      "[2100/2633]  avg loss:  0.3748561314174107 inst loss:  0.386328786611557\n",
      "[2450/2633]  avg loss:  0.3748662209024235 inst loss:  0.36362236738204956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3749, device='cuda:0')\n",
      "valid_loss:  0.40826823803932155\n",
      "valid_score:  0.8798033973027634\n",
      "best_valid_score:  0.8798033973027634\n",
      "time used:  551.8555850982666\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3681913103376116 inst loss:  0.34383177757263184\n",
      "[700/2633]  avg loss:  0.36831063406808034 inst loss:  0.3717655539512634\n",
      "[1050/2633]  avg loss:  0.36968392508370534 inst loss:  0.37861308455467224\n",
      "[1400/2633]  avg loss:  0.3696057564871652 inst loss:  0.394997775554657\n",
      "[1750/2633]  avg loss:  0.3696788853236607 inst loss:  0.34438958764076233\n",
      "[2100/2633]  avg loss:  0.36949640183221727 inst loss:  0.3540642261505127\n",
      "[2450/2633]  avg loss:  0.3693297343351403 inst loss:  0.33006054162979126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3691, device='cuda:0')\n",
      "valid_loss:  0.4437042105959975\n",
      "valid_score:  0.8799002882526966\n",
      "best_valid_score:  0.8799002882526966\n",
      "time used:  551.951996088028\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.363199462890625 inst loss:  0.3259555697441101\n",
      "[700/2633]  avg loss:  0.3640176391601562 inst loss:  0.36516469717025757\n",
      "[1050/2633]  avg loss:  0.36455278669084823 inst loss:  0.37630006670951843\n",
      "[1400/2633]  avg loss:  0.3650522286551339 inst loss:  0.41254594922065735\n",
      "[1750/2633]  avg loss:  0.365106689453125 inst loss:  0.4395408630371094\n",
      "[2100/2633]  avg loss:  0.3650131370907738 inst loss:  0.3566322326660156\n",
      "[2450/2633]  avg loss:  0.36490757533482143 inst loss:  0.3636917471885681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3648, device='cuda:0')\n",
      "valid_loss:  0.4084599493028911\n",
      "valid_score:  0.880162177937432\n",
      "best_valid_score:  0.880162177937432\n",
      "time used:  551.6552650928497\n",
      "Fold:  3\n",
      "training data samples, val data samples:  674000 112000\n",
      "ModelIafossV2\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.4991909354073661 inst loss:  0.39968013763427734\n",
      "[700/2633]  avg loss:  0.46090305873325893 inst loss:  0.39155441522598267\n",
      "[1050/2633]  avg loss:  0.444290771484375 inst loss:  0.3797740936279297\n",
      "[1400/2633]  avg loss:  0.433947274344308 inst loss:  0.39508670568466187\n",
      "[1750/2633]  avg loss:  0.42779031808035717 inst loss:  0.4130452871322632\n",
      "[2100/2633]  avg loss:  0.42283961704799106 inst loss:  0.32450687885284424\n",
      "[2450/2633]  avg loss:  0.41860157246492347 inst loss:  0.3833997845649719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.4171, device='cuda:0')\n",
      "valid_loss:  0.44282456116589236\n",
      "valid_score:  0.8740166309972129\n",
      "best_valid_score:  0.8740166309972129\n",
      "time used:  551.7601633071899\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3894011579241071 inst loss:  0.41626235842704773\n",
      "[700/2633]  avg loss:  0.3877301025390625 inst loss:  0.36799368262290955\n",
      "[1050/2633]  avg loss:  0.3882068161737351 inst loss:  0.3609442412853241\n",
      "[1400/2633]  avg loss:  0.38767163957868306 inst loss:  0.36396634578704834\n",
      "[1750/2633]  avg loss:  0.3870615234375 inst loss:  0.36946234107017517\n",
      "[2100/2633]  avg loss:  0.38658354259672617 inst loss:  0.3926631212234497\n",
      "[2450/2633]  avg loss:  0.3861304956552934 inst loss:  0.35711199045181274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3862, device='cuda:0')\n",
      "valid_loss:  0.405308982961254\n",
      "valid_score:  0.876700425329491\n",
      "best_valid_score:  0.876700425329491\n",
      "time used:  551.8385701179504\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.38006282261439733 inst loss:  0.42001470923423767\n",
      "[700/2633]  avg loss:  0.3802676827566964 inst loss:  0.37239861488342285\n",
      "[1050/2633]  avg loss:  0.38003409249441966 inst loss:  0.38411951065063477\n",
      "[1400/2633]  avg loss:  0.37981087820870535 inst loss:  0.4186553955078125\n",
      "[1750/2633]  avg loss:  0.37978585379464286 inst loss:  0.3850126266479492\n",
      "[2100/2633]  avg loss:  0.37989141555059525 inst loss:  0.35987555980682373\n",
      "[2450/2633]  avg loss:  0.37978126992984695 inst loss:  0.35233598947525024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3796, device='cuda:0')\n",
      "valid_loss:  0.4047781712116172\n",
      "valid_score:  0.8788436810438015\n",
      "best_valid_score:  0.8788436810438015\n",
      "time used:  551.6220486164093\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.37524758475167413 inst loss:  0.42510291934013367\n",
      "[700/2633]  avg loss:  0.37477308000837056 inst loss:  0.34067875146865845\n",
      "[1050/2633]  avg loss:  0.37430274600074404 inst loss:  0.3184123635292053\n",
      "[1400/2633]  avg loss:  0.37450469970703126 inst loss:  0.3617323040962219\n",
      "[1750/2633]  avg loss:  0.37465638950892854 inst loss:  0.35331130027770996\n",
      "[2100/2633]  avg loss:  0.3742342122395833 inst loss:  0.3563998341560364\n",
      "[2450/2633]  avg loss:  0.3737203294403699 inst loss:  0.3965831696987152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3737, device='cuda:0')\n",
      "valid_loss:  0.3996483856412374\n",
      "valid_score:  0.8813569669151676\n",
      "best_valid_score:  0.8813569669151676\n",
      "time used:  551.7905442714691\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.36651123046875 inst loss:  0.3820420503616333\n",
      "[700/2633]  avg loss:  0.36864632742745534 inst loss:  0.35858261585235596\n",
      "[1050/2633]  avg loss:  0.3677800060453869 inst loss:  0.3708727955818176\n",
      "[1400/2633]  avg loss:  0.36713535853794643 inst loss:  0.3744082450866699\n",
      "[1750/2633]  avg loss:  0.3674744349888393 inst loss:  0.406375527381897\n",
      "[2100/2633]  avg loss:  0.3679822067987351 inst loss:  0.38409942388534546\n",
      "[2450/2633]  avg loss:  0.3681454779177296 inst loss:  0.3629993796348572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3682, device='cuda:0')\n",
      "valid_loss:  0.41204402776069293\n",
      "valid_score:  0.8815293331568078\n",
      "best_valid_score:  0.8815293331568078\n",
      "time used:  552.2707071304321\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3659344046456473 inst loss:  0.4010641574859619\n",
      "[700/2633]  avg loss:  0.3650195094517299 inst loss:  0.32591596245765686\n",
      "[1050/2633]  avg loss:  0.36440528506324404 inst loss:  0.37887319922447205\n",
      "[1400/2633]  avg loss:  0.364428231375558 inst loss:  0.3611024022102356\n",
      "[1750/2633]  avg loss:  0.36436226981026787 inst loss:  0.33173203468322754\n",
      "[2100/2633]  avg loss:  0.3643360828218006 inst loss:  0.34491151571273804\n",
      "[2450/2633]  avg loss:  0.3643210399394133 inst loss:  0.40423670411109924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3641, device='cuda:0')\n",
      "valid_loss:  0.4056899685021405\n",
      "valid_score:  0.8814289952997715\n",
      "best_valid_score:  0.8815293331568078\n",
      "time used:  552.1954417228699\n",
      "Fold:  4\n",
      "training data samples, val data samples:  674000 112000\n",
      "ModelIafossV2\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.5024041312081473 inst loss:  0.45513904094696045\n",
      "[700/2633]  avg loss:  0.4675147356305804 inst loss:  0.3932739496231079\n",
      "[1050/2633]  avg loss:  0.4518134416852679 inst loss:  0.4255344271659851\n",
      "[1400/2633]  avg loss:  0.44241707938058034 inst loss:  0.3973941206932068\n",
      "[1750/2633]  avg loss:  0.43538295200892857 inst loss:  0.38236677646636963\n",
      "[2100/2633]  avg loss:  0.43014488583519345 inst loss:  0.4166404604911804\n",
      "[2450/2633]  avg loss:  0.42638338050063773 inst loss:  0.3421199917793274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.4250, device='cuda:0')\n",
      "valid_loss:  0.41434986773691224\n",
      "valid_score:  0.8726566511609386\n",
      "best_valid_score:  0.8726566511609386\n",
      "time used:  552.3950870037079\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.39652321951729913 inst loss:  0.3531532883644104\n",
      "[700/2633]  avg loss:  0.39613006591796873 inst loss:  0.41478291153907776\n",
      "[1050/2633]  avg loss:  0.3963590785435268 inst loss:  0.36504971981048584\n",
      "[1400/2633]  avg loss:  0.3960298810686384 inst loss:  0.41520994901657104\n",
      "[1750/2633]  avg loss:  0.3958601771763393 inst loss:  0.3526204526424408\n",
      "[2100/2633]  avg loss:  0.3952843075706845 inst loss:  0.35186877846717834\n",
      "[2450/2633]  avg loss:  0.39477152921715564 inst loss:  0.3801906704902649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3944, device='cuda:0')\n",
      "valid_loss:  0.412165055127993\n",
      "valid_score:  0.8761281174174461\n",
      "best_valid_score:  0.8761281174174461\n",
      "time used:  552.4149925708771\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3892323521205357 inst loss:  0.42084813117980957\n",
      "[700/2633]  avg loss:  0.38807525634765627 inst loss:  0.38841745257377625\n",
      "[1050/2633]  avg loss:  0.38841817220052083 inst loss:  0.38143402338027954\n",
      "[1400/2633]  avg loss:  0.3884298270089286 inst loss:  0.3873397409915924\n",
      "[1750/2633]  avg loss:  0.38800599888392856 inst loss:  0.39924389123916626\n",
      "[2100/2633]  avg loss:  0.387843017578125 inst loss:  0.36507105827331543\n",
      "[2450/2633]  avg loss:  0.38735147281568877 inst loss:  0.35237860679626465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3872, device='cuda:0')\n",
      "valid_loss:  0.4081032805519017\n",
      "valid_score:  0.8786730332333912\n",
      "best_valid_score:  0.8786730332333912\n",
      "time used:  552.6156754493713\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.382769034249442 inst loss:  0.4073895215988159\n",
      "[700/2633]  avg loss:  0.3828741455078125 inst loss:  0.3565487265586853\n",
      "[1050/2633]  avg loss:  0.38218302408854166 inst loss:  0.38404226303100586\n",
      "[1400/2633]  avg loss:  0.38150020054408484 inst loss:  0.3558756709098816\n",
      "[1750/2633]  avg loss:  0.38183586774553574 inst loss:  0.40900176763534546\n",
      "[2100/2633]  avg loss:  0.3816589064825149 inst loss:  0.3590885400772095\n",
      "[2450/2633]  avg loss:  0.3815655642139668 inst loss:  0.3549858331680298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3812, device='cuda:0')\n",
      "valid_loss:  0.44296999854039926\n",
      "valid_score:  0.8795349505385365\n",
      "best_valid_score:  0.8795349505385365\n",
      "time used:  552.5400593280792\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3759337071010045 inst loss:  0.4100154638290405\n",
      "[700/2633]  avg loss:  0.37581272670200894 inst loss:  0.3749558925628662\n",
      "[1050/2633]  avg loss:  0.3757036481584821 inst loss:  0.41346973180770874\n",
      "[1400/2633]  avg loss:  0.37567108154296874 inst loss:  0.32801568508148193\n",
      "[1750/2633]  avg loss:  0.3761580636160714 inst loss:  0.3752742111682892\n",
      "[2100/2633]  avg loss:  0.37574180966331844 inst loss:  0.3221029043197632\n",
      "[2450/2633]  avg loss:  0.37544747488839286 inst loss:  0.3803778290748596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3756, device='cuda:0')\n",
      "valid_loss:  0.4010704608812724\n",
      "valid_score:  0.8800290792365113\n",
      "best_valid_score:  0.8800290792365113\n",
      "time used:  552.5239539146423\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3716849626813616 inst loss:  0.4024779200553894\n",
      "[700/2633]  avg loss:  0.37182124546595985 inst loss:  0.3824573755264282\n",
      "[1050/2633]  avg loss:  0.3717970493861607 inst loss:  0.44455885887145996\n",
      "[1400/2633]  avg loss:  0.37132886614118304 inst loss:  0.43574798107147217\n",
      "[1750/2633]  avg loss:  0.37165182059151786 inst loss:  0.40014737844467163\n",
      "[2100/2633]  avg loss:  0.3716146995907738 inst loss:  0.3725897967815399\n",
      "[2450/2633]  avg loss:  0.37159974390146683 inst loss:  0.36302995681762695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3713, device='cuda:0')\n",
      "valid_loss:  0.4042333847583701\n",
      "valid_score:  0.8801151519459725\n",
      "best_valid_score:  0.8801151519459725\n",
      "time used:  552.6496922969818\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1021<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/wandb/run-20210926_094722-1hwi2klo/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/wandb/run-20210926_094722-1hwi2klo/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold0] avg_train_loss</td><td>0.36611</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.4189</td></tr><tr><td>[fold0] epoch</td><td>6</td></tr><tr><td>[fold0] loss</td><td>0.29943</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] val_score</td><td>0.8814</td></tr><tr><td>[fold1] avg_train_loss</td><td>0.36382</td></tr><tr><td>[fold1] avg_val_loss</td><td>0.41833</td></tr><tr><td>[fold1] epoch</td><td>6</td></tr><tr><td>[fold1] loss</td><td>0.31995</td></tr><tr><td>[fold1] lr</td><td>0.0</td></tr><tr><td>[fold1] val_score</td><td>0.88018</td></tr><tr><td>[fold2] avg_train_loss</td><td>0.36479</td></tr><tr><td>[fold2] avg_val_loss</td><td>0.40846</td></tr><tr><td>[fold2] epoch</td><td>6</td></tr><tr><td>[fold2] loss</td><td>0.40025</td></tr><tr><td>[fold2] lr</td><td>0.0</td></tr><tr><td>[fold2] val_score</td><td>0.88016</td></tr><tr><td>[fold3] avg_train_loss</td><td>0.36407</td></tr><tr><td>[fold3] avg_val_loss</td><td>0.40569</td></tr><tr><td>[fold3] epoch</td><td>6</td></tr><tr><td>[fold3] loss</td><td>0.34886</td></tr><tr><td>[fold3] lr</td><td>0.0</td></tr><tr><td>[fold3] val_score</td><td>0.88143</td></tr><tr><td>[fold4] avg_train_loss</td><td>0.37131</td></tr><tr><td>[fold4] avg_val_loss</td><td>0.40423</td></tr><tr><td>[fold4] epoch</td><td>6</td></tr><tr><td>[fold4] loss</td><td>0.43264</td></tr><tr><td>[fold4] lr</td><td>0.0</td></tr><tr><td>[fold4] val_score</td><td>0.88012</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold0] avg_train_loss</td><td>█▄▃▂▂▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>█▄█▁▄▇</td></tr><tr><td>[fold0] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold0] loss</td><td>█▄▆▆▄▄▃▆▄▂█▂▅▃▄▅▅▆▅▄▂▄▅▅▃▃▄▃▃▃▂▄▁▅▃▃▂▁▁▅</td></tr><tr><td>[fold0] lr</td><td>▂▃▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold0] val_score</td><td>▁▅▅▇██</td></tr><tr><td>[fold1] avg_train_loss</td><td>█▄▃▂▂▁</td></tr><tr><td>[fold1] avg_val_loss</td><td>█▆▂▁▄▅</td></tr><tr><td>[fold1] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold1] loss</td><td>█▇▄▃▃▃▁▆▄▅▄▄▂▄▅▃▄▂▃▄▄▃▂▄▃▃▃▄▂▁▁▃▄▁▃▃▃▄▂▂</td></tr><tr><td>[fold1] lr</td><td>▂▃▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold1] val_score</td><td>▁▄▆███</td></tr><tr><td>[fold2] avg_train_loss</td><td>█▄▃▂▂▁</td></tr><tr><td>[fold2] avg_val_loss</td><td>▁▅▂▁█▁</td></tr><tr><td>[fold2] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold2] loss</td><td>█▆▇▄▄▄▅▆▄▃▇▃▄▄▃▂▄▄▃▄▅▆▅▄▃▃▃▄▃▁▄▆▃▃▄▃▄▃▄▂</td></tr><tr><td>[fold2] lr</td><td>▂▃▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold2] val_score</td><td>▁▄▆███</td></tr><tr><td>[fold3] avg_train_loss</td><td>█▄▃▂▂▁</td></tr><tr><td>[fold3] avg_val_loss</td><td>█▂▂▁▃▂</td></tr><tr><td>[fold3] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold3] loss</td><td>█▅▅▅▄▄▅▆▄▅▄▃▄▄▄▄▃▃▁▂▄▃▅▄▃▅▅▂▃▁▂▂▁▂▃▂▂▃▃▃</td></tr><tr><td>[fold3] lr</td><td>▂▃▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold3] val_score</td><td>▁▄▅███</td></tr><tr><td>[fold4] avg_train_loss</td><td>█▄▃▂▂▁</td></tr><tr><td>[fold4] avg_val_loss</td><td>▃▃▂█▁▂</td></tr><tr><td>[fold4] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold4] loss</td><td>██▆▆▇▆▅▅▅▆▇▆▃▅▄▄▄▃▆▅▇▅▆▄▆▅▆▃▄▄▅▃▅▂▄▅▅▄▃▁</td></tr><tr><td>[fold4] lr</td><td>▂▃▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold4] val_score</td><td>▁▄▇▇██</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">122nd_V2_PL_6ep_2em3lr_32ch_vf+gn+sc01+tm+ts</strong>: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/1hwi2klo\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/1hwi2klo</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folds score: [0.8813972652276353, 0.8801770258337681, 0.880162177937432, 0.8815293331568078, 0.8801151519459725]\n",
      "Avg: 0.88068\n",
      "Std: 0.00064\n",
      "CPU times: user 4h 24min 8s, sys: 20min 19s, total: 4h 44min 27s\n",
      "Wall time: 4h 36min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "#     %lprun -f DataRetriever.__getitem__ -f Trainer.train_epoch -f Trainer.fit -f Trainer.valid_epoch training_loop() \n",
    "    training_loop(train_df,Config.use_checkpoint)\n",
    "except RuntimeError as e:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()   \n",
    "    print(e)# saving oof predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a03bfac-e977-4cc1-908f-fd91ad857903",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eec60e-00c2-43aa-9768-8fb74482de2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d39375ae-356c-4742-8f68-f020b8d71fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(Config.train_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce12376c-30df-443b-b057-3a54f4c75230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# import Ipython\n",
    "# IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccdc76b4-47ca-4622-a4c1-0763f96aaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c80731b-2b25-48c4-83bc-491572148207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jarviscloud import jarviscloud\n",
    "# jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b00fb94-6a9f-43f0-b73d-5738a7e48439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "successfully saved oof predictions for Fold:  0\n",
      "1\n",
      "successfully saved oof predictions for Fold:  1\n",
      "2\n",
      "successfully saved oof predictions for Fold:  2\n",
      "3\n",
      "successfully saved oof predictions for Fold:  3\n",
      "4\n",
      "successfully saved oof predictions for Fold:  4\n"
     ]
    }
   ],
   "source": [
    "for fold in Config.train_folds:\n",
    "    print(fold)\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    # print(checkpoint['valid_preds'])\n",
    "    try:\n",
    "        # oof = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv') also works, used in replacement of next statement for previously not generated Fold_{fold}_oof_pred.csv\n",
    "        oof = pd.read_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        oof['pred'] = checkpoint['valid_preds']\n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv') \n",
    "        print('successfully saved oof predictions for Fold: ', fold)   \n",
    "    except:\n",
    "        raise RuntimeError('failure in saving predictions for Fold: ', fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb59e1-9caf-4636-bb62-18c237d5c716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6afe6c-0be0-485b-a2e2-a899024b8a80",
   "metadata": {},
   "source": [
    "# add TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dafe5ee8-3a67-43fd-94f8-4f04984c9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e70a9794-653f-4c95-9108-547bf07f2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tbs need pythonic way\n",
    "class TTA(Dataset):\n",
    "    def __init__(self, paths, targets, vflip=False, shuffle_channels=False, time_shift=False, \n",
    "                 add_gaussian_noise = False,  time_stretch=False,shuffle01=False,timemask=False,\n",
    "                 shift_channel=False,reduce_SNR=False, ):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.vflip = vflip\n",
    "        self.shuffle_channels = shuffle_channels\n",
    "        self.time_shift = time_shift\n",
    "        self.add_gaussian_noise = add_gaussian_noise\n",
    "        self.time_stretch = time_stretch\n",
    "        self.shuffle01 = shuffle01\n",
    "        self.timemask = timemask\n",
    "        self.shift_channel = shift_channel\n",
    "        self.reduce_SNR = reduce_SNR\n",
    "        if time_shift:\n",
    "            self.time_shift = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096, \n",
    "                                      max_fraction=Config.time_shift_right*1.0/4096, p=1,rollover=False)\n",
    "        if add_gaussian_noise:\n",
    "            self.add_gaussian_noise = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude= 0.015*0.015, p=1)\n",
    "        if time_stretch:\n",
    "            self.time_stretch = A.TimeStretch(min_rate=0.9, max_rate=1.111,leave_length_unchanged=True, p=1)\n",
    "        if timemask:\n",
    "            self.timemask = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=1.0)\n",
    "\n",
    "              \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "        if self.vflip:\n",
    "            waves = -waves\n",
    "        if self.shuffle_channels:\n",
    "            np.random.shuffle(waves)\n",
    "        if self.time_shift:\n",
    "            waves = self.time_shift(waves, sample_rate=2048)\n",
    "        if self.add_gaussian_noise:\n",
    "            waves = self.add_gaussian_noise(waves, sample_rate=2048)\n",
    "        if self.time_stretch:\n",
    "            waves = self.time_stretch(waves, sample_rate=2048)\n",
    "        if self.shuffle01:\n",
    "            waves[[0,1]] = waves[[1,0]]\n",
    "        if self.timemask:\n",
    "            waves = self.timemask(waves, sample_rate=2048)\n",
    "        if self.shift_channel:\n",
    "            waves = shift_channel_func(waves, sample_rate=2048)\n",
    "        if self.reduce_SNR:\n",
    "            waves = reduce_SNR_func(waves, sample_rate=2048)\n",
    "        #snr, shift_channel tba\n",
    "        \n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device,             \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30c18af1-daaf-409d-bbe8-180ca6db1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e95c39f-c917-4d9b-afe8-dc89d5d8f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(loader,model):\n",
    "    preds = []\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        if step % Config.print_num_steps == 0:\n",
    "            print(\"step {}/{}\".format(step, len(loader)))\n",
    "        with torch.no_grad():\n",
    "            X = batch[0].to(device,non_blocking=Config.non_blocking)\n",
    "            outputs = model(X).squeeze()\n",
    "            preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "def get_tta_pred(df,model,**transforms):\n",
    "    data_retriever = TTA(df['file_path'].values, df['target'].values, **transforms)\n",
    "    loader = DataLoader(data_retriever, \n",
    "                            batch_size=Config.batch_size * 2, \n",
    "                            shuffle=False, \n",
    "                            num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "    return get_pred(loader,model)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f838287-0fa6-442d-b39d-f700b6ea483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['vflip', 'add_gaussian_noise', 'timemask', 'shuffle01', 'time_shift']\n"
     ]
    }
   ],
   "source": [
    "##TTA for oof\n",
    "print(conserv_transform_list_strings)\n",
    "print(aggressive_transform_list_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "709164c2-28c6-4eed-954e-3fc524bc9fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[()]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "conserv_transform_powerset = list(powerset(conserv_transform_list_strings))\n",
    "conserv_transform_powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7343696-bcac-4160-8a76-0d79c0dee4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "for transformations in conserv_transform_powerset:\n",
    "    print({transformation:True for transformation in transformations})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf5e8f7-1349-45fe-b646-42e0febed1c3",
   "metadata": {},
   "source": [
    "## generate oof tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09fa2a-7afc-4c7d-9fbe-fa02890d73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model()\n",
    "\n",
    "for fold in Config.train_folds:\n",
    "    print('Fold ',fold)\n",
    "    oof = train_df.query(f\"fold=={fold}\").copy()\n",
    "    oof['preds'] = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')['valid_preds']\n",
    "    oof['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "    # display(oof)    \n",
    "\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "    \n",
    "    for transformations in conserv_transform_powerset:\n",
    "#         print(transformations)\n",
    "        if transformations:#to avoid double count original\n",
    "            print(\"tta_\"+('_').join(transformations))\n",
    "            oof[\"tta_\"+('_').join(transformations)] = get_tta_pred(oof,model,**{transformation:True for transformation in transformations})\n",
    "        for aggr_transformation in aggressive_transform_list_strings:#tbs combination of conservative and aggressive\n",
    "            print(\"tta_\"+('_').join(transformations)+'_'+aggr_transformation)\n",
    "            oof[\"tta_\"+('_').join(transformations)+'_'+aggr_transformation] = get_tta_pred(oof,model,**{transformation:True for transformation in transformations}, **{aggr_transformation:True})\n",
    "               \n",
    "\n",
    "    oof.to_csv(Config.model_output_folder + f\"/oof_Fold_{fold}.csv\", index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd3fa0d4-3e28-48ea-bf7a-fa6c2df077f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_all = pd.DataFrame()\n",
    "for fold in Config.train_folds:\n",
    "    oof = pd.read_csv(Config.model_output_folder + f\"/oof_Fold_{fold}.csv\")\n",
    "    oof_all = pd.concat([oof_all,oof])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ca818-92aa-4734-8a20-874db80b6c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed70f5fd-00b8-4eb9-a003-6fec6de253d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('_').join(transformations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da1a3065-5238-4efa-8851-cb60c17bc878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 0.8801739879573027\n",
      "tta__vflip 0.8802155890695967\n",
      "tta__add_gaussian_noise 0.8801488065845556\n",
      "tta__timemask 0.8779151828990358\n",
      "tta__shuffle01 0.8796113994017328\n",
      "tta__time_shift 0.8787075903592754\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",roc_auc_score(oof_all['target'], oof_all['preds']))\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    if \"tta\" in col:\n",
    "        print(col,roc_auc_score(oof_all['target'], oof_all[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2757f84-5698-4982-8d06-c3058bc0f410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.880162177937432"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_sample = oof_all[oof_all['fold']==2]\n",
    "roc_auc_score(oof_sample['target'], oof_sample['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b324aba-a293-43a2-86e5-9fae454daa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f727009f8b0>, {'preds': 0.19999999999999996, 'tta__vflip': 0.20000000000000007, 'tta__add_gaussian_noise': 0.20000000000000007, 'tta__timemask': 0.16000000000000003, 'tta__shuffle01': 0.16000000000000003, 'tta__time_shift': 0.08000000000000002})\n",
      "preds 0.19999999999999996\n",
      "tta__vflip 0.20000000000000007\n",
      "tta__add_gaussian_noise 0.20000000000000007\n",
      "tta__timemask 0.16000000000000003\n",
      "tta__shuffle01 0.16000000000000003\n",
      "tta__time_shift 0.08000000000000002\n",
      "preds\n",
      "tta__vflip\n",
      "tta__add_gaussian_noise\n",
      "tta__timemask\n",
      "tta__shuffle01\n",
      "tta__time_shift\n",
      "preds_tta_avg: 0.880417961697041\n"
     ]
    }
   ],
   "source": [
    "oof_all['avg']=0\n",
    "total_weight = 0\n",
    "#weights leaky? not fine tuned\n",
    "\n",
    "oof_weight  = defaultdict(lambda :1)\n",
    "aggr_total_weight = 0\n",
    "for trans in aggressive_transform_list_strings:\n",
    "    aggr_total_weight += getattr(Config(),trans+'_weight')\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    \n",
    "    if 'tta_' in col or 'preds' in col: \n",
    "        for trans in conserv_transform_list_strings:\n",
    "            \n",
    "            if trans in col:\n",
    "                oof_weight[col] *= getattr(Config(),trans+'_proba')\n",
    "            else:\n",
    "                oof_weight[col] *= 1-getattr(Config(),trans+'_proba')\n",
    "            \n",
    "        flag = False\n",
    "        for trans in aggressive_transform_list_strings:\n",
    "            \n",
    "            if trans in col:\n",
    "                oof_weight[col] *= getattr(Config(),trans+'_weight')/aggr_total_weight*Config.aggressive_aug_proba\n",
    "                \n",
    "                flag = True\n",
    "        if not flag:\n",
    "            oof_weight[col] *= (1-Config.aggressive_aug_proba)\n",
    "        \n",
    "print(oof_weight)\n",
    "for key,value in oof_weight.items():\n",
    "    print(key,value)\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    if ('tta_' in col or 'preds' in col): # and 'time_shift' not in col and 'timemask' not in col\n",
    "        print(col)\n",
    "        total_weight+=oof_weight[col]\n",
    "        oof_all['avg'] += oof_all[col]*oof_weight[col]\n",
    "oof_all['avg'] /= total_weight\n",
    "\n",
    "print(\"preds_tta_avg:\",roc_auc_score(oof_all['target'], oof_all['avg']))\n",
    "\n",
    "oof_all.to_csv(Config.model_output_folder + \"/oof_all.csv\", index=False)\n",
    "oof_all[['id','fold','avg']].rename(columns={'id':'id','fold':'fold','avg':'prediction'}).to_csv(Config.model_output_folder + \"/oof_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a7c77-63b3-4efd-82e3-588ff224c583",
   "metadata": {},
   "source": [
    "## generate TTA for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a1c70-e352-435b-8767-3c397d4bd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__vflip_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__timemask_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__shuffle01_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__vflip_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__timemask_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__shuffle01_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__vflip_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__vflip_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__shuffle01_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__vflip_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__timemask_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "test_df['target'] = 0  \n",
    "model = Model()\n",
    "\n",
    "for fold in Config.train_folds:\n",
    "    test_df2 = test_df.copy()\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "\n",
    "    test_df2['preds'+f'_Fold_{fold}'] = get_tta_pred(test_df2,model)\n",
    "\n",
    "    for transformations in conserv_transform_powerset:\n",
    "#         print(transformations)\n",
    "        if transformations:#to avoid double count original\n",
    "            print(\"tta_\"+('_').join(transformations)+f'_Fold_{fold}')\n",
    "            test_df2[\"tta_\"+('_').join(transformations)+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,**{transformation:True for transformation in transformations})\n",
    "        for transformation in aggressive_transform_list_strings:#tbs combination of conservative and aggressive\n",
    "            print(\"tta_\"+('_').join(transformations)+'_'+transformation+f'_Fold_{fold}')\n",
    "            test_df2[\"tta_\"+('_').join(transformations)+'_'+transformation+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,**{transformation:True for transformation in transformations}, **{transformation:True})\n",
    "               \n",
    "    test_df2.to_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab6a61-d4eb-447e-ae1f-33b1af2958ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg = test_df[['id', 'target']].copy()\n",
    "test_avg['target'] = 0\n",
    "# print(test_avg.describe())\n",
    "\n",
    "total_weight = 0\n",
    "for fold in Config.train_folds:\n",
    "#     test_weight = {key+f'_Fold_{fold}':value for key,value in oof_weight.items()}\n",
    "    test_weight = oof_weight #defaultdict(lambda:1)\n",
    "    test_df2 = pd.read_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\")\n",
    "#     print(test_df2.describe())\n",
    "    for col in test_df2.columns:\n",
    "        col_weight = col.split('_Fold_')[0]\n",
    "        if ('tta_' in col or 'preds' in col): \n",
    "#             print(col)\n",
    "#             print(test_weight[col_weight])\n",
    "            total_weight+=test_weight[col_weight]\n",
    "            test_avg['target'] += test_df2[col]*test_weight[col_weight]\n",
    "test_avg['target'] /= total_weight\n",
    "print(test_avg.describe())\n",
    "print(test_avg[\"target\"].hist(bins=100))\n",
    "print(test_avg)\n",
    "# print(total_weight)\n",
    "test_avg.to_csv(Config.model_output_folder + \"/test_avg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd406d-d7fd-4958-8e73-9d2caa624c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a2c45-3493-49b5-a750-d4bb4b33303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg[['id', 'target']].to_csv(\"./submission.csv\", index=False)\n",
    "\n",
    "test_avg[['id', 'target']].to_csv(Config.model_output_folder + \"/submission.csv\", index=False)\n",
    "\n",
    "!mkdir -p ~/.kaggle/ && cp $Config.kaggle_json_path ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f74d3-2783-4014-9b37-a0d6997797ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c g2net-gravitational-wave-detection -f ./submission.csv -m $Config.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6966b-8273-46fa-bc42-ec36bf4b2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarviscloud import jarviscloud\n",
    "jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea4e9c-95c5-4aba-9930-e13842e2c583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "340px",
    "width": "209.2px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
