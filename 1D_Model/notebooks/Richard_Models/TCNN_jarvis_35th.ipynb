{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # !pip install -q nnAudio\n",
    "# !pip install -q --upgrade wandb\n",
    "# !pip install -q grad-cam\n",
    "# # !pip install -q ttach\n",
    "# # !pip install efficientnet_pytorch\n",
    "# # !pip install albumentations\n",
    "# !pip install line_profiler\n",
    "# !pip install transformers\n",
    "# !pip install audiomentations\n",
    "# !pip3 install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ipykernel<6\"\n",
    "# !pip install \"jupyterlab<3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import collections\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "import sys\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "import h5py\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True) \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import IPython.display\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as torch_functional\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts,\n",
    "                    CosineAnnealingLR, ReduceLROnPlateau,_LRScheduler,CyclicLR)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import audiomentations as A\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, PolarityInversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G2Net-Model/main_35th_GeM_vflip_shuffle01_5fold/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "\n",
    "    #frequently changed \n",
    "    model_name = 'TCNN'\n",
    "    model_version = \"main_35th_GeM_vflip_shuffle01_5fold\" \n",
    "    use_pretrain = False\n",
    "    use_pseudo_label = False\n",
    "\n",
    "    debug = False\n",
    "    use_checkpoint = False\n",
    "    use_lr_finder = False\n",
    "    use_subset = False \n",
    "    subset_frac = 0.4\n",
    "\n",
    "    #preproc related\n",
    "    #augmentation\n",
    "    vflip = True\n",
    "    \n",
    "    time_shift = False\n",
    "    time_stretch = False\n",
    "\n",
    "    divide_std = False#std changed... tbs\n",
    "    shuffle_channels = False #need normalization first\n",
    "    add_gaussian_noise = False #need normalization first\n",
    "    shuffle01 = True\n",
    "    timemask = False\n",
    "    shift_channel = False    \n",
    "    pitch_shift = False\n",
    "    use_mixup = False\n",
    "    mixup_alpha = 0.1\n",
    "    cropping = False\n",
    "\n",
    "    #logistic\n",
    "    seed = 48\n",
    "    target_size = 1\n",
    "    target_col = 'target'\n",
    "    n_fold = 5\n",
    "#     gdrive = './drive/MyDrive/Kaggle/G2Net/input/'\n",
    "    kaggle_json_path = 'kaggle/kaggle.json'\n",
    "    output_dir = \"G2Net-Model/\"\n",
    "\n",
    "    #logger\n",
    "    print_num_steps=350\n",
    "    \n",
    "    #training related\n",
    "    train_folds = [0,1,2,3,4]\n",
    "    epochs = 12\n",
    "    batch_size = 256\n",
    "\n",
    "    lr= 5e-3#5e-3 # Optimizer \n",
    "    weight_decay=0 #1e-4  # Optimizer, default value 0.01\n",
    "    gradient_accumulation_steps=1 # Optimizer\n",
    "    scheduler='cosineWithWarmUp' # warm up ratio 0.1 of total steps \n",
    "     \n",
    "    #speedup\n",
    "    num_workers=0\n",
    "    non_blocking=True\n",
    "    amp=True\n",
    "    use_cudnn = True \n",
    "    use_tpu = False\n",
    "    \n",
    "    #CNN structure\n",
    "    channels = 32\n",
    "    reduction = 1.0\n",
    "\n",
    "# no need to change below\n",
    "Config.model_output_folder = Config.output_dir + Config.model_version + \"/\"\n",
    "if not os.path.exists(Config.output_dir):\n",
    "    os.mkdir(Config.output_dir)\n",
    "if not os.path.exists(Config.model_output_folder):\n",
    "    os.mkdir(Config.model_output_folder)\n",
    "\n",
    "torch.backends.cudnn.benchmark = Config.use_cudnn \n",
    "display(Config.model_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "save_object(class2dict(Config), Config.model_output_folder + \"Config.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_2_path(file_id: str, train=True) -> str:\n",
    "    if train:\n",
    "        return \"./output/whiten-train/{}.npy\".format(file_id)\n",
    "    else:\n",
    "        return \"./output/whiten-test/{}.npy\".format(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training_labels.csv')\n",
    "test_df = pd.read_csv('sample_submission.csv')\n",
    "if Config.debug:\n",
    "    Config.epochs = 1\n",
    "    train_df = train_df.sample(n=50000, random_state=Config.seed).reset_index(drop=True)\n",
    "if Config.use_subset:\n",
    "    train_df = train_df.sample(frac=Config.subset_frac, random_state=Config.seed).reset_index(drop=True)\n",
    "train_df['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "test_df['file_path'] = test_df['id'].apply(lambda x :id_2_path(x,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014399681\n",
      "-0.003310739\n",
      "-0.02684033\n",
      "0.01655793\n",
      "-0.02000091\n"
     ]
    }
   ],
   "source": [
    "# checking magnitude of waves\n",
    "num_files = 5\n",
    "input_file_paths = train_df['file_path'].values[:num_files]\n",
    "batch_waves=np.zeros((num_files,3,4096))\n",
    "for i,input_file_path in enumerate(input_file_paths[:num_files]):\n",
    "    file_name = input_file_path.split('/')[-1].split('.npy')[0]\n",
    "    waves = np.load(input_file_path)#.astype(np.float32) # (3, 4096)\n",
    "    # batch_waves[i,:] = np.array([waves.max(),np.abs(waves).max(),np.abs(waves).min()])\n",
    "    whitened_waves = waves#whiten(waves)\n",
    "    print(whitened_waves[2][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold   \n",
       "0     0    0.500125\n",
       "      1    0.499875\n",
       "1     0    0.500125\n",
       "      1    0.499875\n",
       "2     0    0.500125\n",
       "      1    0.499875\n",
       "3     0    0.500125\n",
       "      1    0.499875\n",
       "4     0    0.500125\n",
       "      1    0.499875\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!\n",
    "skf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "splits = skf.split(train_df, train_df[\"target\"])\n",
    "train_df['fold'] = -1\n",
    "for fold, (train_index, valid_index) in enumerate(splits):\n",
    "    train_df.loc[valid_index,\"fold\"] = fold\n",
    "train_df['fold_orig'] = train_df['fold']\n",
    "\n",
    "train_df.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>file_path</th>\n",
       "      <th>fold</th>\n",
       "      <th>fold_orig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000e74ad</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/00000e74ad.npy</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f4945</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/00001f4945.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000661522</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/0000661522.npy</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00007a006a</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/00007a006a.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000a38978</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/0000a38978.npy</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559995</th>\n",
       "      <td>ffff9a5645</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/ffff9a5645.npy</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559996</th>\n",
       "      <td>ffffab0c27</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/ffffab0c27.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559997</th>\n",
       "      <td>ffffcf161a</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/ffffcf161a.npy</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559998</th>\n",
       "      <td>ffffd2c403</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/ffffd2c403.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559999</th>\n",
       "      <td>fffff2180b</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/fffff2180b.npy</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  target                             file_path  fold  \\\n",
       "0       00000e74ad       1  ./output/whiten-train/00000e74ad.npy     3   \n",
       "1       00001f4945       0  ./output/whiten-train/00001f4945.npy     0   \n",
       "2       0000661522       0  ./output/whiten-train/0000661522.npy     4   \n",
       "3       00007a006a       0  ./output/whiten-train/00007a006a.npy     0   \n",
       "4       0000a38978       1  ./output/whiten-train/0000a38978.npy     4   \n",
       "...            ...     ...                                   ...   ...   \n",
       "559995  ffff9a5645       1  ./output/whiten-train/ffff9a5645.npy     3   \n",
       "559996  ffffab0c27       0  ./output/whiten-train/ffffab0c27.npy     1   \n",
       "559997  ffffcf161a       1  ./output/whiten-train/ffffcf161a.npy     2   \n",
       "559998  ffffd2c403       0  ./output/whiten-train/ffffd2c403.npy     1   \n",
       "559999  fffff2180b       0  ./output/whiten-train/fffff2180b.npy     4   \n",
       "\n",
       "        fold_orig  \n",
       "0               3  \n",
       "1               0  \n",
       "2               4  \n",
       "3               0  \n",
       "4               4  \n",
       "...           ...  \n",
       "559995          3  \n",
       "559996          1  \n",
       "559997          2  \n",
       "559998          1  \n",
       "559999          4  \n",
       "\n",
       "[560000 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decides Threshold based on previous oof pred distribution \n",
    "\n",
    "# need to work on the copy, tbs\n",
    "\n",
    "# if Config.use_pseudo_label:\n",
    "#     print(\"Load Checkpoint, epo\")\n",
    "#     checkpoint = torch.load(Config.output_dir + 'SE_reduction1_SiLU/Fold_0_best_model.pth')\n",
    "#     valid_preds = checkpoint['valid_preds']\n",
    "#     kf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "#     train_df[\"fold\"] = -1\n",
    "#     for fold, (train_index, valid_index) in enumerate(kf.split(train_df, train_df[\"target\"])):\n",
    "#         train_df.loc[valid_index,\"fold\"] = fold\n",
    "#     # sanity check\n",
    "#     def get_score(y_true, y_pred):\n",
    "#         score = roc_auc_score(y_true, y_pred)\n",
    "#         return score\n",
    "#     y_true  = train_df.query(\"fold == 0\")[\"target\"]\n",
    "#     train_df.drop(columns=[\"fold\"],inplace=True)\n",
    "#     print(get_score(y_true, valid_preds))\n",
    "#     # get threshold\n",
    "#     up_thresh = 0.9\n",
    "#     down_thresh = 0.1\n",
    "#     for up_thresh in range(6, 10):\n",
    "#         up_thresh /= 10\n",
    "#         for down_thresh in range(4,0,-1):\n",
    "#             down_thresh /= 10\n",
    "#             ratio_up = sum(valid_preds > up_thresh) / len(valid_preds)\n",
    "#             ratio_down = sum(valid_preds < down_thresh) / len(valid_preds)\n",
    "#             acc_up = np.mean(y_true[ valid_preds > up_thresh])\n",
    "#             acc_down = 1 - np.mean(y_true[ valid_preds < down_thresh])\n",
    "#             print(\"Under Threshold Up : {:.0%}, Down: {:.0%}\".format(up_thresh, down_thresh))\n",
    "#             print(\"We can have Up sample: {:.1%}, Down sample: {:.1%}\".format(ratio_up, ratio_down))\n",
    "#             print(\"Up Accuracy: {:.1%}, Down Accuracy: {:.1%}\".format(acc_up, acc_down))\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add pseudo label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>file_path</th>\n",
       "      <th>fold</th>\n",
       "      <th>fold_orig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000e74ad</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/00000e74ad.npy</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f4945</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/00001f4945.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000661522</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/0000661522.npy</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00007a006a</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/00007a006a.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000a38978</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/0000a38978.npy</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559995</th>\n",
       "      <td>ffff9a5645</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/ffff9a5645.npy</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559996</th>\n",
       "      <td>ffffab0c27</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/ffffab0c27.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559997</th>\n",
       "      <td>ffffcf161a</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train/ffffcf161a.npy</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559998</th>\n",
       "      <td>ffffd2c403</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/ffffd2c403.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559999</th>\n",
       "      <td>fffff2180b</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train/fffff2180b.npy</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  target                             file_path  fold  \\\n",
       "0       00000e74ad       1  ./output/whiten-train/00000e74ad.npy     3   \n",
       "1       00001f4945       0  ./output/whiten-train/00001f4945.npy     0   \n",
       "2       0000661522       0  ./output/whiten-train/0000661522.npy     4   \n",
       "3       00007a006a       0  ./output/whiten-train/00007a006a.npy     0   \n",
       "4       0000a38978       1  ./output/whiten-train/0000a38978.npy     4   \n",
       "...            ...     ...                                   ...   ...   \n",
       "559995  ffff9a5645       1  ./output/whiten-train/ffff9a5645.npy     3   \n",
       "559996  ffffab0c27       0  ./output/whiten-train/ffffab0c27.npy     1   \n",
       "559997  ffffcf161a       1  ./output/whiten-train/ffffcf161a.npy     2   \n",
       "559998  ffffd2c403       0  ./output/whiten-train/ffffd2c403.npy     1   \n",
       "559999  fffff2180b       0  ./output/whiten-train/fffff2180b.npy     4   \n",
       "\n",
       "        fold_orig  \n",
       "0               3  \n",
       "1               0  \n",
       "2               4  \n",
       "3               0  \n",
       "4               4  \n",
       "...           ...  \n",
       "559995          3  \n",
       "559996          1  \n",
       "559997          2  \n",
       "559998          1  \n",
       "559999          4  \n",
       "\n",
       "[560000 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if Config.use_pseudo_label:\n",
    "    up_thresh = 0.9\n",
    "    down_thresh = 0.2\n",
    "\n",
    "    pseudo_label_df = pd.read_csv(Config.gdrive + \"pseudo_label_2.csv\")\n",
    "    pseudo_label_df.head()\n",
    "\n",
    "    num_test = pseudo_label_df.shape[0]\n",
    "    num_yes = (pseudo_label_df[\"target\"] >= up_thresh).sum()\n",
    "    num_no = (pseudo_label_df[\"target\"] <= down_thresh).sum()\n",
    "    num_all = num_yes+num_no\n",
    "    print(\"{:.2%} ratio, {:.2%} 1, {:.2%} 0\".format(num_all/num_test, num_yes/num_test, num_no/num_test))\n",
    "    test_df_2 = pseudo_label_df[(pseudo_label_df[\"target\"] >= up_thresh) | (pseudo_label_df[\"target\"] <= down_thresh)].copy()\n",
    "    test_df_2[\"target\"] = (test_df_2[\"target\"] > up_thresh).astype(int)\n",
    "    test_df_2 = test_df_2.merge(test_df[[\"id\",\"file_path\"]],on=\"id\",how=\"left\")\n",
    "    kf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "    test_df_2['fold'] = -1\n",
    "    for fold, (train_index, valid_index) in enumerate(kf.split(test_df_2, test_df_2[\"target\"])):\n",
    "        test_df_2.loc[valid_index,\"fold\"] = fold\n",
    "    train_df = pd.concat([train_df, test_df_2]).reset_index(drop=True)\n",
    "    display(train_df.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True)))\n",
    "    train_df.reset_index(inplace=True, drop=True)\n",
    "    display(train_df.shape)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_list = []\n",
    "\n",
    "if Config.add_gaussian_noise:\n",
    "    transform_list.append(A.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5))\n",
    "if Config.time_shift:\n",
    "    transform_list.append(A.Shift(min_fraction=-512*1.0/4096, max_fraction=-1*1.0/4096, p=0.5,rollover=False))#<0 means shift towards left,  fraction of total sound length\n",
    "# if Config.shift_channel:\n",
    "#     transform_list.append()\n",
    "if Config.pitch_shift:\n",
    "    transform_list.append(A.PitchShift(min_semitones=-1, max_semitones=1, p=0.5))\n",
    "if Config.time_stretch:\n",
    "    transform_list.append(A.TimeStretch(min_rate=0.98, max_rate=1.02,leave_length_unchanged=True, p=0.5))\n",
    "if Config.timemask:\n",
    "    transform_list.append(A.TimeMask(min_band_part=0.0, max_band_part=0.01, fade=False, p=0.5))#try 0.03 next time\n",
    "# if Config.vflip:\n",
    "#     transform_list.append(A.PolarityInversion(p=0.5))\n",
    "\n",
    "train_transform = A.Compose(transform_list)\n",
    "# \n",
    "\n",
    "test_transform = A.Compose([])\n",
    "class DataRetriever(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "        # self.ta_augment = ta_Compose([\n",
    "        #     ta_ShuffleChannels(),\n",
    "        # ])#bad coding style        \n",
    "        start_time =time.time()\n",
    "        array_shape = (len(self.paths),3,4096)\n",
    "        self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "        for i,path in enumerate(self.paths):\n",
    "            waves = np.load(path)\n",
    "            self.data[i,:] = waves\n",
    "        print(time.time()-start_time)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # path = self.paths[index] \n",
    "        # waves = np.load(path)\n",
    "        if Config.cropping:\n",
    "            waves = self.data[index][:,1792:3840+1]\n",
    "        else:\n",
    "            waves = self.data[index]\n",
    "        if Config.divide_std:\n",
    "            waves[0] *= 0.03058\n",
    "            waves[1] *= 0.03058\n",
    "            waves[2] *= 0.03096\n",
    "\n",
    "        if Config.shuffle_channels:\n",
    "            if np.random.random()<0.5:\n",
    "                np.random.shuffle(waves)\n",
    "        if Config.shuffle01:\n",
    "            if np.random.random()<0.5:\n",
    "                waves[[0,1]]=waves[[1,0]]\n",
    "        if Config.vflip:\n",
    "            if np.random.random()<0.5:\n",
    "                waves = -waves\n",
    "              \n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverTest(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "\n",
    "        array_shape = (len(self.paths),3,4096)\n",
    "        self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "        for i,path in enumerate(self.paths):\n",
    "            waves = np.load(path)\n",
    "            self.data[i,:] = waves  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # path = self.paths[index] \n",
    "        # waves = np.load(path)\n",
    "        waves = self.data[index]\n",
    "\n",
    "        if Config.divide_std:\n",
    "            waves[0] *= 0.03058\n",
    "            waves[1] *= 0.03058\n",
    "            waves[2] *= 0.03096\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverLRFinder(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "        # self.ta_augment = ta_Compose([\n",
    "        #     ta_ShuffleChannels(),\n",
    "        # ])#bad coding style        \n",
    "#         start_time =time.time()\n",
    "#         array_shape = (len(self.paths),3,4096)\n",
    "#         self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "#         for i,path in enumerate(self.paths):\n",
    "#             waves = np.load(path)\n",
    "#             self.data[i,:] = waves\n",
    "#         print(time.time()-start_time)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "        \n",
    "#         waves = self.data[index]\n",
    "\n",
    "        if Config.divide_std:\n",
    "            waves[0] *= 0.03058\n",
    "            waves[1] *= 0.03058\n",
    "            waves[2] *= 0.03096\n",
    "\n",
    "        if Config.shuffle_channels:\n",
    "            if np.random.random()<0.5:\n",
    "                np.random.shuffle(waves)\n",
    "        if Config.shuffle01:\n",
    "            if np.random.random()<0.5:\n",
    "                waves[[0,1]]=waves[[1,0]]\n",
    "        if Config.vflip:\n",
    "            if np.random.random()<0.5:\n",
    "                waves = -waves\n",
    "              \n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    '''\n",
    "    Code modified from the 2d code in\n",
    "    https://amaarora.github.io/2020/08/30/gempool.html\n",
    "    '''\n",
    "    def __init__(self, kernel_size=8, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return torch_functional.avg_pool1d(x.clamp(min=eps).pow(p), self.kernel_size).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for SE-----------------------------------------------------------------------\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, int(channel // reduction), bias=False),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(int(channel // reduction), channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, reduction,downsample=True):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        if downsample:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\t\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\t\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                SELayer(out_channels, reduction),\n",
    "                nn.MaxPool1d(2,ceil_mode=True), # downsampling by 2\n",
    "            )\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                     nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "#                     nn.BatchNorm1d(out_channels),\n",
    "#                     nn.MaxPool1d(2,ceil_mode=True),  # downsampling by 2\n",
    "#                 )#skip layers in residual_function, can try simple MaxPool1d\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.MaxPool1d(2,ceil_mode=True),  # downsampling by 2\n",
    "                )\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\t\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\t\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                SELayer(out_channels, reduction),\n",
    "            )\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                     nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "#                     nn.BatchNorm1d(out_channels),\n",
    "#                 )#skip layers in residual_function, can try identity, i.e., nn.Sequential()\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.SiLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "#-------------------------------------------------------------------------------        \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_block):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = Config.channels\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(3, Config.channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(Config.channels),\n",
    "            nn.SiLU(inplace=True))\n",
    "\n",
    "        self.conv2_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[0])\n",
    "        self.conv3_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[1])\n",
    "        self.conv4_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[2])\n",
    "        self.conv5_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[3])\n",
    "        self.conv6_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[4])\n",
    "        self.conv7_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[5])\n",
    "        self.conv8_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[6])\n",
    "        self.conv9_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[7])\n",
    "        self.conv10_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[8])\n",
    "        self.head = nn.Sequential(nn.Flatten(),\t#  8*channels\n",
    "                                  nn.Linear(8*Config.channels, 64),\t\n",
    "                                  nn.SiLU(inplace=True),\n",
    "                                  nn.Dropout(p=.25),#after activation\n",
    "                                  nn.Linear(64, 1),\t\n",
    "        )\n",
    "\n",
    "    def _make_stage(self, block, out_channels, num_blocks):\n",
    "        \"\"\"one stage may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block, bottle neck block, SE-type block etc\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks for this stage\n",
    "        Return:\n",
    "            return a resnet stage\n",
    "        \"\"\"\n",
    "        downsample = [True] + [False] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels,reduction=Config.reduction, downsample=downsample[i]))\n",
    "            self.in_channels = out_channels \n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.conv6_x(output)\n",
    "        output = self.conv7_x(output)\n",
    "        output = self.conv8_x(output)\n",
    "        output = self.conv9_x(output)\n",
    "        output = self.conv10_x(output)\n",
    "        output = self.head(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "class StochasticDepthBasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, reduction,downsample,p,is_train=True):#tbs for test data\n",
    "        super(StochasticDepthBasicBlock, self).__init__()\n",
    "        self.p = p\n",
    "        self.is_train = is_train\n",
    "        if downsample:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\t\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\t\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                SELayer(out_channels, reduction),\n",
    "                nn.MaxPool1d(2,ceil_mode=True), # downsampling by 2\n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    nn.MaxPool1d(2,ceil_mode=True),  # downsampling by 2\n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\t\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\t\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                SELayer(out_channels, reduction),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                )#skip layers in residual_function, can try identity, i.e., nn.Sequential()\n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device))\n",
    "    def forward(self, x):\n",
    "        if self.is_train:\n",
    "            if self.survival():\n",
    "                x = nn.SiLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.shortcut(x)\n",
    "        else:\n",
    "            x = self.residual_function(x)*self.p+self.shortcut(x)  #what's self.p right now?\n",
    "            print(\"p\",self.p)\n",
    "        return x\n",
    "class StochasticDepthResNet(nn.Module):\n",
    "    def __init__(self, block, num_block):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = Config.channels\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(3, Config.channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(Config.channels),\n",
    "            nn.SiLU(inplace=True))\n",
    "        \n",
    "        self.step = (1-0.8)/(sum(num_block)-1)\n",
    "        self.pl = 1\n",
    "\n",
    "        self.conv2_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[0])\n",
    "        self.conv3_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[1])\n",
    "        self.conv4_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[2])\n",
    "        self.conv5_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[3])\n",
    "        self.conv6_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[4])\n",
    "        self.conv7_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[5])\n",
    "        self.conv8_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[6])\n",
    "        self.conv9_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[7])\n",
    "        self.conv10_x = self._make_stage(block, out_channels=Config.channels, num_blocks=num_block[8])\n",
    "        self.head = nn.Sequential(nn.Flatten(),\t#  8*channels\n",
    "                                  nn.Linear(8*Config.channels, 64),\t\n",
    "                                  nn.SiLU(inplace=True),\n",
    "                                  nn.Dropout(p=.25),#after activation\n",
    "                                  nn.Linear(64, 1),\t\n",
    "        )\n",
    "\n",
    "    def _make_stage(self, block, out_channels, num_blocks):\n",
    "        \"\"\"one stage may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block, bottle neck block, SE-type block etc\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks for this stage\n",
    "        Return:\n",
    "            return a resnet stage\n",
    "        \"\"\"\n",
    "        downsample = [True] + [False] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels,reduction=Config.reduction,downsample=downsample[i],p=self.pl))\n",
    "            self.in_channels = out_channels \n",
    "            self.pl -= self.step\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.conv6_x(output)\n",
    "        output = self.conv7_x(output)\n",
    "        output = self.conv8_x(output)\n",
    "        output = self.conv9_x(output)\n",
    "        output = self.conv10_x(output)\n",
    "        output = self.head(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class ModelCNN_Dilations(nn.Module):\n",
    "    \"\"\"1D convolutional neural network with dilations. Classifier of the gravitaitonal waves\n",
    "    Inspired by the https://arxiv.org/pdf/1904.08693.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.init_conv = nn.Sequential(nn.Conv1d(3, 256, kernel_size=1), nn.ReLU())\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(256, 256, kernel_size=2, dilation=2 ** i),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for i in range(11)\n",
    "            ]\n",
    "        )\n",
    "        self.out_conv = nn.Sequential(nn.Conv1d(256, 1, kernel_size=1), nn.ReLU())\n",
    "        self.fc = nn.Linear(2049, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = self.fc(x)\n",
    "        x.squeeze_(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model1DCNN(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            nn.MaxPool1d(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Model1DCNNGEM(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            GeM(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            GeM(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            GeM(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Model():\n",
    "#     \"\"\" return a skip connection network\n",
    "#     \"\"\"\n",
    "#     return ResNet(SEBasicBlock, [1,1,1,1,1,1,1,1,1])\n",
    "# def Model():\n",
    "#     \"\"\" return a StochasticDepthResNet network\n",
    "#     \"\"\"\n",
    "#     return StochasticDepthResNet(StochasticDepthBasicBlock, [1,2,1,1,1,1,1,1,1])\n",
    "# def Model():\n",
    "#     return Model1DCNN(Config.channels)\n",
    "def Model():\n",
    "    return Model1DCNNGEM(Config.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659204"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "model = Model()#can possibly call random\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(seed=Config.seed)    \n",
    "\n",
    "def get_scheduler(optimizer, train_size):\n",
    "    if Config.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=Config.factor, \n",
    "                                      patience=Config.patience, verbose=True, eps=Config.eps)\n",
    "    elif Config.scheduler=='CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, \n",
    "                                      T_max=Config.T_max, \n",
    "                                      eta_min=Config.min_lr, last_epoch=-1)\n",
    "    elif Config.scheduler=='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                T_0=Config.T_0, \n",
    "                                                T_mult=1, \n",
    "                                                eta_min=Config.min_lr, \n",
    "                                                last_epoch=-1)\n",
    "    elif Config.scheduler=='CyclicLR':\n",
    "        iter_per_ep = train_size/Config.batch_size\n",
    "        step_size_up = int(iter_per_ep*Config.step_up_epochs)\n",
    "        step_size_down=int(iter_per_ep*Config.step_down_epochs)\n",
    "        scheduler = CyclicLR(optimizer, \n",
    "                             base_lr=Config.base_lr, \n",
    "                             max_lr=Config.max_lr,\n",
    "                             step_size_up=step_size_up,\n",
    "                             step_size_down=step_size_down,\n",
    "                             mode=Config.mode,\n",
    "                             gamma=Config.cycle_decay**(1/(step_size_up+step_size_down)),\n",
    "                             cycle_momentum=False)\n",
    "        \n",
    "    elif Config.scheduler == 'cosineWithWarmUp':\n",
    "        epoch_step = train_size/Config.batch_size\n",
    "        num_warmup_steps = int(0.1 * epoch_step * Config.epochs)\n",
    "        num_training_steps = int(epoch_step * Config.epochs)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps=num_warmup_steps, \n",
    "                                                    num_training_steps=num_training_steps)      \n",
    "    return scheduler\n",
    "def mixed_criterion(loss_fn, pred, y_a, y_b, lam):\n",
    "    return lam * loss_fn(pred, y_a) + (1 - lam) * loss_fn(pred, y_b)\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size, requires_grad=False).to(x.device,non_blocking=Config.non_blocking)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-PCIE-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Reserved:    8.5 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "if Config.use_tpu:\n",
    "    device = xm.xla_device()\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')#for debug, tb see\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "# watch nvidia-smi\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Reserved:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        torch.save(model.state_dict(), f'{Config.model_output_folder}/init_params.pt')\n",
    "\n",
    "    def range_test(self, loader, end_lr = 10, num_iter = 100, \n",
    "                   smooth_f = 0.05, diverge_th = 5):\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        for step, batch in enumerate(loader):\n",
    "            if step == num_iter:\n",
    "                break\n",
    "            loss = self._train_batch(batch)\n",
    "            lrs.append(lr_scheduler.get_last_lr()[0])\n",
    "            #update lr\n",
    "            lr_scheduler.step()\n",
    "            if step > 0:\n",
    "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "            losses.append(loss)\n",
    "            if loss > diverge_th * best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "        #reset model to initial parameters\n",
    "        model.load_state_dict(torch.load(f'{Config.model_output_folder}/init_params.pt'))\n",
    "        return lrs, losses\n",
    "\n",
    "    def _train_batch(self, batch):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        scaler = GradScaler()\n",
    "        X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "        targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "        \n",
    "        if Config.use_mixup:\n",
    "            (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                X, targets, Config.mixup_alpha\n",
    "            )\n",
    "            with autocast():\n",
    "                outputs = self.model(X_mix).squeeze()\n",
    "                loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "        else:\n",
    "            with autocast():\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "        #loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if Config.use_tpu:\n",
    "            xm.optimizer_step(self.optimizer, barrier=True)  # Note: TPU-specific code! \n",
    "        else:\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "#             self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "          \n",
    "            \n",
    "                    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "class ExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "def plot_lr_finder(lrs, losses, skip_start = 0, skip_end = 0):\n",
    "    if skip_end == 0:\n",
    "        lrs = lrs[skip_start:]\n",
    "        losses = losses[skip_start:]\n",
    "    else:\n",
    "        lrs = lrs[skip_start:-skip_end]\n",
    "        losses = losses[skip_start:-skip_end]\n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Learning rate')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.grid(True, 'both', 'x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.use_lr_finder:\n",
    "    START_LR = 1e-7\n",
    "    model = Model()\n",
    "    model.to(device,non_blocking=Config.non_blocking)\n",
    "    optimizer = AdamW(model.parameters(), lr=START_LR, weight_decay=Config.weight_decay, amsgrad=False)\n",
    "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    train_data_retriever = DataRetrieverLRFinder(train_df['file_path'], train_df[\"target\"].values)\n",
    "    train_loader = DataLoader(train_data_retriever,\n",
    "                                batch_size=Config.batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=Config.num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 3 µs, total: 7 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if Config.use_lr_finder:\n",
    "    try:\n",
    "        END_LR = 10\n",
    "        NUM_ITER = 200\n",
    "        lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "        lrs, losses = lr_finder.range_test(train_loader, END_LR, NUM_ITER)\n",
    "    except RuntimeError as e:\n",
    "        del model, optimizer, criterion, train_data_retriever, train_loader, lr_finder\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.use_lr_finder:\n",
    "    plot_lr_finder(lrs[:-28], losses[:-28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        scheduler,\n",
    "        valid_labels,\n",
    "        best_valid_score,\n",
    "        fold,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.best_valid_score = best_valid_score\n",
    "        self.valid_labels = valid_labels\n",
    "        self.fold = fold\n",
    "\n",
    "    \n",
    "    def fit(self, epochs, train_loader, valid_loader, save_path): \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        for n_epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch: ', n_epoch)\n",
    "            train_loss, train_preds = self.train_epoch(train_loader)\n",
    "            valid_loss, valid_preds = self.valid_epoch(valid_loader)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            if isinstance(self.scheduler, ReduceLROnPlateau):\n",
    "                self.scheduler.step(valid_loss)\n",
    "            valid_score = get_score(self.valid_labels, valid_preds)\n",
    "\n",
    "            numbers = valid_score\n",
    "            filename = Config.model_output_folder+f'score_epoch_{n_epoch}.json'          \n",
    "            with open(filename, 'w') as file_object: \n",
    "                json.dump(numbers, file_object) \n",
    "            \n",
    "\n",
    "            if self.best_valid_score < valid_score:\n",
    "                self.best_valid_score = valid_score\n",
    "                self.save_model(n_epoch, save_path+f'best_model.pth', train_preds, valid_preds)\n",
    "\n",
    "            print('train_loss: ',train_loss)\n",
    "            print('valid_loss: ',valid_loss)\n",
    "            print('valid_score: ',valid_score)\n",
    "            print('best_valid_score: ',self.best_valid_score)\n",
    "            print('time used: ', time.time()-start_time)\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] epoch\": n_epoch+1, \n",
    "                      f\"[fold{self.fold}] avg_train_loss\": train_loss, \n",
    "                      f\"[fold{self.fold}] avg_val_loss\": valid_loss,\n",
    "                      f\"[fold{self.fold}] val_score\": valid_score})        \n",
    "\n",
    "        # fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "        # ax.plot(list(range(epochs)), train_losses, label=\"train_loss\")\n",
    "        # ax.plot(list(range(epochs)), valid_losses, label=\"val_loss\")\n",
    "        # fig.legend()\n",
    "        # plt.show()            \n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        if Config.amp:\n",
    "            scaler = GradScaler()\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        train_loss = 0\n",
    "        # preds = []\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            self.optimizer.zero_grad()\n",
    "            X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "            targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "            \n",
    "            if Config.use_mixup:\n",
    "                (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                    X, targets, Config.mixup_alpha\n",
    "                )\n",
    "                with autocast():\n",
    "                    outputs = self.model(X_mix).squeeze()\n",
    "                    loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                with autocast():\n",
    "                    outputs = self.model(X).squeeze()\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "                \n",
    "            if Config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / Config.gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "          \n",
    "            if (step) % Config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "\n",
    "            if (not isinstance(self.scheduler, ReduceLROnPlateau)):\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # preds.append(outputs.sigmoid().to('cpu').detach().numpy())\n",
    "            loss2 = loss.detach()\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] loss\": loss2,\n",
    "                       f\"[fold{self.fold}] lr\": self.scheduler.get_last_lr()[0]})            \n",
    "\n",
    "            # losses.append(loss2.item())\n",
    "            losses.append(loss2)\n",
    "            train_loss += loss2\n",
    "\n",
    "            if (step) % Config.print_num_steps == 0:\n",
    "                train_loss = train_loss.item() #synch once per print_num_steps instead of once per batch\n",
    "                print(f'[{step}/{len(train_loader)}] ', \n",
    "                      f'avg loss: ',train_loss/step,\n",
    "                      f'inst loss: ', loss2.item())\n",
    "                \n",
    "        # predictions = np.concatenate(preds)\n",
    "\n",
    "#         losses_avg = []\n",
    "#         for i, loss in enumerate(losses):\n",
    "#             if i == 0 :\n",
    "#                 losses_avg.append(loss)\n",
    "#             else:\n",
    "#                 losses_avg.append(losses_avg[-1] * 0.6 + loss * 0.4)\n",
    "#         losses = torch.stack(losses)\n",
    "#         losses_avg = torch.stack(losses_avg)\n",
    "#         fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "#         ax.plot(list(range(step)), losses, label=\"train_loss per step\")\n",
    "#         ax.plot(list(range(step)), losses_avg, label=\"train_loss_avg per step\")\n",
    "#         fig.legend()\n",
    "#         plt.show()            \n",
    "        \n",
    "        return train_loss / step, None#, predictions\n",
    "\n",
    "    def valid_epoch(self, valid_loader):\n",
    "        self.model.eval()      \n",
    "        valid_loss = []\n",
    "        preds = []\n",
    "        for step, batch in enumerate(valid_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "                targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                if Config.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / Config.gradient_accumulation_steps\n",
    "                valid_loss.append(loss.detach().item())\n",
    "                preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "#                 valid_loss.append(loss.detach())#.item())\n",
    "#                 preds.append(outputs.sigmoid())#.to('cpu').numpy())\n",
    "#         valid_loss = torch.cat(valid_loss).to('cpu').numpy()\n",
    "#         predictions = torch.cat(preds).to('cpu').numpy()\n",
    "        predictions = np.concatenate(preds)\n",
    "        return np.mean(valid_loss), predictions\n",
    "\n",
    "    def save_model(self, n_epoch, save_path, train_preds, valid_preds):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"best_valid_score\": self.best_valid_score,\n",
    "                \"n_epoch\": n_epoch,\n",
    "                'scheduler': self.scheduler.state_dict(),\n",
    "                'train_preds': train_preds,\n",
    "                'valid_preds': valid_preds,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(seed=Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(use_checkpoint=Config.use_checkpoint):\n",
    "    kf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "    avg_best_valid_score = 0\n",
    "    folds_val_score = []\n",
    "    for fold in range(Config.n_fold): \n",
    "        train_index, valid_index = train_df.query(f\"fold!={fold}\").index, train_df.query(f\"fold_orig=={fold}\").index \n",
    "        print('Fold: ', fold)\n",
    "        if fold not in Config.train_folds:\n",
    "            print(\"skip\")\n",
    "            continue\n",
    "        train_X, valid_X = train_df.loc[train_index], train_df.loc[valid_index]\n",
    "        valid_labels = train_df.loc[valid_index,Config.target_col].values\n",
    "#         fold_indices = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv')#saved fold ids\n",
    "        oof = pd.DataFrame()\n",
    "        oof['id'] = train_df.loc[valid_index,'id']\n",
    "        oof['id'] = valid_X['id'].values.copy()\n",
    "        oof = oof.reset_index()\n",
    "        # assert oof['id'].eq(fold_indices['id']).all()\n",
    "#         if not Config.use_subset:\n",
    "#             assert oof['id'].eq(fold_indices['id']).sum()==112000\n",
    "        oof['target'] = valid_labels\n",
    "        \n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        # continue # uncomment this is to check oof ids\n",
    "\n",
    "        print('training data samples, val data samples: ', len(train_X) ,len(valid_X))\n",
    "        train_data_retriever = DataRetriever(train_X[\"file_path\"].values, train_X[\"target\"].values, transforms=train_transform)#how to run this only once and use for next experiment?\n",
    "        valid_data_retriever = DataRetrieverTest(valid_X[\"file_path\"].values, valid_X[\"target\"].values, transforms=test_transform)        \n",
    "        train_loader = DataLoader(train_data_retriever,\n",
    "                                  batch_size=Config.batch_size, \n",
    "                                  shuffle=True, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "        valid_loader = DataLoader(valid_data_retriever, \n",
    "                                  batch_size=Config.batch_size * 2, \n",
    "                                  shuffle=False, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        model = Model()\n",
    "        model.to(device,non_blocking=Config.non_blocking)\n",
    "        optimizer = AdamW(model.parameters(), lr=Config.lr, weight_decay=Config.weight_decay, amsgrad=False)\n",
    "        scheduler = get_scheduler(optimizer, len(train_X))\n",
    "        best_valid_score = -np.inf\n",
    "        if use_checkpoint:\n",
    "            print(\"Load Checkpoint, epo\")\n",
    "            checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_valid_score = float(checkpoint['best_valid_score'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        \n",
    "        \n",
    "        criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "        \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model, \n",
    "            device, \n",
    "            optimizer, \n",
    "            criterion,\n",
    "            scheduler,\n",
    "            valid_labels,\n",
    "            best_valid_score,\n",
    "            fold\n",
    "        )\n",
    "\n",
    "        history = trainer.fit(\n",
    "            epochs=Config.epochs, \n",
    "            train_loader=train_loader, \n",
    "            valid_loader=valid_loader,\n",
    "            save_path=f'{Config.model_output_folder}/Fold_{fold}_',\n",
    "        )\n",
    "        folds_val_score.append(trainer.best_valid_score)\n",
    "    wandb.finish()\n",
    "    print('folds score:', folds_val_score)\n",
    "    print(\"Avg: {:.5f}\".format(np.mean(folds_val_score)))\n",
    "    print(\"Std: {:.5f}\".format(np.std(folds_val_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"1b0833b15e81d54fad9cfbbe3d923f57562a6f89\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">main_35th_GeM_vflip_shuffle01_5fold</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kaggle_go/G2Net\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/1wk2jhah\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/1wk2jhah</a><br/>\n",
       "                Run data is saved locally in <code>/home/wandb/run-20210919_203818-1wk2jhah</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_type= \"debug\" if Config.debug else \"train\"\n",
    "# run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=job_type)\n",
    "run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=Config.model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "skip\n",
      "Fold:  1\n",
      "training data samples, val data samples:  448000 112000\n",
      "336.94975185394287\n",
      "Epoch:  0\n",
      "[350/1750]  avg loss:  0.5555198015485491 inst loss:  0.4833698868751526\n",
      "[700/1750]  avg loss:  0.5092899431501116 inst loss:  0.4461674690246582\n",
      "[1050/1750]  avg loss:  0.4889393833705357 inst loss:  0.45250049233436584\n",
      "[1400/1750]  avg loss:  0.4778629847935268 inst loss:  0.45013427734375\n",
      "[1750/1750]  avg loss:  0.470455810546875 inst loss:  0.4560246765613556\n",
      "train_loss:  0.470455810546875\n",
      "valid_loss:  0.5664388945657913\n",
      "valid_score:  0.8592448548826505\n",
      "best_valid_score:  0.8592448548826505\n",
      "time used:  86.73019313812256\n",
      "Epoch:  1\n",
      "[350/1750]  avg loss:  0.4342332240513393 inst loss:  0.48556503653526306\n",
      "[700/1750]  avg loss:  0.4346610804966518 inst loss:  0.45815128087997437\n",
      "[1050/1750]  avg loss:  0.43389721098400297 inst loss:  0.3697444200515747\n",
      "[1400/1750]  avg loss:  0.4320674351283482 inst loss:  0.4148033857345581\n",
      "[1750/1750]  avg loss:  0.43075669642857145 inst loss:  0.4130536913871765\n",
      "train_loss:  0.43075669642857145\n",
      "valid_loss:  0.6198324275343385\n",
      "valid_score:  0.8649350905699227\n",
      "best_valid_score:  0.8649350905699227\n",
      "time used:  81.97447609901428\n",
      "Epoch:  2\n",
      "[350/1750]  avg loss:  0.4235768781389509 inst loss:  0.4019158184528351\n",
      "[700/1750]  avg loss:  0.4236705235072545 inst loss:  0.43980905413627625\n",
      "[1050/1750]  avg loss:  0.4233011881510417 inst loss:  0.38091182708740234\n",
      "[1400/1750]  avg loss:  0.4229610770089286 inst loss:  0.36894679069519043\n",
      "[1750/1750]  avg loss:  0.4223216378348214 inst loss:  0.438323438167572\n",
      "train_loss:  0.4223216378348214\n",
      "valid_loss:  4.689788607157529\n",
      "valid_score:  0.7717840574839525\n",
      "best_valid_score:  0.8649350905699227\n",
      "time used:  83.51699495315552\n",
      "Epoch:  3\n",
      "[350/1750]  avg loss:  0.41804373604910716 inst loss:  0.39310896396636963\n",
      "[700/1750]  avg loss:  0.419033203125 inst loss:  0.3911699056625366\n",
      "[1050/1750]  avg loss:  0.4182027762276786 inst loss:  0.4124021530151367\n",
      "[1400/1750]  avg loss:  0.4184849766322545 inst loss:  0.44965946674346924\n",
      "[1750/1750]  avg loss:  0.418466064453125 inst loss:  0.4474080801010132\n",
      "train_loss:  0.418466064453125\n",
      "valid_loss:  0.41679625070258364\n",
      "valid_score:  0.8701461785869422\n",
      "best_valid_score:  0.8701461785869422\n",
      "time used:  87.9884102344513\n",
      "Epoch:  4\n",
      "[350/1750]  avg loss:  0.4156993756975446 inst loss:  0.45926976203918457\n",
      "[700/1750]  avg loss:  0.41501887730189735 inst loss:  0.39315930008888245\n",
      "[1050/1750]  avg loss:  0.414193841843378 inst loss:  0.38111335039138794\n",
      "[1400/1750]  avg loss:  0.4144939749581473 inst loss:  0.3710930645465851\n",
      "[1750/1750]  avg loss:  0.4145767647879464 inst loss:  0.4489924907684326\n",
      "train_loss:  0.4145767647879464\n",
      "valid_loss:  0.4371607375743727\n",
      "valid_score:  0.8718917749970625\n",
      "best_valid_score:  0.8718917749970625\n",
      "time used:  85.55157136917114\n",
      "Epoch:  5\n",
      "[350/1750]  avg loss:  0.4112687029157366 inst loss:  0.4149573743343353\n",
      "[700/1750]  avg loss:  0.4105291312081473 inst loss:  0.44241154193878174\n",
      "[1050/1750]  avg loss:  0.41152657645089286 inst loss:  0.39217233657836914\n",
      "[1400/1750]  avg loss:  0.4112488228934152 inst loss:  0.47246992588043213\n",
      "[1750/1750]  avg loss:  0.4105794503348214 inst loss:  0.44493260979652405\n",
      "train_loss:  0.4105794503348214\n",
      "valid_loss:  0.42334773864375946\n",
      "valid_score:  0.8729526129141301\n",
      "best_valid_score:  0.8729526129141301\n",
      "time used:  97.47209048271179\n",
      "Epoch:  6\n",
      "[350/1750]  avg loss:  0.40919477190290177 inst loss:  0.44908496737480164\n",
      "[700/1750]  avg loss:  0.4086641148158482 inst loss:  0.39151936769485474\n",
      "[1050/1750]  avg loss:  0.4077909342447917 inst loss:  0.4170674681663513\n",
      "[1400/1750]  avg loss:  0.40698438371930806 inst loss:  0.32764437794685364\n",
      "[1750/1750]  avg loss:  0.40717752511160715 inst loss:  0.3839240074157715\n",
      "train_loss:  0.40717752511160715\n",
      "valid_loss:  0.4322737014456971\n",
      "valid_score:  0.8740548451258767\n",
      "best_valid_score:  0.8740548451258767\n",
      "time used:  101.1887526512146\n",
      "Epoch:  7\n",
      "[350/1750]  avg loss:  0.4023199026925223 inst loss:  0.4067496061325073\n",
      "[700/1750]  avg loss:  0.4032977294921875 inst loss:  0.46494850516319275\n",
      "[1050/1750]  avg loss:  0.4028980073474702 inst loss:  0.3881062865257263\n",
      "[1400/1750]  avg loss:  0.40345838274274554 inst loss:  0.38398703932762146\n",
      "[1750/1750]  avg loss:  0.40291053989955355 inst loss:  0.40940210223197937\n",
      "train_loss:  0.40291053989955355\n",
      "valid_loss:  0.44503359380922364\n",
      "valid_score:  0.874605383744469\n",
      "best_valid_score:  0.874605383744469\n",
      "time used:  136.29656767845154\n",
      "Epoch:  8\n",
      "[350/1750]  avg loss:  0.40056309291294645 inst loss:  0.4237503409385681\n",
      "[700/1750]  avg loss:  0.39904209681919645 inst loss:  0.3853340744972229\n",
      "[1050/1750]  avg loss:  0.39951849074590773 inst loss:  0.35529783368110657\n",
      "[1400/1750]  avg loss:  0.3995152064732143 inst loss:  0.36774367094039917\n",
      "[1750/1750]  avg loss:  0.39917449079241074 inst loss:  0.4176271855831146\n",
      "train_loss:  0.39917449079241074\n",
      "valid_loss:  0.40370422356749236\n",
      "valid_score:  0.8757294914358993\n",
      "best_valid_score:  0.8757294914358993\n",
      "time used:  129.63374543190002\n",
      "Epoch:  9\n",
      "[350/1750]  avg loss:  0.394373299734933 inst loss:  0.4456680119037628\n",
      "[700/1750]  avg loss:  0.3938283429827009 inst loss:  0.35556522011756897\n",
      "[1050/1750]  avg loss:  0.39483526320684526 inst loss:  0.4074999988079071\n",
      "[1400/1750]  avg loss:  0.39461098807198663 inst loss:  0.3646920919418335\n",
      "[1750/1750]  avg loss:  0.39529356166294644 inst loss:  0.3573950529098511\n",
      "train_loss:  0.39529356166294644\n",
      "valid_loss:  0.4044310612493454\n",
      "valid_score:  0.8760714600478335\n",
      "best_valid_score:  0.8760714600478335\n",
      "time used:  108.89405369758606\n",
      "Epoch:  10\n",
      "[350/1750]  avg loss:  0.3953656005859375 inst loss:  0.37962085008621216\n",
      "[700/1750]  avg loss:  0.3930753435407366 inst loss:  0.4086833596229553\n",
      "[1050/1750]  avg loss:  0.3923973156156994 inst loss:  0.3539249897003174\n",
      "[1400/1750]  avg loss:  0.3928467232840402 inst loss:  0.3773343563079834\n",
      "[1750/1750]  avg loss:  0.39253770228794643 inst loss:  0.37094059586524963\n",
      "train_loss:  0.39253770228794643\n",
      "valid_loss:  0.40710381938986584\n",
      "valid_score:  0.8759478264304126\n",
      "best_valid_score:  0.8760714600478335\n",
      "time used:  121.55478549003601\n",
      "Epoch:  11\n",
      "[350/1750]  avg loss:  0.38967154366629464 inst loss:  0.40785109996795654\n",
      "[700/1750]  avg loss:  0.39118334089006696 inst loss:  0.354001522064209\n",
      "[1050/1750]  avg loss:  0.39115121023995536 inst loss:  0.4294019341468811\n",
      "[1400/1750]  avg loss:  0.39093431745256696 inst loss:  0.3686048090457916\n",
      "[1750/1750]  avg loss:  0.39132174246651785 inst loss:  0.38438671827316284\n",
      "train_loss:  0.39132174246651785\n",
      "valid_loss:  0.4048360961757294\n",
      "valid_score:  0.875987448562991\n",
      "best_valid_score:  0.8760714600478335\n",
      "time used:  130.06622910499573\n",
      "Fold:  2\n",
      "training data samples, val data samples:  448000 112000\n",
      "322.77420449256897\n",
      "Epoch:  0\n",
      "[350/1750]  avg loss:  0.5624036516462053 inst loss:  0.5216227173805237\n",
      "[700/1750]  avg loss:  0.5126162283761161 inst loss:  0.47433194518089294\n",
      "[1050/1750]  avg loss:  0.4916318475632441 inst loss:  0.4318382740020752\n",
      "[1400/1750]  avg loss:  0.47973951067243303 inst loss:  0.4196833074092865\n",
      "[1750/1750]  avg loss:  0.4717814592633929 inst loss:  0.43657633662223816\n",
      "train_loss:  0.4717814592633929\n",
      "valid_loss:  0.5044173307070449\n",
      "valid_score:  0.8623977973628727\n",
      "best_valid_score:  0.8623977973628727\n",
      "time used:  108.64819145202637\n",
      "Epoch:  1\n",
      "[350/1750]  avg loss:  0.4377203369140625 inst loss:  0.4094345271587372\n",
      "[700/1750]  avg loss:  0.4350354439871652 inst loss:  0.4704452157020569\n",
      "[1050/1750]  avg loss:  0.43378290085565474 inst loss:  0.43383583426475525\n",
      "[1400/1750]  avg loss:  0.4318042428152902 inst loss:  0.5192790627479553\n",
      "[1750/1750]  avg loss:  0.43064578683035715 inst loss:  0.41219162940979004\n",
      "train_loss:  0.43064578683035715\n",
      "valid_loss:  0.42573025672947434\n",
      "valid_score:  0.8662658057360005\n",
      "best_valid_score:  0.8662658057360005\n",
      "time used:  127.38808703422546\n",
      "Epoch:  2\n",
      "[350/1750]  avg loss:  0.42466766357421876 inst loss:  0.3915471136569977\n",
      "[700/1750]  avg loss:  0.42319405691964285 inst loss:  0.40248534083366394\n",
      "[1050/1750]  avg loss:  0.4223992919921875 inst loss:  0.41215503215789795\n",
      "[1400/1750]  avg loss:  0.42271824428013394 inst loss:  0.384719580411911\n",
      "[1750/1750]  avg loss:  0.42244817243303573 inst loss:  0.4345785677433014\n",
      "train_loss:  0.42244817243303573\n",
      "valid_loss:  0.4453884790477143\n",
      "valid_score:  0.8683704295920294\n",
      "best_valid_score:  0.8683704295920294\n",
      "time used:  101.97371554374695\n",
      "Epoch:  3\n",
      "[350/1750]  avg loss:  0.41955082484654016 inst loss:  0.38435447216033936\n",
      "[700/1750]  avg loss:  0.417866690499442 inst loss:  0.4439418911933899\n",
      "[1050/1750]  avg loss:  0.41856712704613097 inst loss:  0.4437652826309204\n",
      "[1400/1750]  avg loss:  0.4182291085379464 inst loss:  0.38993677496910095\n",
      "[1750/1750]  avg loss:  0.41820751953125 inst loss:  0.4085206985473633\n",
      "train_loss:  0.41820751953125\n",
      "valid_loss:  0.45119166183689413\n",
      "valid_score:  0.8704397753846288\n",
      "best_valid_score:  0.8704397753846288\n",
      "time used:  108.72928190231323\n",
      "Epoch:  4\n",
      "[350/1750]  avg loss:  0.41564121791294645 inst loss:  0.41783612966537476\n",
      "[700/1750]  avg loss:  0.41587371826171876 inst loss:  0.43933987617492676\n",
      "[1050/1750]  avg loss:  0.41621250697544643 inst loss:  0.3913506269454956\n",
      "[1400/1750]  avg loss:  0.41562687465122766 inst loss:  0.47214004397392273\n",
      "[1750/1750]  avg loss:  0.414713623046875 inst loss:  0.4038549065589905\n",
      "train_loss:  0.414713623046875\n",
      "valid_loss:  0.41156197261048233\n",
      "valid_score:  0.8699280471957581\n",
      "best_valid_score:  0.8704397753846288\n",
      "time used:  104.4187445640564\n",
      "Epoch:  5\n",
      "[350/1750]  avg loss:  0.4093104771205357 inst loss:  0.4237549304962158\n",
      "[700/1750]  avg loss:  0.4102437046595982 inst loss:  0.4182991683483124\n",
      "[1050/1750]  avg loss:  0.4109619721912202 inst loss:  0.41198158264160156\n",
      "[1400/1750]  avg loss:  0.41127428327287946 inst loss:  0.41253137588500977\n",
      "[1750/1750]  avg loss:  0.4109645647321429 inst loss:  0.41612833738327026\n",
      "train_loss:  0.4109645647321429\n",
      "valid_loss:  0.4447548409847364\n",
      "valid_score:  0.8735156727388622\n",
      "best_valid_score:  0.8735156727388622\n",
      "time used:  104.0580985546112\n",
      "Epoch:  6\n",
      "[350/1750]  avg loss:  0.4071187918526786 inst loss:  0.41905340552330017\n",
      "[700/1750]  avg loss:  0.40668648856026784 inst loss:  0.3680439293384552\n",
      "[1050/1750]  avg loss:  0.40634422665550596 inst loss:  0.3721317648887634\n",
      "[1400/1750]  avg loss:  0.40631726946149554 inst loss:  0.3902502954006195\n",
      "[1750/1750]  avg loss:  0.4069010532924107 inst loss:  0.4286087155342102\n",
      "train_loss:  0.4069010532924107\n",
      "valid_loss:  0.41351104326988464\n",
      "valid_score:  0.8738465828360746\n",
      "best_valid_score:  0.8738465828360746\n",
      "time used:  130.47120809555054\n",
      "Epoch:  7\n",
      "[350/1750]  avg loss:  0.4039220755440848 inst loss:  0.40100783109664917\n",
      "[700/1750]  avg loss:  0.4041469900948661 inst loss:  0.40065136551856995\n",
      "[1050/1750]  avg loss:  0.4029685756138393 inst loss:  0.43651577830314636\n",
      "[1400/1750]  avg loss:  0.40328717912946427 inst loss:  0.38786208629608154\n",
      "[1750/1750]  avg loss:  0.4031583426339286 inst loss:  0.38719087839126587\n",
      "train_loss:  0.4031583426339286\n",
      "valid_loss:  0.40963417332466334\n",
      "valid_score:  0.8751223407283096\n",
      "best_valid_score:  0.8751223407283096\n",
      "time used:  108.83074021339417\n",
      "Epoch:  8\n",
      "[350/1750]  avg loss:  0.3963899884905134 inst loss:  0.39057111740112305\n",
      "[700/1750]  avg loss:  0.3989522007533482 inst loss:  0.40247201919555664\n",
      "[1050/1750]  avg loss:  0.3992110188802083 inst loss:  0.3969058394432068\n",
      "[1400/1750]  avg loss:  0.3995286342075893 inst loss:  0.4478042423725128\n",
      "[1750/1750]  avg loss:  0.3994674595424107 inst loss:  0.4105992913246155\n",
      "train_loss:  0.3994674595424107\n",
      "valid_loss:  0.4113712525803205\n",
      "valid_score:  0.8757658721779691\n",
      "best_valid_score:  0.8757658721779691\n",
      "time used:  130.3971655368805\n",
      "Epoch:  9\n",
      "[350/1750]  avg loss:  0.39434496198381697 inst loss:  0.3824692368507385\n",
      "[700/1750]  avg loss:  0.39518218994140625 inst loss:  0.39782100915908813\n",
      "[1050/1750]  avg loss:  0.39518290201822914 inst loss:  0.4356676936149597\n",
      "[1400/1750]  avg loss:  0.39497554234095983 inst loss:  0.3927035331726074\n",
      "[1750/1750]  avg loss:  0.3955565359933036 inst loss:  0.3607827126979828\n",
      "train_loss:  0.3955565359933036\n",
      "valid_loss:  0.4054704043146682\n",
      "valid_score:  0.8758218611801929\n",
      "best_valid_score:  0.8758218611801929\n",
      "time used:  135.99437522888184\n",
      "Epoch:  10\n",
      "[350/1750]  avg loss:  0.3918076433454241 inst loss:  0.33954286575317383\n",
      "[700/1750]  avg loss:  0.3924219185965402 inst loss:  0.3663320243358612\n",
      "[1050/1750]  avg loss:  0.3926651436941964 inst loss:  0.3982241153717041\n",
      "[1400/1750]  avg loss:  0.3933855765206473 inst loss:  0.45543405413627625\n",
      "[1750/1750]  avg loss:  0.3929552525111607 inst loss:  0.3582175374031067\n",
      "train_loss:  0.3929552525111607\n",
      "valid_loss:  0.4046455076023868\n",
      "valid_score:  0.8760220258929582\n",
      "best_valid_score:  0.8760220258929582\n",
      "time used:  109.40187048912048\n",
      "Epoch:  11\n",
      "[350/1750]  avg loss:  0.3912638636997768 inst loss:  0.41990575194358826\n",
      "[700/1750]  avg loss:  0.3912863595145089 inst loss:  0.4615830183029175\n",
      "[1050/1750]  avg loss:  0.39054300944010417 inst loss:  0.3796839714050293\n",
      "[1400/1750]  avg loss:  0.391224844796317 inst loss:  0.3691937327384949\n",
      "[1750/1750]  avg loss:  0.3910755440848214 inst loss:  0.3825753927230835\n",
      "train_loss:  0.3910755440848214\n",
      "valid_loss:  0.40517010985444124\n",
      "valid_score:  0.8760509249700195\n",
      "best_valid_score:  0.8760509249700195\n",
      "time used:  106.615558385849\n",
      "Fold:  3\n",
      "training data samples, val data samples:  448000 112000\n",
      "348.9396641254425\n",
      "Epoch:  0\n",
      "[350/1750]  avg loss:  0.5569801548549107 inst loss:  0.5118223428726196\n",
      "[700/1750]  avg loss:  0.5080095999581473 inst loss:  0.5053654909133911\n",
      "[1050/1750]  avg loss:  0.48871983119419643 inst loss:  0.4115869700908661\n",
      "[1400/1750]  avg loss:  0.4775801740373884 inst loss:  0.4133591949939728\n",
      "[1750/1750]  avg loss:  0.47027584402901784 inst loss:  0.41248369216918945\n",
      "train_loss:  0.47027584402901784\n",
      "valid_loss:  1.0073690716534445\n",
      "valid_score:  0.8572152029700828\n",
      "best_valid_score:  0.8572152029700828\n",
      "time used:  102.06542038917542\n",
      "Epoch:  1\n",
      "[350/1750]  avg loss:  0.43358939034598215 inst loss:  0.425776869058609\n",
      "[700/1750]  avg loss:  0.43455008370535714 inst loss:  0.43746083974838257\n",
      "[1050/1750]  avg loss:  0.43352326892671134 inst loss:  0.4452226161956787\n",
      "[1400/1750]  avg loss:  0.4321435982840402 inst loss:  0.4494762420654297\n",
      "[1750/1750]  avg loss:  0.4313452845982143 inst loss:  0.3843189477920532\n",
      "train_loss:  0.4313452845982143\n",
      "valid_loss:  0.46230746077620277\n",
      "valid_score:  0.8685329248190222\n",
      "best_valid_score:  0.8685329248190222\n",
      "time used:  94.06138777732849\n",
      "Epoch:  2\n",
      "[350/1750]  avg loss:  0.424224373953683 inst loss:  0.4249388575553894\n",
      "[700/1750]  avg loss:  0.4246612112862723 inst loss:  0.4476016163825989\n",
      "[1050/1750]  avg loss:  0.4243978736514137 inst loss:  0.43504098057746887\n",
      "[1400/1750]  avg loss:  0.42357940673828126 inst loss:  0.4477542042732239\n",
      "[1750/1750]  avg loss:  0.4233051409040179 inst loss:  0.4155779182910919\n",
      "train_loss:  0.4233051409040179\n",
      "valid_loss:  0.5680843446080543\n",
      "valid_score:  0.8676127246658463\n",
      "best_valid_score:  0.8685329248190222\n",
      "time used:  104.98631453514099\n",
      "Epoch:  3\n",
      "[350/1750]  avg loss:  0.4226348005022321 inst loss:  0.3458080291748047\n",
      "[700/1750]  avg loss:  0.4204072788783482 inst loss:  0.42636170983314514\n",
      "[1050/1750]  avg loss:  0.419729730515253 inst loss:  0.4248981475830078\n",
      "[1400/1750]  avg loss:  0.4190753173828125 inst loss:  0.3659549653530121\n",
      "[1750/1750]  avg loss:  0.41899734933035715 inst loss:  0.3939540386199951\n",
      "train_loss:  0.41899734933035715\n",
      "valid_loss:  0.41545649464816264\n",
      "valid_score:  0.8710187017600974\n",
      "best_valid_score:  0.8710187017600974\n",
      "time used:  87.93706822395325\n",
      "Epoch:  4\n",
      "[350/1750]  avg loss:  0.4168284388950893 inst loss:  0.45305371284484863\n",
      "[700/1750]  avg loss:  0.4152707345145089 inst loss:  0.5215066075325012\n",
      "[1050/1750]  avg loss:  0.4150687953404018 inst loss:  0.45497697591781616\n",
      "[1400/1750]  avg loss:  0.41588134765625 inst loss:  0.4042662978172302\n",
      "[1750/1750]  avg loss:  0.415575927734375 inst loss:  0.4256227910518646\n",
      "train_loss:  0.415575927734375\n",
      "valid_loss:  0.42261189031818686\n",
      "valid_score:  0.8731132371269752\n",
      "best_valid_score:  0.8731132371269752\n",
      "time used:  100.23930788040161\n",
      "Epoch:  5\n",
      "[350/1750]  avg loss:  0.4124607631138393 inst loss:  0.46801066398620605\n",
      "[700/1750]  avg loss:  0.41198512486049105 inst loss:  0.40552258491516113\n",
      "[1050/1750]  avg loss:  0.41173624674479165 inst loss:  0.4132610559463501\n",
      "[1400/1750]  avg loss:  0.41215510777064734 inst loss:  0.4124968647956848\n",
      "[1750/1750]  avg loss:  0.4114799107142857 inst loss:  0.3859868347644806\n",
      "train_loss:  0.4114799107142857\n",
      "valid_loss:  0.41197992922508553\n",
      "valid_score:  0.8751878955793455\n",
      "best_valid_score:  0.8751878955793455\n",
      "time used:  102.44130611419678\n",
      "Epoch:  6\n",
      "[350/1750]  avg loss:  0.40963923863002233 inst loss:  0.39269113540649414\n",
      "[700/1750]  avg loss:  0.40892333984375 inst loss:  0.39563441276550293\n",
      "[1050/1750]  avg loss:  0.4082419259207589 inst loss:  0.31487250328063965\n",
      "[1400/1750]  avg loss:  0.4088954380580357 inst loss:  0.3655335307121277\n",
      "[1750/1750]  avg loss:  0.40784322684151786 inst loss:  0.3947744369506836\n",
      "train_loss:  0.40784322684151786\n",
      "valid_loss:  0.41029102263385303\n",
      "valid_score:  0.8760345612891498\n",
      "best_valid_score:  0.8760345612891498\n",
      "time used:  122.66559362411499\n",
      "Epoch:  7\n",
      "[350/1750]  avg loss:  0.40502777099609377 inst loss:  0.3567699193954468\n",
      "[700/1750]  avg loss:  0.4040838623046875 inst loss:  0.4099401831626892\n",
      "[1050/1750]  avg loss:  0.4042099144345238 inst loss:  0.4037401080131531\n",
      "[1400/1750]  avg loss:  0.4033664376395089 inst loss:  0.36328431963920593\n",
      "[1750/1750]  avg loss:  0.40380353655133927 inst loss:  0.453194797039032\n",
      "train_loss:  0.40380353655133927\n",
      "valid_loss:  0.41719161702073326\n",
      "valid_score:  0.8768707191857975\n",
      "best_valid_score:  0.8768707191857975\n",
      "time used:  91.0571506023407\n",
      "Epoch:  8\n",
      "[350/1750]  avg loss:  0.40034048897879465 inst loss:  0.39230042695999146\n",
      "[700/1750]  avg loss:  0.40058951241629465 inst loss:  0.38627660274505615\n",
      "[1050/1750]  avg loss:  0.39977332705543156 inst loss:  0.4088188111782074\n",
      "[1400/1750]  avg loss:  0.39973397391183035 inst loss:  0.3600338101387024\n",
      "[1750/1750]  avg loss:  0.39981783621651784 inst loss:  0.38334864377975464\n",
      "train_loss:  0.39981783621651784\n",
      "valid_loss:  0.401162698388644\n",
      "valid_score:  0.8772821192433977\n",
      "best_valid_score:  0.8772821192433977\n",
      "time used:  119.75168108940125\n",
      "Epoch:  9\n",
      "[350/1750]  avg loss:  0.39758645193917413 inst loss:  0.35746240615844727\n",
      "[700/1750]  avg loss:  0.39671774727957587 inst loss:  0.3976740837097168\n",
      "[1050/1750]  avg loss:  0.39617085774739585 inst loss:  0.4183967113494873\n",
      "[1400/1750]  avg loss:  0.39636993408203125 inst loss:  0.41882961988449097\n",
      "[1750/1750]  avg loss:  0.396168212890625 inst loss:  0.4399246573448181\n",
      "train_loss:  0.396168212890625\n",
      "valid_loss:  0.40170295080637825\n",
      "valid_score:  0.8778050577327141\n",
      "best_valid_score:  0.8778050577327141\n",
      "time used:  127.1747899055481\n",
      "Epoch:  10\n",
      "[350/1750]  avg loss:  0.39565438406808034 inst loss:  0.37403255701065063\n",
      "[700/1750]  avg loss:  0.3945644269670759 inst loss:  0.36066579818725586\n",
      "[1050/1750]  avg loss:  0.393622320265997 inst loss:  0.42624741792678833\n",
      "[1400/1750]  avg loss:  0.3929605974469866 inst loss:  0.3892451524734497\n",
      "[1750/1750]  avg loss:  0.3933003627232143 inst loss:  0.4297035336494446\n",
      "train_loss:  0.3933003627232143\n",
      "valid_loss:  0.40166787102342194\n",
      "valid_score:  0.8779104317826671\n",
      "best_valid_score:  0.8779104317826671\n",
      "time used:  157.31635069847107\n",
      "Epoch:  11\n",
      "[350/1750]  avg loss:  0.39149915422712056 inst loss:  0.3651338815689087\n",
      "[700/1750]  avg loss:  0.39339168003627234 inst loss:  0.36938732862472534\n",
      "[1050/1750]  avg loss:  0.39264014834449407 inst loss:  0.3809566795825958\n",
      "[1400/1750]  avg loss:  0.3922093418666295 inst loss:  0.4095074534416199\n",
      "[1750/1750]  avg loss:  0.3919127720424107 inst loss:  0.44938141107559204\n",
      "train_loss:  0.3919127720424107\n",
      "valid_loss:  0.4024390922561628\n",
      "valid_score:  0.8778856498933634\n",
      "best_valid_score:  0.8779104317826671\n",
      "time used:  92.42323803901672\n",
      "Fold:  4\n",
      "training data samples, val data samples:  448000 112000\n",
      "316.92089772224426\n",
      "Epoch:  0\n",
      "[350/1750]  avg loss:  0.5509177507672991 inst loss:  0.44070881605148315\n",
      "[700/1750]  avg loss:  0.5065579223632812 inst loss:  0.45849132537841797\n",
      "[1050/1750]  avg loss:  0.48629833403087797 inst loss:  0.4492119550704956\n",
      "[1400/1750]  avg loss:  0.47562975202287944 inst loss:  0.4477863311767578\n",
      "[1750/1750]  avg loss:  0.4679451729910714 inst loss:  0.42683660984039307\n",
      "train_loss:  0.4679451729910714\n",
      "valid_loss:  0.6418409891868835\n",
      "valid_score:  0.8539069645617874\n",
      "best_valid_score:  0.8539069645617874\n",
      "time used:  83.62168049812317\n",
      "Epoch:  1\n",
      "[350/1750]  avg loss:  0.4364990234375 inst loss:  0.45598334074020386\n",
      "[700/1750]  avg loss:  0.4345855712890625 inst loss:  0.44400230050086975\n",
      "[1050/1750]  avg loss:  0.4334580775669643 inst loss:  0.4410662055015564\n",
      "[1400/1750]  avg loss:  0.4319662039620536 inst loss:  0.38309478759765625\n",
      "[1750/1750]  avg loss:  0.4307877720424107 inst loss:  0.4296487271785736\n",
      "train_loss:  0.4307877720424107\n",
      "valid_loss:  0.6989059276776771\n",
      "valid_score:  0.8628476862621641\n",
      "best_valid_score:  0.8628476862621641\n",
      "time used:  84.69758367538452\n",
      "Epoch:  2\n",
      "[350/1750]  avg loss:  0.4236894008091518 inst loss:  0.44218939542770386\n",
      "[700/1750]  avg loss:  0.42356959751674106 inst loss:  0.42207813262939453\n",
      "[1050/1750]  avg loss:  0.4232949683779762 inst loss:  0.38150548934936523\n",
      "[1400/1750]  avg loss:  0.42255623953683036 inst loss:  0.4145527482032776\n",
      "[1750/1750]  avg loss:  0.4221514369419643 inst loss:  0.44810909032821655\n",
      "train_loss:  0.4221514369419643\n",
      "valid_loss:  4.70128001034532\n",
      "valid_score:  0.7821498518180392\n",
      "best_valid_score:  0.8628476862621641\n",
      "time used:  95.84208273887634\n",
      "Epoch:  3\n",
      "[350/1750]  avg loss:  0.42042510986328124 inst loss:  0.4124287962913513\n",
      "[700/1750]  avg loss:  0.4194565691266741 inst loss:  0.4414832592010498\n",
      "[1050/1750]  avg loss:  0.4188708786737351 inst loss:  0.42113322019577026\n",
      "[1400/1750]  avg loss:  0.41789437430245535 inst loss:  0.4575914442539215\n",
      "[1750/1750]  avg loss:  0.4180570591517857 inst loss:  0.419267475605011\n",
      "train_loss:  0.4180570591517857\n",
      "valid_loss:  0.4191974051738983\n",
      "valid_score:  0.8705316937577208\n",
      "best_valid_score:  0.8705316937577208\n",
      "time used:  85.03638195991516\n",
      "Epoch:  4\n",
      "[350/1750]  avg loss:  0.4134465680803571 inst loss:  0.416543185710907\n",
      "[700/1750]  avg loss:  0.4132090541294643 inst loss:  0.4413851201534271\n",
      "[1050/1750]  avg loss:  0.4138026355561756 inst loss:  0.5011452436447144\n",
      "[1400/1750]  avg loss:  0.4142054094587054 inst loss:  0.39452892541885376\n",
      "[1750/1750]  avg loss:  0.4141190011160714 inst loss:  0.42743951082229614\n",
      "train_loss:  0.4141190011160714\n",
      "valid_loss:  0.4886639805417083\n",
      "valid_score:  0.8704269759578085\n",
      "best_valid_score:  0.8705316937577208\n",
      "time used:  84.55886888504028\n",
      "Epoch:  5\n",
      "[350/1750]  avg loss:  0.41380850655691964 inst loss:  0.396914005279541\n",
      "[700/1750]  avg loss:  0.4127321515764509 inst loss:  0.46812692284584045\n",
      "[1050/1750]  avg loss:  0.41192391531808037 inst loss:  0.3761909604072571\n",
      "[1400/1750]  avg loss:  0.4112780325753348 inst loss:  0.4280821681022644\n",
      "[1750/1750]  avg loss:  0.41061049107142855 inst loss:  0.47761982679367065\n",
      "train_loss:  0.41061049107142855\n",
      "valid_loss:  0.40566749823147846\n",
      "valid_score:  0.8733166759152005\n",
      "best_valid_score:  0.8733166759152005\n",
      "time used:  83.52611327171326\n",
      "Epoch:  6\n",
      "[350/1750]  avg loss:  0.4078261021205357 inst loss:  0.3654715418815613\n",
      "[700/1750]  avg loss:  0.40662684849330355 inst loss:  0.3949223756790161\n",
      "[1050/1750]  avg loss:  0.40646658761160714 inst loss:  0.4198521077632904\n",
      "[1400/1750]  avg loss:  0.4066132027762277 inst loss:  0.40950196981430054\n",
      "[1750/1750]  avg loss:  0.4064160505022321 inst loss:  0.39818423986434937\n",
      "train_loss:  0.4064160505022321\n",
      "valid_loss:  0.4088361283687696\n",
      "valid_score:  0.8741365884345573\n",
      "best_valid_score:  0.8741365884345573\n",
      "time used:  83.15840458869934\n",
      "Epoch:  7\n",
      "[350/1750]  avg loss:  0.4042867170061384 inst loss:  0.46353286504745483\n",
      "[700/1750]  avg loss:  0.4039101736886161 inst loss:  0.40820762515068054\n",
      "[1050/1750]  avg loss:  0.4030790201822917 inst loss:  0.4139903485774994\n",
      "[1400/1750]  avg loss:  0.40298200334821427 inst loss:  0.4064345359802246\n",
      "[1750/1750]  avg loss:  0.40268913922991073 inst loss:  0.41408586502075195\n",
      "train_loss:  0.40268913922991073\n",
      "valid_loss:  0.40752479991956386\n",
      "valid_score:  0.8746313287397132\n",
      "best_valid_score:  0.8746313287397132\n",
      "time used:  82.37698531150818\n",
      "Epoch:  8\n",
      "[350/1750]  avg loss:  0.4021167428152902 inst loss:  0.42117539048194885\n",
      "[700/1750]  avg loss:  0.40080701555524556 inst loss:  0.48826274275779724\n",
      "[1050/1750]  avg loss:  0.4001767985026042 inst loss:  0.3716440200805664\n",
      "[1400/1750]  avg loss:  0.39947644914899555 inst loss:  0.45271819829940796\n",
      "[1750/1750]  avg loss:  0.39900917271205355 inst loss:  0.40079736709594727\n",
      "train_loss:  0.39900917271205355\n",
      "valid_loss:  0.42865751282265196\n",
      "valid_score:  0.8758493072916022\n",
      "best_valid_score:  0.8758493072916022\n",
      "time used:  86.67343473434448\n",
      "Epoch:  9\n",
      "[350/1750]  avg loss:  0.39807041713169644 inst loss:  0.41260507702827454\n",
      "[700/1750]  avg loss:  0.39666730608258927 inst loss:  0.37713730335235596\n",
      "[1050/1750]  avg loss:  0.3955113583519345 inst loss:  0.3728145360946655\n",
      "[1400/1750]  avg loss:  0.3950574166434152 inst loss:  0.3843797743320465\n",
      "[1750/1750]  avg loss:  0.3953676409040179 inst loss:  0.3823809027671814\n",
      "train_loss:  0.3953676409040179\n",
      "valid_loss:  0.402722211052838\n",
      "valid_score:  0.8762285171367314\n",
      "best_valid_score:  0.8762285171367314\n",
      "time used:  92.79514837265015\n",
      "Epoch:  10\n",
      "[350/1750]  avg loss:  0.3938585989815848 inst loss:  0.38912421464920044\n",
      "[700/1750]  avg loss:  0.3926538521902902 inst loss:  0.35222747921943665\n",
      "[1050/1750]  avg loss:  0.3927916899181548 inst loss:  0.3627939224243164\n",
      "[1400/1750]  avg loss:  0.39258963448660716 inst loss:  0.3783520758152008\n",
      "[1750/1750]  avg loss:  0.3924853515625 inst loss:  0.3476257920265198\n",
      "train_loss:  0.3924853515625\n",
      "valid_loss:  0.4021817727448189\n",
      "valid_score:  0.8761980479065107\n",
      "best_valid_score:  0.8762285171367314\n",
      "time used:  103.11388778686523\n",
      "Epoch:  11\n",
      "[350/1750]  avg loss:  0.39116367885044645 inst loss:  0.39062514901161194\n",
      "[700/1750]  avg loss:  0.39086107526506697 inst loss:  0.35936635732650757\n",
      "[1050/1750]  avg loss:  0.3915988304501488 inst loss:  0.42356425523757935\n",
      "[1400/1750]  avg loss:  0.3919667271205357 inst loss:  0.39520370960235596\n",
      "[1750/1750]  avg loss:  0.3914610072544643 inst loss:  0.3891758322715759\n",
      "train_loss:  0.3914610072544643\n",
      "valid_loss:  0.4031841837924365\n",
      "valid_score:  0.8761844382117825\n",
      "best_valid_score:  0.8762285171367314\n",
      "time used:  115.89105677604675\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 380<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/wandb/run-20210919_203818-1wk2jhah/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/wandb/run-20210919_203818-1wk2jhah/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold1] avg_train_loss</td><td>0.39132</td></tr><tr><td>[fold1] avg_val_loss</td><td>0.40484</td></tr><tr><td>[fold1] epoch</td><td>12</td></tr><tr><td>[fold1] loss</td><td>0.38439</td></tr><tr><td>[fold1] lr</td><td>0.0</td></tr><tr><td>[fold1] val_score</td><td>0.87599</td></tr><tr><td>[fold2] avg_train_loss</td><td>0.39108</td></tr><tr><td>[fold2] avg_val_loss</td><td>0.40517</td></tr><tr><td>[fold2] epoch</td><td>12</td></tr><tr><td>[fold2] loss</td><td>0.38258</td></tr><tr><td>[fold2] lr</td><td>0.0</td></tr><tr><td>[fold2] val_score</td><td>0.87605</td></tr><tr><td>[fold3] avg_train_loss</td><td>0.39191</td></tr><tr><td>[fold3] avg_val_loss</td><td>0.40244</td></tr><tr><td>[fold3] epoch</td><td>12</td></tr><tr><td>[fold3] loss</td><td>0.44938</td></tr><tr><td>[fold3] lr</td><td>0.0</td></tr><tr><td>[fold3] val_score</td><td>0.87789</td></tr><tr><td>[fold4] avg_train_loss</td><td>0.39146</td></tr><tr><td>[fold4] avg_val_loss</td><td>0.40318</td></tr><tr><td>[fold4] epoch</td><td>12</td></tr><tr><td>[fold4] loss</td><td>0.38918</td></tr><tr><td>[fold4] lr</td><td>0.0</td></tr><tr><td>[fold4] val_score</td><td>0.87618</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold1] avg_train_loss</td><td>█▄▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>[fold1] avg_val_loss</td><td>▁▁█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[fold1] epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>[fold1] loss</td><td>█▆▆▄▅█▄▅▄▄▅▆▄▄▃▄▄▃▁▄▄▄▆▂▅▆▃▃▄▄▃▅▄▃▂▁▃▁▃▁</td></tr><tr><td>[fold1] lr</td><td>▂▄▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold1] val_score</td><td>▇▇▁█████████</td></tr><tr><td>[fold2] avg_train_loss</td><td>█▄▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>[fold2] avg_val_loss</td><td>█▂▄▄▁▄▂▁▁▁▁▁</td></tr><tr><td>[fold2] epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>[fold2] loss</td><td>█▆▄▆▃▅▆▄▃▄▄▄▄▂▅▃▃▂▁█▃▂▅▄▃▄▄▄▂▁▁▇▅▂▄▂▂▂▂▁</td></tr><tr><td>[fold2] lr</td><td>▂▄▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold2] val_score</td><td>▁▃▄▅▅▇▇█████</td></tr><tr><td>[fold3] avg_train_loss</td><td>█▅▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>[fold3] avg_val_loss</td><td>█▂▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>[fold3] loss</td><td>█▆▆▂▅█▅▂▅▃▅▄▃▃▃▄▅▆▂▃▁▃▃▃▄▅▄▁▂▃▃▃▃▁▃▄▃▂▄▂</td></tr><tr><td>[fold3] lr</td><td>▂▄▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold3] val_score</td><td>▁▅▅▆▆▇▇█████</td></tr><tr><td>[fold4] avg_train_loss</td><td>█▅▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>[fold4] avg_val_loss</td><td>▁▁█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[fold4] epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>[fold4] loss</td><td>▇▆▅▄▅▆▄▇▆▆▇▆▇▃▂▇▆▃▇▅▄▄█▄█▅▅▅▆▅▃▆▆▄▆▁▅▅▅▆</td></tr><tr><td>[fold4] lr</td><td>▂▄▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold4] val_score</td><td>▆▇▁█████████</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">main_35th_GeM_vflip_shuffle01_5fold</strong>: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/1wk2jhah\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/1wk2jhah</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folds score: [0.8760714600478335, 0.8760509249700195, 0.8779104317826671, 0.8762285171367314]\n",
      "Avg: 0.87657\n",
      "Std: 0.00078\n",
      "CPU times: user 3h 1min 3s, sys: 6h 30min 49s, total: 9h 31min 52s\n",
      "Wall time: 1h 52min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 1099.92 s\n",
       "File: <ipython-input-44-065401859317>\n",
       "Function: __getitem__ at line 42\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    42                                               def __getitem__(self, index):\n",
       "    43                                                   \n",
       "    44                                                   # path = self.paths[index] \n",
       "    45                                                   # waves = np.load(path)\n",
       "    46  21504000   21233771.0      1.0      1.9          if Config.cropping:\n",
       "    47                                                       waves = self.data[index][:,1792:3840+1]\n",
       "    48                                                   else:\n",
       "    49  21504000   28698601.0      1.3      2.6              waves = self.data[index]\n",
       "    50  21504000   18701391.0      0.9      1.7          if Config.divide_std:\n",
       "    51                                                       waves[0] *= 0.03058\n",
       "    52                                                       waves[1] *= 0.03058\n",
       "    53                                                       waves[2] *= 0.03096\n",
       "    54                                           \n",
       "    55  21504000   17460395.0      0.8      1.6          if Config.shuffle_channels:\n",
       "    56                                                       if np.random.random()<0.5:\n",
       "    57                                                           np.random.shuffle(waves)\n",
       "    58  21504000   17777089.0      0.8      1.6          if Config.shuffle01:\n",
       "    59  21504000   42694155.0      2.0      3.9              if np.random.random()<0.5:\n",
       "    60  10753136  320058380.0     29.8     29.1                  waves[[0,1]]=waves[[1,0]]\n",
       "    61  21504000   20650618.0      1.0      1.9          if Config.vflip:\n",
       "    62  21504000   37210712.0      1.7      3.4              if np.random.random()<0.5:\n",
       "    63  10746294  241622251.0     22.5     22.0                  waves = -waves\n",
       "    64                                                         \n",
       "    65  21504000   20745252.0      1.0      1.9          if self.transforms is not None:\n",
       "    66  21504000   72347045.0      3.4      6.6              waves= self.transforms(waves,sample_rate=2048)\n",
       "    67  21504000   69653362.0      3.2      6.3          waves = torch.from_numpy(waves) \n",
       "    68                                                   # if Config.ta:#on tensor, batch*channel*ts\n",
       "    69                                                   #     waves = self.ta_augment(waves,sample_rate=2048)\n",
       "    70  21504000  152666900.0      7.1     13.9          target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
       "    71                                                       \n",
       "    72  21504000   18398029.0      0.9      1.7          return (waves, target)\n",
       "\n",
       "Total time: 5011.26 s\n",
       "File: <ipython-input-55-b68b2d0ab5d2>\n",
       "Function: fit at line 23\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    23                                               def fit(self, epochs, train_loader, valid_loader, save_path): \n",
       "    24         4         33.0      8.2      0.0          train_losses = []\n",
       "    25         4          4.0      1.0      0.0          valid_losses = []\n",
       "    26        52         84.0      1.6      0.0          for n_epoch in range(epochs):\n",
       "    27        48         94.0      2.0      0.0              start_time = time.time()\n",
       "    28        48      72666.0   1513.9      0.0              print('Epoch: ', n_epoch)\n",
       "    29        48 4641118555.0 96689969.9     92.6              train_loss, train_preds = self.train_epoch(train_loader)\n",
       "    30        48  366447953.0 7634332.4      7.3              valid_loss, valid_preds = self.valid_epoch(valid_loader)\n",
       "    31                                           \n",
       "    32        48        100.0      2.1      0.0              train_losses.append(train_loss)\n",
       "    33        48         55.0      1.1      0.0              valid_losses.append(valid_loss)\n",
       "    34                                           \n",
       "    35        48        166.0      3.5      0.0              if isinstance(self.scheduler, ReduceLROnPlateau):\n",
       "    36                                                           self.scheduler.step(valid_loss)\n",
       "    37        48    1658365.0  34549.3      0.0              valid_score = get_score(self.valid_labels, valid_preds)\n",
       "    38                                           \n",
       "    39        48        131.0      2.7      0.0              numbers = valid_score\n",
       "    40        48        307.0      6.4      0.0              filename = Config.model_output_folder+f'score_epoch_{n_epoch}.json'          \n",
       "    41        48      11888.0    247.7      0.0              with open(filename, 'w') as file_object: \n",
       "    42        48      11099.0    231.2      0.0                  json.dump(numbers, file_object) \n",
       "    43                                                       \n",
       "    44                                           \n",
       "    45        48        160.0      3.3      0.0              if self.best_valid_score < valid_score:\n",
       "    46        38         47.0      1.2      0.0                  self.best_valid_score = valid_score\n",
       "    47        38    1780857.0  46864.7      0.0                  self.save_model(n_epoch, save_path+f'best_model.pth', train_preds, valid_preds)\n",
       "    48                                           \n",
       "    49        48      49126.0   1023.5      0.0              print('train_loss: ',train_loss)\n",
       "    50        48      18426.0    383.9      0.0              print('valid_loss: ',valid_loss)\n",
       "    51        48      17142.0    357.1      0.0              print('valid_score: ',valid_score)\n",
       "    52        48      17260.0    359.6      0.0              print('best_valid_score: ',self.best_valid_score)\n",
       "    53        48      16215.0    337.8      0.0              print('time used: ', time.time()-start_time)\n",
       "    54                                           \n",
       "    55        96      36143.0    376.5      0.0              wandb.log({f\"[fold{self.fold}] epoch\": n_epoch+1, \n",
       "    56        48         91.0      1.9      0.0                        f\"[fold{self.fold}] avg_train_loss\": train_loss, \n",
       "    57        48         87.0      1.8      0.0                        f\"[fold{self.fold}] avg_val_loss\": valid_loss,\n",
       "    58        48         80.0      1.7      0.0                        f\"[fold{self.fold}] val_score\": valid_score})        \n",
       "    59                                           \n",
       "    60                                                   # fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
       "    61                                                   # ax.plot(list(range(epochs)), train_losses, label=\"train_loss\")\n",
       "    62                                                   # ax.plot(list(range(epochs)), valid_losses, label=\"val_loss\")\n",
       "    63                                                   # fig.legend()\n",
       "    64                                                   # plt.show()            \n",
       "\n",
       "Total time: 4637.82 s\n",
       "File: <ipython-input-55-b68b2d0ab5d2>\n",
       "Function: train_epoch at line 66\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    66                                               def train_epoch(self, train_loader):\n",
       "    67        48        143.0      3.0      0.0          if Config.amp:\n",
       "    68        48       1883.0     39.2      0.0              scaler = GradScaler()\n",
       "    69        48      30450.0    634.4      0.0          self.model.train()\n",
       "    70        48         75.0      1.6      0.0          losses = []\n",
       "    71        48         54.0      1.1      0.0          train_loss = 0\n",
       "    72                                                   # preds = []\n",
       "    73     84048 2121443456.0  25240.9     45.7          for step, batch in enumerate(train_loader, 1):\n",
       "    74     84000   62569588.0    744.9      1.3              self.optimizer.zero_grad()\n",
       "    75     84000    6149216.0     73.2      0.1              X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
       "    76     84000    1961623.0     23.4      0.0              targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
       "    77                                                       \n",
       "    78     84000     117254.0      1.4      0.0              if Config.use_mixup:\n",
       "    79                                                           (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
       "    80                                                               X, targets, Config.mixup_alpha\n",
       "    81                                                           )\n",
       "    82                                                           with autocast():\n",
       "    83                                                               outputs = self.model(X_mix).squeeze()\n",
       "    84                                                               loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
       "    85                                                       else:\n",
       "    86     84000    2451189.0     29.2      0.1                  with autocast():\n",
       "    87     84000  410469432.0   4886.5      8.9                      outputs = self.model(X).squeeze()\n",
       "    88     84000   30727563.0    365.8      0.7                      loss = self.criterion(outputs, targets)\n",
       "    89                                           \n",
       "    90                                           \n",
       "    91                                           \n",
       "    92                                               \n",
       "    93                                               \n",
       "    94                                           \n",
       "    95                                                           \n",
       "    96     84000     328466.0      3.9      0.0              if Config.gradient_accumulation_steps > 1:\n",
       "    97                                                           loss = loss / Config.gradient_accumulation_steps\n",
       "    98     84000  483239148.0   5752.8     10.4              scaler.scale(loss).backward()\n",
       "    99                                                     \n",
       "   100     84000     277842.0      3.3      0.0              if (step) % Config.gradient_accumulation_steps == 0:\n",
       "   101     84000 1365440246.0  16255.2     29.4                  scaler.step(self.optimizer)\n",
       "   102     84000    4915637.0     58.5      0.1                  scaler.update()\n",
       "   103                                                       \n",
       "   104                                           \n",
       "   105     84000     374092.0      4.5      0.0              if (not isinstance(self.scheduler, ReduceLROnPlateau)):\n",
       "   106     84000    7543398.0     89.8      0.2                  self.scheduler.step()\n",
       "   107                                           \n",
       "   108                                                       # preds.append(outputs.sigmoid().to('cpu').detach().numpy())\n",
       "   109     84000     953590.0     11.4      0.0              loss2 = loss.detach()\n",
       "   110                                           \n",
       "   111    168000   58148441.0    346.1      1.3              wandb.log({f\"[fold{self.fold}] loss\": loss2,\n",
       "   112     84000     250322.0      3.0      0.0                         f\"[fold{self.fold}] lr\": self.scheduler.get_last_lr()[0]})            \n",
       "   113                                           \n",
       "   114                                                       # losses.append(loss2.item())\n",
       "   115     84000     234593.0      2.8      0.0              losses.append(loss2)\n",
       "   116     84000   79485796.0    946.3      1.7              train_loss += loss2\n",
       "   117                                           \n",
       "   118     84000     427846.0      5.1      0.0              if (step) % Config.print_num_steps == 0:\n",
       "   119       240      13537.0     56.4      0.0                  train_loss = train_loss.item() #synch once per print_num_steps instead of once per batch\n",
       "   120       480     259216.0    540.0      0.0                  print(f'[{step}/{len(train_loader)}] ', \n",
       "   121       240        349.0      1.5      0.0                        f'avg loss: ',train_loss/step,\n",
       "   122       240       4761.0     19.8      0.0                        f'inst loss: ', loss2.item())\n",
       "   123                                                           \n",
       "   124                                                   # predictions = np.concatenate(preds)\n",
       "   125                                           \n",
       "   126                                           #         losses_avg = []\n",
       "   127                                           #         for i, loss in enumerate(losses):\n",
       "   128                                           #             if i == 0 :\n",
       "   129                                           #                 losses_avg.append(loss)\n",
       "   130                                           #             else:\n",
       "   131                                           #                 losses_avg.append(losses_avg[-1] * 0.6 + loss * 0.4)\n",
       "   132                                           #         losses = torch.stack(losses)\n",
       "   133                                           #         losses_avg = torch.stack(losses_avg)\n",
       "   134                                           #         fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
       "   135                                           #         ax.plot(list(range(step)), losses, label=\"train_loss per step\")\n",
       "   136                                           #         ax.plot(list(range(step)), losses_avg, label=\"train_loss_avg per step\")\n",
       "   137                                           #         fig.legend()\n",
       "   138                                           #         plt.show()            \n",
       "   139                                                   \n",
       "   140        48        247.0      5.1      0.0          return train_loss / step, None#, predictions\n",
       "\n",
       "Total time: 366.304 s\n",
       "File: <ipython-input-55-b68b2d0ab5d2>\n",
       "Function: valid_epoch at line 142\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   142                                               def valid_epoch(self, valid_loader):\n",
       "   143        48      25514.0    531.5      0.0          self.model.eval()      \n",
       "   144        48         64.0      1.3      0.0          valid_loss = []\n",
       "   145        48         24.0      0.5      0.0          preds = []\n",
       "   146     10560  241602246.0  22879.0     66.0          for step, batch in enumerate(valid_loader, 1):\n",
       "   147     10512     380805.0     36.2      0.1              with torch.no_grad():\n",
       "   148     10512     846712.0     80.5      0.2                  X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
       "   149     10512     230760.0     22.0      0.1                  targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
       "   150     10512   36718878.0   3493.0     10.0                  outputs = self.model(X).squeeze()\n",
       "   151     10512    2020704.0    192.2      0.6                  loss = self.criterion(outputs, targets)\n",
       "   152     10512      21601.0      2.1      0.0                  if Config.gradient_accumulation_steps > 1:\n",
       "   153                                                               loss = loss / Config.gradient_accumulation_steps\n",
       "   154     10512   83418452.0   7935.5     22.8                  valid_loss.append(loss.detach().item())\n",
       "   155     10512    1016263.0     96.7      0.3                  preds.append(outputs.sigmoid().to('cpu').numpy())\n",
       "   156                                           #                 valid_loss.append(loss.detach())#.item())\n",
       "   157                                           #                 preds.append(outputs.sigmoid())#.to('cpu').numpy())\n",
       "   158                                           #         valid_loss = torch.cat(valid_loss).to('cpu').numpy()\n",
       "   159                                           #         predictions = torch.cat(preds).to('cpu').numpy()\n",
       "   160        48      11497.0    239.5      0.0          predictions = np.concatenate(preds)\n",
       "   161        48      10255.0    213.6      0.0          return np.mean(valid_loss), predictions"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    %lprun -f DataRetriever.__getitem__ -f Trainer.train_epoch -f Trainer.fit -f Trainer.valid_epoch training_loop() \n",
    "#     training_loop(Config.use_checkpoint)\n",
    "except RuntimeError as e:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()   \n",
    "    print(e)# saving oof predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarviscloud import jarviscloud\n",
    "jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "successfully saved oof predictions for Fold:  0\n",
      "1\n",
      "successfully saved oof predictions for Fold:  1\n",
      "2\n",
      "successfully saved oof predictions for Fold:  2\n",
      "3\n",
      "successfully saved oof predictions for Fold:  3\n",
      "4\n",
      "successfully saved oof predictions for Fold:  4\n"
     ]
    }
   ],
   "source": [
    "for fold in Config.train_folds:\n",
    "    print(fold)\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    # print(checkpoint['valid_preds'])\n",
    "    try:\n",
    "        # oof = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv') also works, used in replacement of next statement for previously not generated Fold_{fold}_oof_pred.csv\n",
    "        oof = pd.read_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        oof['pred'] = checkpoint['valid_preds']\n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv') \n",
    "        print('successfully saved oof predictions for Fold: ', fold)   \n",
    "    except:\n",
    "        raise RuntimeError('failure in saving predictions for Fold: ', fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTA(Dataset):\n",
    "    def __init__(self, paths, targets, use_vflip=False, shuffle_channels=False, time_shift=False, add_gaussian_noise = False,  time_stretch=False,shuffle01=False ):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.use_vflip = use_vflip\n",
    "        self.shuffle_channels = shuffle_channels\n",
    "        self.time_shift = time_shift\n",
    "        self.gaussian_noise = add_gaussian_noise\n",
    "        self.time_stretch = time_stretch\n",
    "        self.shuffle01 = shuffle01\n",
    "        if time_shift:\n",
    "            self.time_shift = A.Shift(min_fraction=-512*1.0/4096, max_fraction=-1.0/4096, p=1,rollover=False)\n",
    "        if add_gaussian_noise:\n",
    "            self.gaussian_noise = A.AddGaussianNoise(min_amplitude=0.001, max_amplitude= 0.015, p=1)\n",
    "        if time_stretch:\n",
    "            self.time_stretch = A.TimeStretch(min_rate=0.9, max_rate=1.111,leave_length_unchanged=True, p=1)\n",
    "              \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "\n",
    "        if Config.divide_std:\n",
    "            waves[0] *= 0.03058\n",
    "            waves[1] *= 0.03058\n",
    "            waves[2] *= 0.03096\n",
    "\n",
    "        if self.use_vflip:\n",
    "            waves = -waves\n",
    "        if self.shuffle_channels:\n",
    "            np.random.shuffle(waves)\n",
    "        if self.time_shift:\n",
    "            waves = self.time_shift(waves, sample_rate=2048)\n",
    "        if self.gaussian_noise:\n",
    "            waves = self.gaussian_noise(waves, sample_rate=2048)\n",
    "        if self.time_stretch:\n",
    "            waves = self.time_stretch(waves, sample_rate=2048)\n",
    "        if self.shuffle01:\n",
    "            waves[[0,1]] = waves[[1,0]]\n",
    "        \n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device,             \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(loader,model):\n",
    "    preds = []\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        if step % Config.print_num_steps == 0:\n",
    "            print(\"step {}/{}\".format(step, len(loader)))\n",
    "        with torch.no_grad():\n",
    "            X = batch[0].to(device,non_blocking=Config.non_blocking)\n",
    "            outputs = model(X).squeeze()\n",
    "            preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "def get_tta_pred(df,model,**transforms):\n",
    "    data_retriever = TTA(df['file_path'].values, df['target'].values, **transforms)\n",
    "    loader = DataLoader(data_retriever, \n",
    "                            batch_size=Config.batch_size * 2, \n",
    "                            shuffle=False, \n",
    "                            num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "    return get_pred(loader,model)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TTA for oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "oof_all = pd.DataFrame()\n",
    "for fold in Config.train_folds:\n",
    "    oof = train_df.query(f\"fold=={fold}\").copy()\n",
    "    oof['preds'] = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')['valid_preds']\n",
    "    oof['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "    # display(oof)    \n",
    "\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    oof[\"tta_vflip\"] = get_tta_pred(oof,model,use_vflip=True)\n",
    "    # oof[\"tta_shift\"] = get_tta_pred(oof,model,time_shift=True)\n",
    "    # oof[\"tta_vflip_shift\"] = get_tta_pred(oof,model,use_vflip=True,time_shift=True)\n",
    "    oof[\"tta_shuffle01\"] = get_tta_pred(oof,model,shuffle01=True)\n",
    "    oof[\"tta_vflip_shuffle01\"] = get_tta_pred(oof,model,use_vflip=True,shuffle01=True)\n",
    "    # oof[\"tta_shift_shuffle01\"] = get_tta_pred(oof,model,time_shift=True,shuffle01=True)\n",
    "    # oof[\"tta_vflip_shift_shuffle01\"] = get_tta_pred(oof,model,use_vflip=True,time_shift=True,shuffle01=True)\n",
    "\n",
    "    oof.to_csv(Config.model_output_folder + f\"/oof_Fold_{fold}.csv\", index=False)\n",
    "    oof_all = pd.concat([oof_all,oof])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 0.8766354666392064\n",
      "tta_vflip 0.876561745269803\n",
      "tta_shuffle01 0.8764093953304342\n",
      "tta_vflip_shuffle01 0.8766249669510503\n",
      "preds_tta_avg: 0.8774725437897382\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",roc_auc_score(oof_all['target'], oof_all['preds']))\n",
    "\n",
    "for col in oof.columns:\n",
    "    if \"tta\" in col:\n",
    "        print(col,roc_auc_score(oof_all['target'], oof_all[col]))\n",
    "\n",
    "oof_all['avg']=0\n",
    "count = 0\n",
    "for col in oof_all.columns:\n",
    "    if \"tta\" in col or 'preds' in col: \n",
    "        count+=1\n",
    "        oof_all['avg'] += oof_all[col]\n",
    "oof_all['avg'] /= count\n",
    "print(\"preds_tta_avg:\",roc_auc_score(oof_all['target'], oof_all['avg']))\n",
    "\n",
    "oof_all.to_csv(Config.model_output_folder + \"/oof_all.csv\", index=False)\n",
    "oof_all[['id','fold','avg']].rename(columns={'id':'id','fold':'fold','avg':'prediction'}).to_csv(Config.model_output_folder + \"/oof_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TTA for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n",
      "step 350/442\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "test_df['target'] = 0  \n",
    "model = Model()\n",
    "test_avg = test_df[['id', 'target']].copy()\n",
    "for fold in Config.train_folds:\n",
    "    test_df2 = test_df.copy()\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "\n",
    "    test_df2['preds'+f'_Fold_{fold}'] = get_tta_pred(test_df2,model)\n",
    "    test_df2[\"tta_vflip\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,use_vflip=True)\n",
    "#     test_df2[\"tta_shift\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,time_shift=True)\n",
    "#     test_df2[\"tta_vflip_shift\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,use_vflip=True,time_shift=True)\n",
    "    test_df2[\"tta_shuffle01\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,shuffle01=True)\n",
    "    test_df2[\"tta_vflip_shuffle01\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,use_vflip=True,shuffle01=True)\n",
    "#     test_df2[\"tta_shift_shuffle01\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,time_shift=True,shuffle01=True)\n",
    "#     test_df2[\"tta_vflip_shift_shuffle01\"+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,use_vflip=True,time_shift=True,shuffle01=True)\n",
    "    \n",
    "    test_df2.to_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\", index=False)\n",
    "    count = 0\n",
    "    for col in test_df2.columns:\n",
    "        if \"tta\" in col or 'preds' in col: \n",
    "            count+=1\n",
    "            test_avg['target'] += test_df2[col]/len(Config.train_folds)\n",
    "    test_avg['target'] /= count\n",
    "test_avg.to_csv(Config.model_output_folder + \"/test_avg.csv\", index=False)\n",
    "\n",
    "#just used vflip here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg[['id', 'target']].to_csv(\"./submission.csv\", index=False)\n",
    "\n",
    "test_avg[['id', 'target']].to_csv(Config.model_output_folder + \"/submission.csv\", index=False)\n",
    "\n",
    "!mkdir -p ~/.kaggle/ && cp $Config.kaggle_json_path ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c g2net-gravitational-wave-detection -f ./submission.csv -m $Config.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
