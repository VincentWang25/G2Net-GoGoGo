{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd22902b-841f-41c9-a66f-77395174ae4c",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55bb6b65-2c6f-453d-8b6f-c6bf386f446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # !pip install -q nnAudio\n",
    "# !pip install -q --upgrade wandb\n",
    "# !pip install -q grad-cam\n",
    "# # !pip install -q ttach\n",
    "# # !pip install efficientnet_pytorch\n",
    "# # !pip install albumentations\n",
    "# !pip install line_profiler\n",
    "# !pip install transformers\n",
    "# !pip install audiomentations\n",
    "# !pip3 install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd2025db-3b0f-43ab-a216-e9058852e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ipykernel<6\"\n",
    "# !pip install \"jupyterlab<3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "171b030f-1a09-489c-8b3a-c574d306ade7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import collections\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "import sys\n",
    "import json\n",
    "import wandb\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "from glob import glob\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True) \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import IPython.display\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as torch_functional\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts,\n",
    "                    CosineAnnealingLR, ReduceLROnPlateau,_LRScheduler,CyclicLR)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import audiomentations as A\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, PolarityInversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd075088-f395-437c-a176-da7bf90a4fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3e2de-86a7-4a6f-ad50-ef450f241250",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c74b277a-2cf0-4edc-8d36-75d33f89d081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G2Net-Model/124th_V2SDCBAM_PL_6ep_2em3lr_32ch_vf+gn+sc01+tm+ts/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "\n",
    "    #frequently changed \n",
    "    model_name = 'TCNN'\n",
    "    model_version = \"124th_V2SDCBAM_PL_6ep_2em3lr_32ch_vf+gn+sc01+tm+ts\" \n",
    "    model_module = 'V2SDCBAM'#V2StochasticDepth,ModelIafossV2,V2SDCBAM,BoTCBAMV2SD,\n",
    "    use_pretrain = False\n",
    "    use_pseudo_label = True\n",
    "    up_thresh = 0.70\n",
    "    down_thresh = 0.15\n",
    "\n",
    "    debug = False\n",
    "    use_checkpoint = False\n",
    "    use_lr_finder = True\n",
    "    use_subset = False \n",
    "    subset_frac = 0.4\n",
    "\n",
    "    #preproc related\n",
    "    #augmentation\n",
    "    #proba for conservative, weight for aggressive\n",
    "    \n",
    "    #conservative\n",
    "    conservative_aug = []#'vflip','add_gaussian_noise',\n",
    "    #aggressive, OneOf \n",
    "    aggressive_aug_proba = 0.80\n",
    "    aggressive_aug = ['vflip','add_gaussian_noise','shuffle01','timemask','time_shift',]     #'reduce_SNR'\n",
    "    \n",
    "    \n",
    "    vflip = True\n",
    "    vflip_proba = 0.5\n",
    "    vflip_weight = 1.0 \n",
    "    add_gaussian_noise = True \n",
    "    add_gaussian_noise_proba = 0.5 \n",
    "    add_gaussian_noise_weight = 1.0    \n",
    "    timemask = True\n",
    "    timemask_proba = 0.35\n",
    "    timemask_weight = 0.8\n",
    "    shuffle01 = True\n",
    "    shuffle01_proba = 0.35\n",
    "    shuffle01_weight = 0.8\n",
    "    time_shift = True\n",
    "    time_shift_left = 96\n",
    "    time_shift_right = 96\n",
    "    time_shift_proba = 0.35\n",
    "    time_shift_weight = 0.4\n",
    "    \n",
    "    shift_channel = False\n",
    "    shift_channel_left = 16\n",
    "    shift_channel_right = 16\n",
    "    shift_channel_proba = 0.5\n",
    "    shift_channel_weight = 1.0\n",
    "    shift_two_channels = False #tba\n",
    "    shift_two_channels_proba = 0.5\n",
    "    shift_two_channels_weight= 1.0\n",
    "    reduce_SNR = False\n",
    "    reduce_SNR_ratio = 0.9998\n",
    "    reduce_SNR_proba = 0.5\n",
    "    reduce_SNR_weight = 1.0\n",
    "\n",
    "    time_stretch = False\n",
    "    divide_std = False \n",
    "    shuffle_channels = False    \n",
    "    pitch_shift = False\n",
    "    use_mixup = False\n",
    "    mixup_alpha = 0.1\n",
    "    cropping = False\n",
    "    \n",
    "    #logistic\n",
    "    seed = 48\n",
    "    target_size = 1\n",
    "    target_col = 'target'\n",
    "    n_fold = 5\n",
    "#     gdrive = './drive/MyDrive/Kaggle/G2Net/input/'\n",
    "    kaggle_json_path = 'kaggle/kaggle.json'\n",
    "    output_dir = \"G2Net-Model/\"\n",
    "    pseudo_label_folder = \"G2Net-Model/main_112th_V2SD_PL_6ep_5Fold/\"#main_35th_GeM_vflip_shuffle01_5fold,#main_112th_V2SD_PL_6ep_5Fold\n",
    "\n",
    "    #logger\n",
    "    print_num_steps=350\n",
    "    \n",
    "    #training related\n",
    "    train_folds = [0]\n",
    "    epochs = 6\n",
    "    batch_size = 256\n",
    "    \n",
    "    lr=  2e-3 #2e-3#8e-3#1e-2#5e-3, 1e-2 # Optimizer  1e-2 channel8, 5e-3 or 2e-3 channel32, 7e-3 channel 16\n",
    "    weight_decay=0 #1e-4  # Optimizer, default value 0.01\n",
    "    gradient_accumulation_steps=1 # Optimizer\n",
    "    scheduler='cosineWithWarmUp' # warm up ratio 0.1 of total steps \n",
    "     \n",
    "    #speedup\n",
    "    num_workers=7\n",
    "    non_blocking=False\n",
    "    amp=True\n",
    "    use_cudnn = True \n",
    "    use_tpu = False\n",
    "    use_ram = False\n",
    "    continuous_exp = False\n",
    "    \n",
    "    #CNN structure\n",
    "    channels = 32\n",
    "    reduction = 1.0\n",
    "    stochastic_final_layer_proba = 0.8\n",
    "    CBAM_SG_kernel_size = 15\n",
    "\n",
    "# no need to change below\n",
    "Config.model_output_folder = Config.output_dir + Config.model_version + \"/\"\n",
    "if not os.path.exists(Config.output_dir):\n",
    "    os.mkdir(Config.output_dir)\n",
    "if not os.path.exists(Config.model_output_folder):\n",
    "    os.mkdir(Config.model_output_folder)\n",
    "\n",
    "torch.backends.cudnn.benchmark = Config.use_cudnn \n",
    "display(Config.model_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2fd5ddd-01f0-4e0c-a483-fe055e23bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run once for Fold 0, save it in RAM and then do experiments multiple times       \n",
    "# if Config.continuous_exp and Config.train_folds == [0]:\n",
    "#     start_time =time.time()  \n",
    "#     if Config.use_pseudo_label:\n",
    "#         with open('fold_0_data_PL.npy', 'rb') as f:\n",
    "#             fold_0_data_PL = np.load(f)\n",
    "#     else:\n",
    "#         with open('fold_0_data.npy', 'rb') as f:\n",
    "#             fold_0_data = np.load(f)\n",
    "#     print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d7181-da0e-4f49-b7dc-d1af56fa584c",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08b1bd21-a513-4b67-9181-01e1468b1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "save_object(class2dict(Config), Config.model_output_folder + \"Config.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f52f10-a321-463e-be55-6f1267407999",
   "metadata": {},
   "source": [
    "# Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ba86f1b-6607-4b9d-a251-f79d9cb7b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_2_path(file_id: str, train=True) -> str:\n",
    "    if train:\n",
    "        return \"./output/whiten-train-w0/{}.npy\".format(file_id)\n",
    "    else:\n",
    "        return \"./output/whiten-test-w0/{}.npy\".format(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93a20a41-a049-4ce9-aaf7-61856e1d1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training_labels.csv')\n",
    "test_df = pd.read_csv('sample_submission.csv')\n",
    "if Config.debug:\n",
    "    Config.epochs = 1\n",
    "    train_df = train_df.sample(n=50000, random_state=Config.seed).reset_index(drop=True)\n",
    "if Config.use_subset:\n",
    "    train_df = train_df.sample(frac=Config.subset_frac, random_state=Config.seed).reset_index(drop=True)\n",
    "train_df['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "test_df['file_path'] = test_df['id'].apply(lambda x :id_2_path(x,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adcdac68-2ef7-46b7-91b4-bd52e2585fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.013057893\n",
      "0.01869043\n",
      "0.005618966\n",
      "-0.01463365\n",
      "0.010516807\n"
     ]
    }
   ],
   "source": [
    "# checking magnitude of waves\n",
    "num_files = 5\n",
    "input_file_paths = train_df['file_path'].values[:num_files]\n",
    "batch_waves=np.zeros((num_files,3,4096))\n",
    "for i,input_file_path in enumerate(input_file_paths[:num_files]):\n",
    "    file_name = input_file_path.split('/')[-1].split('.npy')[0]\n",
    "    waves = np.load(input_file_path)#.astype(np.float32) # (3, 4096)\n",
    "#     batch_waves[i,:] = np.array([waves.max(axis=1),np.abs(waves).max(axis=1),np.abs(waves).min(axis=1)])\n",
    "    whitened_waves = waves#whiten(waves)\n",
    "    print(whitened_waves[2][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99e6d52c-c6fe-4ef7-bae2-e5dbfcffed6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold   \n",
       "0     0    0.500125\n",
       "      1    0.499875\n",
       "1     0    0.500125\n",
       "      1    0.499875\n",
       "2     0    0.500125\n",
       "      1    0.499875\n",
       "3     0    0.500125\n",
       "      1    0.499875\n",
       "4     0    0.500125\n",
       "      1    0.499875\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!\n",
    "skf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "splits = skf.split(train_df, train_df[\"target\"])\n",
    "train_df['fold'] = -1\n",
    "for fold, (train_index, valid_index) in enumerate(splits):\n",
    "    train_df.loc[valid_index,\"fold\"] = fold\n",
    "# train_df['fold_PL'] = train_df['fold']\n",
    "\n",
    "train_df.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7621283-4686-41fc-a66f-c99b7bfa6143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>file_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000e74ad</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/00000e74ad.npy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f4945</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/00001f4945.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000661522</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/0000661522.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00007a006a</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/00007a006a.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000a38978</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/0000a38978.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559995</th>\n",
       "      <td>ffff9a5645</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/ffff9a5645.npy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559996</th>\n",
       "      <td>ffffab0c27</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/ffffab0c27.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559997</th>\n",
       "      <td>ffffcf161a</td>\n",
       "      <td>1</td>\n",
       "      <td>./output/whiten-train-w0/ffffcf161a.npy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559998</th>\n",
       "      <td>ffffd2c403</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/ffffd2c403.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559999</th>\n",
       "      <td>fffff2180b</td>\n",
       "      <td>0</td>\n",
       "      <td>./output/whiten-train-w0/fffff2180b.npy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  target                                file_path  fold\n",
       "0       00000e74ad       1  ./output/whiten-train-w0/00000e74ad.npy     3\n",
       "1       00001f4945       0  ./output/whiten-train-w0/00001f4945.npy     0\n",
       "2       0000661522       0  ./output/whiten-train-w0/0000661522.npy     4\n",
       "3       00007a006a       0  ./output/whiten-train-w0/00007a006a.npy     0\n",
       "4       0000a38978       1  ./output/whiten-train-w0/0000a38978.npy     4\n",
       "...            ...     ...                                      ...   ...\n",
       "559995  ffff9a5645       1  ./output/whiten-train-w0/ffff9a5645.npy     3\n",
       "559996  ffffab0c27       0  ./output/whiten-train-w0/ffffab0c27.npy     1\n",
       "559997  ffffcf161a       1  ./output/whiten-train-w0/ffffcf161a.npy     2\n",
       "559998  ffffd2c403       0  ./output/whiten-train-w0/ffffd2c403.npy     1\n",
       "559999  fffff2180b       0  ./output/whiten-train-w0/fffff2180b.npy     4\n",
       "\n",
       "[560000 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1750c4-a631-4586-9f0a-400fbbae54a6",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b11e6-fe2c-4dcb-80c5-6f6e5acf3903",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "446a1dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservative transforms:  []\n",
      "aggressive transforms:  ['vflip', 'add_gaussian_noise', 'timemask', 'shuffle01', 'time_shift']\n"
     ]
    }
   ],
   "source": [
    "conserv_transform_list = []\n",
    "aggressive_transform_list = []\n",
    "conserv_transform_list_strings = []\n",
    "aggressive_transform_list_strings = []\n",
    "\n",
    "#-------------------------vflip\n",
    "if Config.vflip:\n",
    "#     trans = lambda x:-x\n",
    "    def vflip_func(x,sample_rate=2048):\n",
    "        return -x\n",
    "    def vflip_func_random(x,sample_rate=2048):\n",
    "        if np.random.random()<Config.vflip_proba:\n",
    "            return -x\n",
    "        else:\n",
    "            return x\n",
    "    if 'vflip' in Config.aggressive_aug:\n",
    "        aggressive_transform_list.append(vflip_func)\n",
    "        aggressive_transform_list_strings.append('vflip')\n",
    "    else:\n",
    "        conserv_transform_list.append(vflip_func_random)\n",
    "        conserv_transform_list_strings.append('vflip')\n",
    "#----------------------add_gaussian_noise        \n",
    "if Config.add_gaussian_noise:\n",
    "    \n",
    "    if 'add_gaussian_noise' in Config.aggressive_aug:\n",
    "        trans = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude=0.015*0.015, p=1) #tbs #0.015 is the estimated std\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('add_gaussian_noise')\n",
    "    else:\n",
    "        trans = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude=0.015*0.015, p=Config.add_gaussian_noise_proba) #tbs #0.015 is the estimated std\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('add_gaussian_noise')\n",
    "\n",
    "#--------------------------timemask\n",
    "if Config.timemask:\n",
    "    \n",
    "    if 'timemask' in Config.aggressive_aug:\n",
    "        trans = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=1)\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('timemask')\n",
    "    else:\n",
    "        trans = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=Config.timemask_proba)\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('timemask')\n",
    "\n",
    "#--------------------------shuffle01        \n",
    "def shuffle01_func(x,sample_rate=2048):\n",
    "    return x[[1,0,2]]\n",
    "def shuffle01_func_random(x,sample_rate=2048):\n",
    "    if np.random.random()<Config.shuffle01_proba: \n",
    "        return x[[1,0,2]]\n",
    "    else:\n",
    "        return x\n",
    "if Config.shuffle01:\n",
    "#     trans = lambda x:x[[1,0,2]]\n",
    "\n",
    "    if 'shuffle01' in Config.aggressive_aug:\n",
    "        aggressive_transform_list.append(shuffle01_func)\n",
    "        aggressive_transform_list_strings.append('shuffle01')\n",
    "    else:\n",
    "        conserv_transform_list.append(shuffle01_func_random)\n",
    "        conserv_transform_list_strings.append('shuffle01')\n",
    "#---------------------------time_shift\n",
    "if Config.time_shift:\n",
    "    if 'time_shift' in Config.aggressive_aug:\n",
    "        trans = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096,\n",
    "                        max_fraction=Config.time_shift_right*1.0/4096, \n",
    "                        p=1,rollover=False)#<0 means shift towards left,  fraction of total sound length\n",
    "        aggressive_transform_list.append(trans)\n",
    "        aggressive_transform_list_strings.append('time_shift')\n",
    "    else:\n",
    "        trans = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096,\n",
    "                                max_fraction=Config.time_shift_right*1.0/4096, \n",
    "                                p=Config.time_shift_proba,rollover=False)\n",
    "        conserv_transform_list.append(trans)\n",
    "        conserv_transform_list_strings.append('time_shift')\n",
    "\n",
    "#-----------------shift_channel        \n",
    "def shift_channel_func(x,sample_rate=2048):\n",
    "    channel = np.random.choice(3)\n",
    "    trans = A.Shift(min_fraction=-Config.shift_channel_left*1.0/4096,\n",
    "                max_fraction=Config.shift_channel_right*1.0/4096, \n",
    "                p=1,rollover=False)\n",
    "    x[channel] = trans(x[channel],sample_rate=2048)\n",
    "    return x\n",
    "def shift_channel_func_random(x,sample_rate=2048):\n",
    "    channel = np.random.choice(3)\n",
    "    trans = A.Shift(min_fraction=-Config.shift_channel_left*1.0/4096,\n",
    "                max_fraction=Config.shift_channel_right*1.0/4096, \n",
    "                p=Config.shift_channel_proba,rollover=False)\n",
    "    x[channel] = trans(x[channel],sample_rate=2048)\n",
    "    return x\n",
    "if Config.shift_channel:\n",
    "    if 'shift_channel' in Config.aggressive_aug:\n",
    "        \n",
    "        aggressive_transform_list.append(shift_channel_func)\n",
    "        aggressive_transform_list_strings.append('shift_channel')\n",
    "    else:\n",
    "        \n",
    "        conserv_transform_list.append(shift_channel_func_random)\n",
    "        conserv_transform_list_strings.append('shift_channel')\n",
    "#-----------------reduce_SNR        \n",
    "def reduce_SNR_func(x,sample_rate=2048):\n",
    "    x = x * Config.reduce_SNR_ratio\n",
    "    trans = A.AddGaussianNoise(min_amplitude=multiplier, max_amplitude=multiplier, p=1)\n",
    "    x = trans(x,sample_rate=2048)\n",
    "    return x \n",
    "def reduce_SNR_func_random(x,sample_rate=2048):\n",
    "    if np.random.random() < Config.reduce_SNR_proba:\n",
    "        x = x * Config.reduce_SNR_ratio\n",
    "        trans = A.AddGaussianNoise(min_amplitude=multiplier, max_amplitude=multiplier, p=1)\n",
    "        x = trans(x,sample_rate=2048)\n",
    "    return x\n",
    "if Config.reduce_SNR:\n",
    "    multiplier = math.sqrt(1-Config.reduce_SNR_ratio**2)\n",
    "    if 'reduce_SNR' in Config.aggressive_aug:\n",
    "\n",
    "        aggressive_transform_list.append(reduce_SNR_func)\n",
    "        aggressive_transform_list_strings.append('reduce_SNR')\n",
    "    else:\n",
    "\n",
    "        conserv_transform_list.append(reduce_SNR_func_random)\n",
    "        conserv_transform_list_strings.append('reduce_SNR')\n",
    "        \n",
    "# if Config.time_stretch:\n",
    "#     trans = A.TimeStretch(min_rate=0.98, max_rate=1.02,leave_length_unchanged=True, p=0.5)\n",
    "#     if 'time_stretch' in aggressive_aug:\n",
    "#         aggressive_transform_list.append(trans)\n",
    "#         aggressive_transform_list_strings.append('time_stretch')\n",
    "#     else:\n",
    "#         conserv_transform_list.append(trans)\n",
    "#         conserv_transform_list_strings.append('time_stretch')\n",
    "# if Config.pitch_shift:\n",
    "#     trans = A.PitchShift(min_semitones=-1, max_semitones=1, p=0.5)\n",
    "#     if 'pitch_shift' in aggressive_aug:\n",
    "#         aggressive_transform_list.append(trans)\n",
    "#         aggressive_transform_list_strings.append('pitch_shift')\n",
    "#     else:\n",
    "#         conserv_transform_list.append(trans)\n",
    "#         conserv_transform_list_strings.append('pitch_shift')\n",
    "# if Config.shift_channel:\n",
    "#     pass\n",
    "\n",
    "print('conservative transforms: ',conserv_transform_list_strings)\n",
    "print('aggressive transforms: ',aggressive_transform_list_strings)\n",
    "train_transform = conserv_transform_list#A.Compose(conserv_transform_list)#,OneOf(aggressive_transform_list,p=0.5)) # no OneOf in audiomentation\n",
    "# \n",
    "\n",
    "test_transform = None #A.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a66e416-c7ac-4526-b32a-96d6aa50190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [getattr(Config(), f'{agg}_weight') for agg in aggressive_transform_list_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f6f8d25-feda-4576-9080-b2e40228ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRetriever(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "\n",
    "\n",
    "        #reading data for fold 0 for fast iteration\n",
    "        if Config.continuous_exp and Config.train_folds == [0]:\n",
    "            if Config.use_pseudo_label:\n",
    "                self.data = fold_0_data_PL\n",
    "            else:\n",
    "                self.data = fold_0_data\n",
    "        else:\n",
    "            if Config.use_ram:\n",
    "                start_time =time.time()\n",
    "                array_shape = (len(self.paths),3,4096)\n",
    "                self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "                for i,path in enumerate(self.paths):\n",
    "                    waves = np.load(path)\n",
    "                    self.data[i,:] = waves            \n",
    "                print(time.time()-start_time)\n",
    "\n",
    "                \n",
    "            # saving Fold 0 data for later use\n",
    "#         with open('fold_0_data_PL.npy', 'wb') as f:\n",
    "#             np.save(f, self.data)\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if Config.use_ram:\n",
    "            waves = self.data[index]\n",
    "        else:\n",
    "            path = self.paths[index] \n",
    "            waves = np.load(path)\n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015 #causing NaN?\n",
    "\n",
    "#         if Config.shuffle_channels:#nn.ChannelShuffle\n",
    "#             if np.random.random()<0.5:\n",
    "#                 np.random.shuffle(waves)\n",
    "                \n",
    "#         if Config.vflip:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves = -waves\n",
    "            \n",
    "        if self.transforms is not None:\n",
    "            for i,_ in enumerate(self.transforms):\n",
    "                transform = conserv_transform_list[i]\n",
    "                waves= transform(waves,sample_rate=2048)\n",
    "            \n",
    "        if aggressive_transform_list_strings:\n",
    "            if np.random.random()<Config.aggressive_aug_proba:\n",
    "                n = len(aggressive_transform_list_strings)\n",
    "                probas = np.array([getattr(Config(), f'{agg}_weight') for agg in aggressive_transform_list_strings])\n",
    "                probas /= probas.sum()\n",
    "                trans_idx = np.random.choice(n,p=probas)\n",
    "                trans = aggressive_transform_list[trans_idx]\n",
    "                waves = trans(waves,sample_rate=2048)\n",
    "\n",
    "\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverTest(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms\n",
    "        if Config.use_ram:\n",
    "            array_shape = (len(self.paths),3,4096)\n",
    "            self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "            for i,path in enumerate(self.paths):\n",
    "                waves = np.load(path)\n",
    "                self.data[i,:] = waves  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if Config.use_ram:\n",
    "            waves = self.data[index]\n",
    "        else:\n",
    "            path = self.paths[index] \n",
    "            waves = np.load(path)\n",
    "            \n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "            \n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)\n",
    "\n",
    "class DataRetrieverLRFinder(Dataset):\n",
    "    def __init__(self, paths, targets, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.transforms = transforms     \n",
    "#         start_time =time.time()\n",
    "#         array_shape = (len(self.paths),3,4096)\n",
    "#         self.data = np.zeros(array_shape,dtype=np.float32)\n",
    "#         for i,path in enumerate(self.paths):\n",
    "#             waves = np.load(path)\n",
    "#             self.data[i,:] = waves\n",
    "#         print(time.time()-start_time)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "        \n",
    "#         if Config.cropping:\n",
    "#             waves = waves[:,1792:3840+1]\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "#         if Config.shuffle_channels:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 np.random.shuffle(waves)\n",
    "#         if Config.shuffle01:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves[[0,1]]=waves[[1,0]]\n",
    "#         if Config.vflip:\n",
    "#             if np.random.random()<0.5:\n",
    "#                 waves = -waves\n",
    "              \n",
    "        if self.transforms is not None:\n",
    "            waves= self.transforms(waves,sample_rate=2048)\n",
    "        waves = torch.from_numpy(waves) \n",
    "        # if Config.ta:#on tensor, batch*channel*ts\n",
    "        #     waves = self.ta_augment(waves,sample_rate=2048)\n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device, \n",
    "            \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b86ab7a-b88e-4a0f-9cb7-3a023ae12408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(aggressive_transform_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6f55460-98fb-47e8-b4b6-3d898ff751da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.choice(5,p=[0.1, 0, 0.3, 0.6, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d3d1abe-3f31-4b01-bf9a-15b300461b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    '''\n",
    "    Code modified from the 2d code in\n",
    "    https://amaarora.github.io/2020/08/30/gempool.html\n",
    "    '''\n",
    "    def __init__(self, kernel_size=8, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        with torch.cuda.amp.autocast(enabled=False):#to avoid NaN issue for fp16\n",
    "            return torch_functional.avg_pool1d(x.clamp(min=eps).pow(p), self.kernel_size).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45273271-348f-453b-a905-01e846d7bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/iafoss/mish-activation\n",
    "import torch.nn.functional as F\n",
    "class MishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        sigmoid = torch.sigmoid(x)\n",
    "        tanh_sp = torch.tanh(F.softplus(x)) \n",
    "        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return MishFunction.apply(x)\n",
    "\n",
    "def to_Mish(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, Mish())\n",
    "        else:\n",
    "            to_Mish(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd2ba5-d403-4abf-b971-2303a92ea8ea",
   "metadata": {},
   "source": [
    "## neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de5c4e68-068e-451e-aa4c-ac1e28ed5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCNN_Dilations(nn.Module):\n",
    "    \"\"\"1D convolutional neural network with dilations. Classifier of the gravitaitonal waves\n",
    "    Inspired by the https://arxiv.org/pdf/1904.08693.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.init_conv = nn.Sequential(nn.Conv1d(3, 256, kernel_size=1), nn.ReLU())\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(256, 256, kernel_size=2, dilation=2 ** i),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for i in range(11)\n",
    "            ]\n",
    "        )\n",
    "        self.out_conv = nn.Sequential(nn.Conv1d(256, 1, kernel_size=1), nn.ReLU())\n",
    "        self.fc = nn.Linear(2049, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = self.fc(x)\n",
    "        x.squeeze_(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model1DCNN(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            nn.MaxPool1d(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Model1DCNNGEM(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_channnels=8):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, initial_channnels, kernel_size=64),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels, kernel_size=32),\n",
    "            GeM(kernel_size=8),\n",
    "            nn.BatchNorm1d(initial_channnels),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels, initial_channnels * 2, kernel_size=32),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 2, kernel_size=16),\n",
    "            GeM(kernel_size=6),\n",
    "            nn.BatchNorm1d(initial_channnels * 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 2, initial_channnels * 4, kernel_size=16),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(initial_channnels * 4, initial_channnels * 4, kernel_size=16),\n",
    "            GeM(kernel_size=4),\n",
    "            nn.BatchNorm1d(initial_channnels * 4),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        if Config.cropping:\n",
    "            fm_size = tbd\n",
    "        else:\n",
    "            fm_size = 11\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(initial_channnels * 4 * fm_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(1)\n",
    "        # x = x.mean(-1)\n",
    "        # x = torch.cat([x.mean(-1), x.max(-1)[0]])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x    \n",
    "\n",
    "#--------------------------------------------------------------------------- V0\n",
    "class ExtractorMaxPool(nn.Sequential):\n",
    "    def __init__(self, in_c=8, out_c=8, kernel_size=64, maxpool=8, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_c), act,\n",
    "            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.MaxPool1d(kernel_size=maxpool),\n",
    "        )\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes))\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class ModelIafoss(nn.Module):\n",
    "    def __init__(self, n=8, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(ExtractorMaxPool(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),),\n",
    "            nn.Sequential(ExtractorMaxPool(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),)\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlock(3*n,2*n,kernel_size=31,stride=4),\n",
    "            ResBlock(2*n,2*n,kernel_size=31),\n",
    "            ResBlock(2*n,4*n,kernel_size=15,stride=4),\n",
    "            ResBlock(4*n,4*n,kernel_size=15),\n",
    "            ResBlock(4*n,8*n,kernel_size=7,stride=4),\n",
    "            ResBlock(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Flatten(),\n",
    "            nn.Linear(n*8*8,256),nn.BatchNorm1d(256),nn.Dropout(ps), act,\n",
    "            nn.Linear(256, 256),nn.BatchNorm1d(256),nn.Dropout(ps), act,\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "\n",
    "#----------------------------------------------V1    \n",
    "    \n",
    "class AdaptiveConcatPool1d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`\"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "\n",
    "# using GeM\n",
    "class Extractor(nn.Sequential):\n",
    "    def __init__(self, in_c=8, out_c=8, kernel_size=64, maxpool=8, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__(\n",
    "            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(out_c), act,\n",
    "            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "#             nn.MaxPool1d(kernel_size=maxpool),\n",
    "            GeM(kernel_size=maxpool),\n",
    "        )\n",
    "    \n",
    "class ModelIafossV1(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            ResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            ResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            ResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#for SE-----------------------------------------------------------------------------\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, int(channel // reduction), bias=False),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(int(channel // reduction), channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class SEResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True),reduction=Config.reduction):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            SELayer(out_planes, reduction)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1SE(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            SEResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            SEResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            SEResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            SEResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            SEResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            SEResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[1](x[:,1].unsqueeze(1)),\n",
    "            self.ex[2](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#for CBAM-----------------------------------------------------------------------\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, silu=True):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm1d(out_planes,eps=1e-5, momentum=0.01, affine=True) #0.01,default momentum 0.1\n",
    "        self.silu = nn.SiLU(inplace=True) if silu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.silu is not None:\n",
    "            x = self.silu(x)\n",
    "        return x\n",
    "    \n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self,kernel_size=15):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = kernel_size\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, silu=True)#silu False\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = torch.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "    \n",
    "class CBAMResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, kernel_size=3, act=nn.SiLU(inplace=True),reduction=Config.reduction):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size,\n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes), act,\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                      padding=kernel_size//2, bias=False),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            SELayer(out_planes, reduction),\n",
    "            SpatialGate(),\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_planes)\n",
    "            )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.shortcut(x))\n",
    "    \n",
    "class ModelIafossV1CBAM(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),CBAMResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          CBAMResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),CBAMResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          CBAMResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            CBAMResBlock(3*n,3*n,kernel_size=31,stride=4), #512\n",
    "            CBAMResBlock(3*n,3*n,kernel_size=31), #128\n",
    "            CBAMResBlock(3*n,4*n,kernel_size=15,stride=4), #128\n",
    "            CBAMResBlock(4*n,4*n,kernel_size=15), #32\n",
    "            CBAMResBlock(4*n,8*n,kernel_size=7,stride=4), #32\n",
    "            CBAMResBlock(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))    \n",
    "\n",
    "#---------------------------------------------------------------------------------------------------  \n",
    "    \n",
    "    \n",
    "class BasicBlockPool(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.MaxPool1d(downsample,ceil_mode=True), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    nn.MaxPool1d(downsample,ceil_mode=True),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                     nn.MaxPool1d(2,ceil_mode=True),  # downsampling \n",
    "#                 )\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "    #             self.shortcut = nn.Sequential(\n",
    "    #                     nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "    #                     nn.BatchNorm1d(out_channels),\n",
    "    #                 )#skip layers in residual_function, can try identity, i.e., nn.Sequential()\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1Pool(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            BasicBlockPool(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            BasicBlockPool(3*n,3*n,kernel_size=31), #128\n",
    "            BasicBlockPool(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            BasicBlockPool(4*n,4*n,kernel_size=15), #32\n",
    "            BasicBlockPool(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            BasicBlockPool(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------  \n",
    "    \n",
    "    \n",
    "class ResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple MaxPool1d\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ModelIafossV1GeM(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "                          ResBlock(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "#-----------------------------------------------------------------------------\n",
    "class ModelIafossV1GeMAll(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "#-----------------------------------------------------------------------------    \n",
    "class AdaptiveConcatPool1dx3(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d`,`AdaptiveMaxPool1d` and 'GeM' \"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "        self.gemp = GeM(kernel_size=8)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x),self.gemp(x)], 1)\n",
    "    \n",
    "class ModelGeMx3(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1dx3(),nn.Flatten(),\n",
    "            nn.Linear(n*8*3,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "#-----------------------------------------------------------------------------\n",
    "class ModelIafossV1GeMAllDeep(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), #128\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), #32\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "    \n",
    "class StochasticDepthResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=False),p=1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.act = act\n",
    "\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple Pooling\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "            \n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:#attribute inherited\n",
    "            if self.survival():\n",
    "                x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.act(self.shortcut(x))\n",
    "        else:\n",
    "            x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "        return x\n",
    "    \n",
    "   \n",
    "    \n",
    "class DeepStochastic(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 11\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.conv = nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,p=self.survival_proba[0]), #512\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[1]), #128\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[2]), \n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,p=self.survival_proba[3]), \n",
    "            StochasticDepthResBlockGeM(3*n,4*n,kernel_size=15,downsample=4,p=self.survival_proba[4]), #128\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[5]), #32\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[6]),\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,p=self.survival_proba[7]),\n",
    "            StochasticDepthResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,p=self.survival_proba[8]), #32\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,p=self.survival_proba[9]), #8\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,p=self.survival_proba[10]),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "#-----------------------------------------------------------------------------\n",
    "class Deeper(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), #128\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=3), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=4), #128\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3), #32\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=3),\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), #8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "    \n",
    "class Deeper2(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "            nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4),\n",
    "                          ResBlockGeM(n,n,kernel_size=31)),\n",
    "#             nn.Sequential(Extractor(1,n,63,maxpool=2,act=act),ResBlock(n,n,kernel_size=31,stride=4),\n",
    "#                           ResBlock(n,n,kernel_size=31))\n",
    "        ])\n",
    "        self.conv = nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=2), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=2), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31), \n",
    "            ResBlockGeM(3*n,4*n,kernel_size=15,downsample=2), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15,downsample=2),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15), \n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=2),\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7,downsample=2),\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7),#8\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7), \n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.ex[0](x[:,0].unsqueeze(1)),\n",
    "            self.ex[0](x[:,1].unsqueeze(1)),\n",
    "            self.ex[1](x[:,2].unsqueeze(1))],1)\n",
    "        return self.head(self.conv(x))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#-------------------------------------------------------------------V2    \n",
    "\n",
    "class ModelIafossV2(nn.Module):\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=True), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(1*n,1*n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act), #512\n",
    "            ResBlockGeM(3*n,3*n,kernel_size=31,act=act)),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act),\n",
    "            ResBlockGeM(4*n,4*n,kernel_size=15,act=act),#128\n",
    "            ResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act), #32\n",
    "            ResBlockGeM(8*n,8*n,kernel_size=7,act=act), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "    \n",
    "#-----------------------------------\n",
    "class V2StochasticDepth(nn.Module):#stocnot on ex\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        \n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 10\n",
    "#         self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "#         self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(1,num_block+1)]\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[0]), #512\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[1])),\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[2]), #512\n",
    "            StochasticDepthResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[3])),\n",
    "            nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[4]), #512\n",
    "            StochasticDepthResBlockGeM(3*n,3*n,kernel_size=31,act=act,p=self.survival_proba[5])),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            StochasticDepthResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act,p=self.survival_proba[6]),\n",
    "            StochasticDepthResBlockGeM(4*n,4*n,kernel_size=15,act=act,p=self.survival_proba[7]),#128\n",
    "            StochasticDepthResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act,p=self.survival_proba[8]), #32\n",
    "            StochasticDepthResBlockGeM(8*n,8*n,kernel_size=7,act=act,p=self.survival_proba[9]), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "    \n",
    "#for StochasticCBAM-----------------------------------------------------------------------\n",
    "    \n",
    "class StochasticCBAMResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, \n",
    "                 downsample=1, act=nn.SiLU(inplace=False),p=1,reduction=1.0):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.act = act\n",
    "\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                SELayer(out_channels, reduction),\n",
    "                SpatialGate(Config.CBAM_SG_kernel_size),\n",
    "                GeM(kernel_size=downsample), # downsampling \n",
    "            )\n",
    "            self.shortcut = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    GeM(kernel_size=downsample),  # downsampling \n",
    "                )#skip layers in residual_function, can try simple Pooling\n",
    "        else:\n",
    "            self.residual_function = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act,\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                SELayer(out_channels, reduction),\n",
    "                SpatialGate(Config.CBAM_SG_kernel_size),\n",
    "            )\n",
    "            self.shortcut = nn.Sequential()\n",
    "            \n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:#attribute inherited\n",
    "            if self.survival():\n",
    "                x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.act(self.shortcut(x))\n",
    "        else:\n",
    "            x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "        return x \n",
    "\n",
    "    \n",
    "class V2SDCBAM(nn.Module):#stocnot on ex\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        \n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 10\n",
    "#         self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "#         self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(1,num_block+1)]\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            StochasticCBAMResBlock(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[0],reduction=Config.reduction), #512\n",
    "            StochasticCBAMResBlock(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[1],reduction=Config.reduction)),\n",
    "            nn.Sequential(\n",
    "            StochasticCBAMResBlock(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[2],reduction=Config.reduction), #512\n",
    "            StochasticCBAMResBlock(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[3],reduction=Config.reduction)),\n",
    "            nn.Sequential(\n",
    "            StochasticCBAMResBlock(3*n,3*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[4],reduction=Config.reduction), #512\n",
    "            StochasticCBAMResBlock(3*n,3*n,kernel_size=31,act=act,p=self.survival_proba[5],reduction=Config.reduction)),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            StochasticCBAMResBlock(6*n,4*n,kernel_size=15,downsample=4,act=act,p=self.survival_proba[6],reduction=Config.reduction),\n",
    "            StochasticCBAMResBlock(4*n,4*n,kernel_size=15,act=act,p=self.survival_proba[7],reduction=Config.reduction),#128\n",
    "            StochasticCBAMResBlock(4*n,8*n,kernel_size=7,downsample=4,act=act,p=self.survival_proba[8],reduction=Config.reduction), #32\n",
    "            StochasticCBAMResBlock(8*n,8*n,kernel_size=7,act=act,p=self.survival_proba[9],reduction=Config.reduction), #8\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n",
    "    \n",
    "#BoT---------------------------------------------------------------------------------------------    \n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, n_dims, length, heads=4):\n",
    "        super(MHSA, self).__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "        self.query = nn.Conv1d(n_dims, n_dims, kernel_size=1)\n",
    "        self.key = nn.Conv1d(n_dims, n_dims, kernel_size=1)\n",
    "        self.value = nn.Conv1d(n_dims, n_dims, kernel_size=1)\n",
    "        self.rel_pos = nn.Parameter(torch.randn([1, heads, n_dims // heads, length]), requires_grad=True)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_batch, C, width, height = x.size()\n",
    "        q = self.query(x).view(n_batch, self.heads, C // self.heads, -1)\n",
    "        k = self.key(x).view(n_batch, self.heads, C // self.heads, -1)\n",
    "        v = self.value(x).view(n_batch, self.heads, C // self.heads, -1)\n",
    "\n",
    "        content_content = torch.matmul(q.permute(0, 2, 1), k)\n",
    "\n",
    "        content_position = self.rel_pos.view(1, self.heads, C // self.heads, -1).permute(0, 2, 1)\n",
    "        content_position = torch.matmul(content_position, q)\n",
    "\n",
    "        energy = content_content + content_position\n",
    "        attention = self.softmax(energy)\n",
    "\n",
    "        out = torch.matmul(v, attention.permute(0, 2, 1))\n",
    "        out = out.view(n_batch, C, width, height)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BoTCBAMSDResBlockGeM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, downsample=1, act=nn.SiLU(inplace=False),p=1,mhsa=False,heads=4,length=None):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.act = act\n",
    "        \n",
    "        \n",
    "        layers = nn.ModuleList()\n",
    "        layers.append(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False))\n",
    "        layers.append(nn.BatchNorm1d(out_channels))\n",
    "        layers.append(act)\n",
    "        if mhsa:\n",
    "            layers.append(MHSA(out_channels, length=length, heads=heads))\n",
    "        else:\n",
    "            layers.append(nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False))\n",
    "        layers.append(nn.BatchNorm1d(out_channels))\n",
    "        layers.append(SELayer(out_channels, reduction))\n",
    "        layers.append(SpatialGate(Config.CBAM_SG_kernel_size))\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            layers.append(GeM(kernel_size=downsample))\n",
    "        self.residual_function = nn.Sequential(*layers)\n",
    "        \n",
    "        sc_layers = nn.ModuleList()\n",
    "        if downsample != 1 or in_channels != out_channels:\n",
    "            sc_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2, bias=False))\n",
    "            sc_layers.append(nn.BatchNorm1d(out_channels))\n",
    "            sc_layers.append(GeM(kernel_size=downsample))\n",
    "        self.shortcut = nn.Sequential(*sc_layers)\n",
    "        \n",
    "            \n",
    "    def survival(self):\n",
    "        var = torch.bernoulli(torch.tensor(self.p).float())#,device=device)\n",
    "        return torch.equal(var,torch.tensor(1).float().to(var.device,non_blocking=Config.non_blocking))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:#attribute inherited\n",
    "            if self.survival():\n",
    "                x = self.act(self.residual_function(x) + self.shortcut(x))\n",
    "            else:\n",
    "                x = self.act(self.shortcut(x))\n",
    "        else:\n",
    "            x = self.act(self.residual_function(x) * self.p + self.shortcut(x))  \n",
    "        return x\n",
    "\n",
    "    \n",
    "class BoTCBAMV2SD(nn.Module):#stocnot on ex\n",
    "    def __init__(self, n=8, nh=256, act=nn.SiLU(inplace=False), ps=0.5):\n",
    "        super().__init__()\n",
    "        self.ex = nn.ModuleList([\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act)),\n",
    "            nn.Sequential(Extractor(1,n,127,maxpool=2,act=act),ResBlockGeM(n,n,kernel_size=31,downsample=4,act=act),\n",
    "                          ResBlockGeM(n,n,kernel_size=31,act=act))\n",
    "        ])\n",
    "        self.length = 4096/8#tbs, bad code style\n",
    "        proba_final_layer = Config.stochastic_final_layer_proba \n",
    "        num_block = 10\n",
    "#         self.proba_step = (1-proba_final_layer)/(num_block-1)\n",
    "#         self.survival_proba = [1-i*self.proba_step for i in range(num_block)]\n",
    "        self.proba_step = (1-proba_final_layer)/(num_block)\n",
    "        self.survival_proba = [1-i*self.proba_step for i in range(1,num_block+1)]\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            BoTCBAMSDResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[0],reduction=Config.reduction), #128\n",
    "            BoTCBAMSDResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[1],reduction=Config.reduction)),\n",
    "            nn.Sequential(\n",
    "            BoTCBAMSDResBlockGeM(1*n,1*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[2],reduction=Config.reduction), #128\n",
    "            BoTCBAMSDResBlockGeM(1*n,1*n,kernel_size=31,act=act,p=self.survival_proba[3],reduction=Config.reduction)),\n",
    "            nn.Sequential(\n",
    "            BoTCBAMSDResBlockGeM(3*n,3*n,kernel_size=31,downsample=4,act=act,p=self.survival_proba[4],reduction=Config.reduction), #128\n",
    "            BoTCBAMSDResBlockGeM(3*n,3*n,kernel_size=31,act=act,p=self.survival_proba[5],reduction=Config.reduction)),#128\n",
    "            ])\n",
    "        self.conv2 = nn.Sequential(\n",
    "            BoTCBAMSDResBlockGeM(6*n,4*n,kernel_size=15,downsample=4,act=act,p=self.survival_proba[6],reduction=Config.reduction,length=self.length/16),#32\n",
    "            BoTCBAMSDResBlockGeM(4*n,4*n,kernel_size=15,act=act,p=self.survival_proba[7],reduction=Config.reduction,length=self.length/16),#32\n",
    "            BoTCBAMSDResBlockGeM(4*n,8*n,kernel_size=7,downsample=4,act=act,p=self.survival_proba[8],reduction=Config.reduction,mhsa=True,length=self.length/64), #8 #mhsa for last stage\n",
    "            BoTCBAMSDResBlockGeM(8*n,8*n,kernel_size=7,act=act,p=self.survival_proba[9],reduction=Config.reduction,mhsa=True,length=self.length/64), #8 #mhsa for last stage\n",
    "        )\n",
    "        self.head = nn.Sequential(AdaptiveConcatPool1d(),nn.Flatten(),\n",
    "            nn.Linear(n*8*2,nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, nh),nn.BatchNorm1d(nh),nn.Dropout(ps), act,\n",
    "            nn.Linear(nh, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = [self.ex[0](x[:,0].unsqueeze(1)),self.ex[0](x[:,1].unsqueeze(1)),\n",
    "              self.ex[1](x[:,2].unsqueeze(1))]\n",
    "        x1 = [self.conv1[0](x0[0]),self.conv1[0](x0[1]),self.conv1[1](x0[2]),\n",
    "              self.conv1[2](torch.cat([x0[0],x0[1],x0[2]],1))]\n",
    "        x2 = torch.cat(x1,1)\n",
    "        return self.head(self.conv2(x2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e1528ce-06af-4d53-84d9-bbbcaf4991b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    model_name = Config.model_module \n",
    "    if model_name == 'Model1DCNN':\n",
    "        model = Model1DCNN(Config.channels)\n",
    "    elif model_name == 'Model1DCNNGEM':\n",
    "        model = Model1DCNNGEM(Config.channels)\n",
    "    elif model_name == 'ModelIafoss':\n",
    "        model = ModelIafoss(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1':\n",
    "        model = ModelIafossV1(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1SE':\n",
    "        model = ModelIafossV1SE(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1CBAM':\n",
    "        model = ModelIafossV1CBAM(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1Pool':\n",
    "        model = ModelIafossV1Pool(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeM':\n",
    "        model = ModelIafossV1GeM(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeMAll':\n",
    "        model = ModelIafossV1GeMAll(Config.channels)\n",
    "    elif model_name == 'ModelGeMx3':\n",
    "        model = ModelGeMx3(Config.channels)\n",
    "    elif model_name == 'ModelIafossV1GeMAllDeep':\n",
    "        model = ModelIafossV1GeMAllDeep(Config.channels)\n",
    "    elif model_name == 'DeepStochastic':\n",
    "        model = DeepStochastic(Config.channels)\n",
    "    elif model_name == 'Deeper':\n",
    "        model = Deeper(Config.channels)\n",
    "    elif model_name == 'Deeper2':\n",
    "        model = Deeper2(Config.channels)\n",
    "    elif model_name == 'ModelIafossV2':\n",
    "        model = ModelIafossV2(Config.channels)\n",
    "    elif model_name == 'ModelIafossV2Mish':\n",
    "        model = ModelIafossV2(Config.channels,act=Mish())\n",
    "    elif model_name == 'ModelIafossV2Elu':\n",
    "        model = ModelIafossV2(Config.channels,act=torch.nn.ELU())\n",
    "    elif model_name == 'V2StochasticDepth':\n",
    "        model = V2StochasticDepth(Config.channels)\n",
    "    elif model_name == 'V2SDCBAM':\n",
    "        model = V2SDCBAM(Config.channels)\n",
    "    elif model_name == 'BoTCBAMV2SD':\n",
    "        model = BoTCBAMV2SD(Config.channels)\n",
    "    elif model_name == 'BoTV2SD':\n",
    "        model = BoTV2SD(Config.channels)\n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "#     elif model_name == '':\n",
    "#         model = \n",
    "    print(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ce418f8-18dc-40c4-ac7b-fe4384f44245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2SDCBAM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6218971"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "model = Model()#can possibly call random\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c9f2c-fc5f-41ee-afa5-2dfbec2ef82f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b76b69f-3f6d-40dd-92e9-b04be3232814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(seed=Config.seed)    \n",
    "\n",
    "def get_scheduler(optimizer, train_size):\n",
    "    if Config.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=Config.factor, \n",
    "                                      patience=Config.patience, verbose=True, eps=Config.eps)\n",
    "    elif Config.scheduler=='CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, \n",
    "                                      T_max=Config.T_max, \n",
    "                                      eta_min=Config.min_lr, last_epoch=-1)\n",
    "    elif Config.scheduler=='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                T_0=Config.T_0, \n",
    "                                                T_mult=1, \n",
    "                                                eta_min=Config.min_lr, \n",
    "                                                last_epoch=-1)\n",
    "    elif Config.scheduler=='CyclicLR':\n",
    "        iter_per_ep = train_size/Config.batch_size\n",
    "        step_size_up = int(iter_per_ep*Config.step_up_epochs)\n",
    "        step_size_down=int(iter_per_ep*Config.step_down_epochs)\n",
    "        scheduler = CyclicLR(optimizer, \n",
    "                             base_lr=Config.base_lr, \n",
    "                             max_lr=Config.max_lr,\n",
    "                             step_size_up=step_size_up,\n",
    "                             step_size_down=step_size_down,\n",
    "                             mode=Config.mode,\n",
    "                             gamma=Config.cycle_decay**(1/(step_size_up+step_size_down)),\n",
    "                             cycle_momentum=False)\n",
    "        \n",
    "    elif Config.scheduler == 'cosineWithWarmUp':\n",
    "        epoch_step = train_size/Config.batch_size\n",
    "        num_warmup_steps = int(0.1 * epoch_step * Config.epochs)\n",
    "        num_training_steps = int(epoch_step * Config.epochs)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps=num_warmup_steps, \n",
    "                                                    num_training_steps=num_training_steps)      \n",
    "    return scheduler\n",
    "def mixed_criterion(loss_fn, pred, y_a, y_b, lam):\n",
    "    return lam * loss_fn(pred, y_a) + (1 - lam) * loss_fn(pred, y_b)\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size, requires_grad=False).to(x.device,non_blocking=Config.non_blocking)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "498ba49a-52a8-4e90-9b51-4fb267a03350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-PCIE-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.1 GB\n",
      "Reserved:    16.6 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "if Config.use_tpu:\n",
    "    device = xm.xla_device()\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')#for debug, tb see\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "# watch nvidia-smi\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Reserved:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588c2fb-6f2f-4a71-ae80-af4e7facbf40",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1791f02a-cd63-448f-ad07-a216643c196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        torch.save(model.state_dict(), f'{Config.model_output_folder}/init_params.pt')\n",
    "\n",
    "    def range_test(self, loader, end_lr = 10, num_iter = 100, \n",
    "                   smooth_f = 0.05, diverge_th = 5):\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        for step, batch in enumerate(loader):\n",
    "            if step == num_iter:\n",
    "                break\n",
    "            loss = self._train_batch(batch)\n",
    "            lrs.append(lr_scheduler.get_last_lr()[0])\n",
    "            #update lr\n",
    "            lr_scheduler.step()\n",
    "            if step > 0:\n",
    "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "            losses.append(loss)\n",
    "            if loss > diverge_th * best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "        #reset model to initial parameters\n",
    "        model.load_state_dict(torch.load(f'{Config.model_output_folder}/init_params.pt'))\n",
    "        return lrs, losses\n",
    "\n",
    "    def _train_batch(self, batch):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        scaler = GradScaler()\n",
    "        X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "        targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "        \n",
    "        if Config.use_mixup:\n",
    "            (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                X, targets, Config.mixup_alpha\n",
    "            )\n",
    "            with autocast(enabled=False):\n",
    "                outputs = self.model(X_mix).squeeze()\n",
    "                loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "        else:\n",
    "            with autocast(enabled=False):\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "        #loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if Config.use_tpu:\n",
    "            xm.optimizer_step(self.optimizer, barrier=True)  # Note: TPU-specific code! \n",
    "        else:\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "#             self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "                    \n",
    "class ExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "def plot_lr_finder(lrs, losses, skip_start = 0, skip_end = 0):\n",
    "    if skip_end == 0:\n",
    "        lrs = lrs[skip_start:]\n",
    "        losses = losses[skip_start:]\n",
    "    else:\n",
    "        lrs = lrs[skip_start:-skip_end]\n",
    "        losses = losses[skip_start:-skip_end]\n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Learning rate')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.grid(True, 'both', 'x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0acca149-21fd-4ee7-9195-6e9c4ab3343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2SDCBAM\n"
     ]
    }
   ],
   "source": [
    "if Config.use_lr_finder:\n",
    "    START_LR = 1e-7\n",
    "    model = Model()\n",
    "    model.to(device,non_blocking=Config.non_blocking)\n",
    "    optimizer = AdamW(model.parameters(), lr=START_LR, weight_decay=Config.weight_decay, amsgrad=False)\n",
    "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    train_data_retriever = DataRetrieverLRFinder(train_df['file_path'], train_df[\"target\"].values)\n",
    "    train_loader = DataLoader(train_data_retriever,\n",
    "                                batch_size=Config.batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=Config.num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a155a5-34a6-4f13-bca8-ea8efaade09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13a2e828-f63d-4564-874b-b56924fb62e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "CPU times: user 27.9 s, sys: 3.59 s, total: 31.5 s\n",
      "Wall time: 30.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if Config.use_lr_finder:\n",
    "    try:\n",
    "        END_LR = 10\n",
    "        NUM_ITER = 150\n",
    "        lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "        lrs, losses = lr_finder.range_test(train_loader, END_LR, NUM_ITER)\n",
    "    except RuntimeError as e:\n",
    "        del model, optimizer, criterion, train_data_retriever, train_loader, lr_finder\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c0570b0-2648-4761-993f-66b8aa6e5118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if Config.use_lr_finder:\n",
    "    plot_lr_finder(lrs[:-18], losses[:-18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfb7f1-6e67-4dbe-9244-2d08a880b108",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70036313-ffa6-4249-a4b8-faadad8bafa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        scheduler,\n",
    "        valid_labels,\n",
    "        best_valid_score,\n",
    "        fold,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.best_valid_score = best_valid_score\n",
    "        self.valid_labels = valid_labels\n",
    "        self.fold = fold\n",
    "\n",
    "    \n",
    "    def fit(self, epochs, train_loader, valid_loader, save_path): \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "#         global N_EPOCH_EXPLICIT  #tbs later\n",
    "        for n_epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch: ', n_epoch)\n",
    "            N_EPOCH_EXPLICIT = n_epoch\n",
    "            train_loss, train_preds = self.train_epoch(train_loader)\n",
    "            valid_loss, valid_preds = self.valid_epoch(valid_loader)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            if isinstance(self.scheduler, ReduceLROnPlateau):\n",
    "                self.scheduler.step(valid_loss)\n",
    "            valid_score = get_score(self.valid_labels, valid_preds)\n",
    "\n",
    "            numbers = valid_score\n",
    "            filename = Config.model_output_folder+f'score_epoch_{n_epoch}.json'          \n",
    "            with open(filename, 'w') as file_object: \n",
    "                json.dump(numbers, file_object) \n",
    "            \n",
    "\n",
    "            if self.best_valid_score < valid_score:\n",
    "                self.best_valid_score = valid_score\n",
    "                self.save_model(n_epoch, save_path+f'best_model.pth', train_preds, valid_preds)\n",
    "\n",
    "            print('train_loss: ',train_loss)\n",
    "            print('valid_loss: ',valid_loss)\n",
    "            print('valid_score: ',valid_score)\n",
    "            print('best_valid_score: ',self.best_valid_score)\n",
    "            print('time used: ', time.time()-start_time)\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] epoch\": n_epoch+1, \n",
    "                      f\"[fold{self.fold}] avg_train_loss\": train_loss, \n",
    "                      f\"[fold{self.fold}] avg_val_loss\": valid_loss,\n",
    "                      f\"[fold{self.fold}] val_score\": valid_score})        \n",
    "\n",
    "        # fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "        # ax.plot(list(range(epochs)), train_losses, label=\"train_loss\")\n",
    "        # ax.plot(list(range(epochs)), valid_losses, label=\"val_loss\")\n",
    "        # fig.legend()\n",
    "        # plt.show()            \n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        if Config.amp:\n",
    "            scaler = GradScaler()\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        train_loss = 0\n",
    "        # preds = []\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            self.optimizer.zero_grad()\n",
    "            X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "            targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "            \n",
    "            if Config.use_mixup:\n",
    "                (X_mix, targets_a, targets_b, lam) = mixup_data(\n",
    "                    X, targets, Config.mixup_alpha\n",
    "                )\n",
    "                with autocast(enabled=False):\n",
    "                    outputs = self.model(X_mix).squeeze()\n",
    "                    loss = mixed_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                with autocast(enabled=False):\n",
    "                    outputs = self.model(X).squeeze()\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "\n",
    "                \n",
    "            if Config.gradient_accumulation_steps > 1:\n",
    "                loss = loss / Config.gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "          \n",
    "            if (step) % Config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "\n",
    "            if (not isinstance(self.scheduler, ReduceLROnPlateau)):\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # preds.append(outputs.sigmoid().to('cpu').detach().numpy())\n",
    "            loss2 = loss.detach()\n",
    "\n",
    "            wandb.log({f\"[fold{self.fold}] loss\": loss2,\n",
    "                       f\"[fold{self.fold}] lr\": self.scheduler.get_last_lr()[0]})            \n",
    "\n",
    "            # losses.append(loss2.item())\n",
    "            losses.append(loss2)\n",
    "            train_loss += loss2\n",
    "\n",
    "            if (step) % Config.print_num_steps == 0:\n",
    "                train_loss = train_loss.item() #synch once per print_num_steps instead of once per batch\n",
    "                print(f'[{step}/{len(train_loader)}] ', \n",
    "                      f'avg loss: ',train_loss/step,\n",
    "                      f'inst loss: ', loss2.item())\n",
    "                \n",
    "        # predictions = np.concatenate(preds)\n",
    "\n",
    "#         losses_avg = []\n",
    "#         for i, loss in enumerate(losses):\n",
    "#             if i == 0 :\n",
    "#                 losses_avg.append(loss)\n",
    "#             else:\n",
    "#                 losses_avg.append(losses_avg[-1] * 0.6 + loss * 0.4)\n",
    "#         losses = torch.stack(losses)\n",
    "#         losses_avg = torch.stack(losses_avg)\n",
    "#         fig,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "#         ax.plot(list(range(step)), losses, label=\"train_loss per step\")\n",
    "#         ax.plot(list(range(step)), losses_avg, label=\"train_loss_avg per step\")\n",
    "#         fig.legend()\n",
    "#         plt.show()            \n",
    "        \n",
    "        return train_loss / step, None#, predictions\n",
    "\n",
    "    def valid_epoch(self, valid_loader):\n",
    "        self.model.eval()      \n",
    "        valid_loss = []\n",
    "        preds = []\n",
    "        for step, batch in enumerate(valid_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                X = batch[0].to(self.device,non_blocking=Config.non_blocking)\n",
    "                targets = batch[1].to(self.device,non_blocking=Config.non_blocking)\n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                if Config.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / Config.gradient_accumulation_steps\n",
    "                valid_loss.append(loss.detach().item())\n",
    "                preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "#                 valid_loss.append(loss.detach())#.item())\n",
    "#                 preds.append(outputs.sigmoid())#.to('cpu').numpy())\n",
    "#         valid_loss = torch.cat(valid_loss).to('cpu').numpy()\n",
    "#         predictions = torch.cat(preds).to('cpu').numpy()\n",
    "        predictions = np.concatenate(preds)\n",
    "        return np.mean(valid_loss), predictions\n",
    "\n",
    "    def save_model(self, n_epoch, save_path, train_preds, valid_preds):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"best_valid_score\": self.best_valid_score,\n",
    "                \"n_epoch\": n_epoch,\n",
    "                'scheduler': self.scheduler.state_dict(),\n",
    "                'train_preds': train_preds,\n",
    "                'valid_preds': valid_preds,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0dfd0b-83ea-48f9-aca0-7e84b3346689",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f26270c-7a97-4d8a-80bd-db5ec6ad345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(seed=Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4cce08b8-36a9-4fe2-b764-b612927f3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_PL(fold):\n",
    "#     up_thresh = Config.up_thresh\n",
    "#     down_thresh = Config.down_thresh\n",
    "#     pseudo_label_df = pd.read_csv(Config.pseudo_label_folder + f\"test_Fold_{fold}.csv\") \n",
    "#     pseudo_label_df.head()\n",
    "#     pseudo_label_df[\"target\"] = pseudo_label_df[f'preds_Fold_{fold}']#or adding tta\n",
    "#     num_test = pseudo_label_df.shape[0]\n",
    "#     num_yes = (pseudo_label_df[\"target\"] >= up_thresh).sum()\n",
    "#     num_no = (pseudo_label_df[\"target\"] <= down_thresh).sum()\n",
    "#     num_all = num_yes+num_no\n",
    "#     print(\"{:.2%} ratio, {:.2%} 1, {:.2%} 0\".format(num_all/num_test, num_yes/num_test, num_no/num_test))\n",
    "#     print(num_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae6c66a1-0cda-4447-b5c8-017af0d03874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Config.use_pseudo_label:\n",
    "#     for fold in Config.train_folds:\n",
    "#         check_PL(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aed9b-d467-49b5-9611-8a170be6d00a",
   "metadata": {},
   "source": [
    "## non-leaky PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "14d354a8-d7e3-4bbb-85ee-f5e6a0126652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_PL(fold,up_thresh,down_thresh,train_df,test_df):\n",
    "    pseudo_label_df = pd.read_csv(Config.pseudo_label_folder + f\"test_Fold_{fold}.csv\") \n",
    "    \n",
    "    #soft labels\n",
    "    pseudo_label_df[\"target\"] = pseudo_label_df[f'preds_Fold_{fold}']\n",
    "    \n",
    "    #harden labels\n",
    "#     test_df_2 = pseudo_label_df[(pseudo_label_df[\"target\"] >= up_thresh) | (pseudo_label_df[\"target\"] <= down_thresh)].copy()\n",
    "#     test_df_2[\"target\"] = (test_df_2[\"target\"] >= up_thresh).astype(int)\n",
    "#     test_df_2 = test_df_2.merge(test_df[[\"id\",\"file_path\"]],on=\"id\",how=\"left\") #no need for this line if already has path\n",
    "    test_df_2 = pseudo_label_df.copy()\n",
    "    test_df_2['fold'] = Config.n_fold\n",
    "    PL_train_df = pd.concat([train_df, test_df_2]).reset_index(drop=True)\n",
    "    PL_train_df.reset_index(inplace=True, drop=True)\n",
    "#         display(train_df_PL.groupby('fold')['target'].apply(lambda s: s.value_counts(normalize=True)))\n",
    "#         display(train_df_PL.shape)\n",
    "#         display(train_df_PL)\n",
    "    return PL_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bdaf0da2-d079-4982-bb12-8ff2ea2751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_PL(fold,Config.up_thresh,Config.down_thresh,train_df.copy(),test_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1aff774-130a-4ba1-9132-5a7a60553992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_df, use_checkpoint=Config.use_checkpoint):\n",
    "    kf = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\n",
    "    avg_best_valid_score = 0\n",
    "    folds_val_score = []\n",
    "    original_train_df = train_df.copy()#for PL\n",
    "    for fold in range(Config.n_fold): \n",
    "        if Config.use_pseudo_label:\n",
    "            PL_train_df = generate_PL(fold,Config.up_thresh,Config.down_thresh,original_train_df.copy(),test_df)   \n",
    "            train_df = PL_train_df\n",
    "        train_index, valid_index = train_df.query(f\"fold!={fold}\").index, train_df.query(f\"fold=={fold}\").index #fold means fold_valid \n",
    "        print('Fold: ', fold)\n",
    "        if fold not in Config.train_folds:\n",
    "            print(\"skip\")\n",
    "            continue\n",
    "        train_X, valid_X = train_df.loc[train_index], train_df.loc[valid_index]\n",
    "        valid_labels = train_df.loc[valid_index,Config.target_col].values\n",
    "#         fold_indices = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv')#saved fold ids\n",
    "        oof = pd.DataFrame()\n",
    "        oof['id'] = train_df.loc[valid_index,'id']\n",
    "        oof['id'] = valid_X['id'].values.copy()\n",
    "        oof = oof.reset_index()\n",
    "        # assert oof['id'].eq(fold_indices['id']).all()\n",
    "#         if not Config.use_subset:\n",
    "#             assert oof['id'].eq(fold_indices['id']).sum()==112000\n",
    "        oof['target'] = valid_labels\n",
    "        \n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        # continue # uncomment this is to check oof ids\n",
    "\n",
    "        print('training data samples, val data samples: ', len(train_X) ,len(valid_X))\n",
    "        train_data_retriever = DataRetriever(train_X[\"file_path\"].values, train_X[\"target\"].values, transforms=train_transform)#how to run this only once and use for next experiment?\n",
    "        valid_data_retriever = DataRetrieverTest(valid_X[\"file_path\"].values, valid_X[\"target\"].values, transforms=test_transform)        \n",
    "        train_loader = DataLoader(train_data_retriever,\n",
    "                                  batch_size=Config.batch_size, \n",
    "                                  shuffle=True, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "        valid_loader = DataLoader(valid_data_retriever, \n",
    "                                  batch_size=Config.batch_size * 2, \n",
    "                                  shuffle=False, \n",
    "                                  num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        model = Model()\n",
    "        model.to(device,non_blocking=Config.non_blocking)\n",
    "        optimizer = AdamW(model.parameters(), lr=Config.lr,eps=1e-04, weight_decay=Config.weight_decay, amsgrad=False) #eps to avoid NaN/Inf in training loss\n",
    "        scheduler = get_scheduler(optimizer, len(train_X))\n",
    "        best_valid_score = -np.inf\n",
    "        if use_checkpoint:\n",
    "            print(\"Load Checkpoint, epo\")\n",
    "            checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_valid_score = float(checkpoint['best_valid_score'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        \n",
    "        \n",
    "        criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "        \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model, \n",
    "            device, \n",
    "            optimizer, \n",
    "            criterion,\n",
    "            scheduler,\n",
    "            valid_labels,\n",
    "            best_valid_score,\n",
    "            fold\n",
    "        )\n",
    "\n",
    "        history = trainer.fit(\n",
    "            epochs=Config.epochs, \n",
    "            train_loader=train_loader, \n",
    "            valid_loader=valid_loader,\n",
    "            save_path=f'{Config.model_output_folder}/Fold_{fold}_',\n",
    "        )\n",
    "        folds_val_score.append(trainer.best_valid_score)\n",
    "        del train_data_retriever\n",
    "    wandb.finish()\n",
    "    print('folds score:', folds_val_score)\n",
    "    print(\"Avg: {:.5f}\".format(np.mean(folds_val_score)))\n",
    "    print(\"Std: {:.5f}\".format(np.std(folds_val_score)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36681e6-4f7d-4d6c-b54f-57d800d63016",
   "metadata": {},
   "source": [
    "# Weight & Bias Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d285326b-438d-40af-a1db-eb618cf145c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"1b0833b15e81d54fad9cfbbe3d923f57562a6f89\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d12bd58-7094-43db-85b9-9071b0fecfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">124th_V2SDCBAM_PL_6ep_2em3lr_32ch_vf+gn+sc01+tm+ts</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kaggle_go/G2Net\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/95awpvh6\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/95awpvh6</a><br/>\n",
       "                Run data is saved locally in <code>/home/wandb/run-20210926_174629-95awpvh6</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_type= \"debug\" if Config.debug else \"train\"\n",
    "# run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=job_type)\n",
    "run = wandb.init(project=\"G2Net\", name=Config.model_version, config=class2dict(Config), group=Config.model_name, job_type=Config.model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f8adc-4368-48be-8015-cf9e1e9719ab",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3560abc6-3e87-4021-84e8-abddfd72130a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "training data samples, val data samples:  674000 112000\n",
      "V2SDCBAM\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.5110068620954241 inst loss:  0.44457563757896423\n",
      "[700/2633]  avg loss:  0.4687482561383929 inst loss:  0.44863206148147583\n",
      "[1050/2633]  avg loss:  0.4513379196893601 inst loss:  0.3859938383102417\n",
      "[1400/2633]  avg loss:  0.44009647914341515 inst loss:  0.40524232387542725\n",
      "[1750/2633]  avg loss:  0.43237193080357145 inst loss:  0.35278525948524475\n",
      "[2100/2633]  avg loss:  0.4267700776599702 inst loss:  0.362128883600235\n",
      "[2450/2633]  avg loss:  0.422255859375 inst loss:  0.39610034227371216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.4204, device='cuda:0')\n",
      "valid_loss:  0.42637498778839633\n",
      "valid_score:  0.8750649642897745\n",
      "best_valid_score:  0.8750649642897745\n",
      "time used:  570.5144779682159\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.39110613141741074 inst loss:  0.42305994033813477\n",
      "[700/2633]  avg loss:  0.39087223597935267 inst loss:  0.41152483224868774\n",
      "[1050/2633]  avg loss:  0.3901913597470238 inst loss:  0.3835468590259552\n",
      "[1400/2633]  avg loss:  0.39009695870535716 inst loss:  0.3615749776363373\n",
      "[1750/2633]  avg loss:  0.3894356515066964 inst loss:  0.35822197794914246\n",
      "[2100/2633]  avg loss:  0.3887672061011905 inst loss:  0.3435099124908447\n",
      "[2450/2633]  avg loss:  0.38842372349330356 inst loss:  0.36138856410980225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3882, device='cuda:0')\n",
      "valid_loss:  0.4057838060812319\n",
      "valid_score:  0.8787377674529983\n",
      "best_valid_score:  0.8787377674529983\n",
      "time used:  570.299928188324\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3806362043108259 inst loss:  0.42745405435562134\n",
      "[700/2633]  avg loss:  0.3813029261997768 inst loss:  0.381150484085083\n",
      "[1050/2633]  avg loss:  0.3823302060081845 inst loss:  0.437661349773407\n",
      "[1400/2633]  avg loss:  0.38204254150390626 inst loss:  0.39919450879096985\n",
      "[1750/2633]  avg loss:  0.3820068359375 inst loss:  0.3569439649581909\n",
      "[2100/2633]  avg loss:  0.38160298665364584 inst loss:  0.35844627022743225\n",
      "[2450/2633]  avg loss:  0.3814984006297832 inst loss:  0.4145825505256653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3813, device='cuda:0')\n",
      "valid_loss:  0.40118193218152814\n",
      "valid_score:  0.8798835192784342\n",
      "best_valid_score:  0.8798835192784342\n",
      "time used:  571.341135263443\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3778019060407366 inst loss:  0.38965171575546265\n",
      "[700/2633]  avg loss:  0.37742723737444195 inst loss:  0.3905324637889862\n",
      "[1050/2633]  avg loss:  0.3769336809430804 inst loss:  0.37901031970977783\n",
      "[1400/2633]  avg loss:  0.3766016932896205 inst loss:  0.3427102863788605\n",
      "[1750/2633]  avg loss:  0.3763491908482143 inst loss:  0.397398978471756\n",
      "[2100/2633]  avg loss:  0.37605657668340775 inst loss:  0.38928139209747314\n",
      "[2450/2633]  avg loss:  0.37549592932876275 inst loss:  0.3957338333129883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3754, device='cuda:0')\n",
      "valid_loss:  0.4087719601583263\n",
      "valid_score:  0.8806297656579828\n",
      "best_valid_score:  0.8806297656579828\n",
      "time used:  570.8083019256592\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.3731123570033482 inst loss:  0.34809553623199463\n",
      "[700/2633]  avg loss:  0.37108799525669645 inst loss:  0.3579268455505371\n",
      "[1050/2633]  avg loss:  0.3704957217261905 inst loss:  0.3678751289844513\n",
      "[1400/2633]  avg loss:  0.3706329345703125 inst loss:  0.3745664358139038\n",
      "[1750/2633]  avg loss:  0.36999319893973215 inst loss:  0.386250376701355\n",
      "[2100/2633]  avg loss:  0.36988688151041665 inst loss:  0.34248268604278564\n",
      "[2450/2633]  avg loss:  0.3697311463647959 inst loss:  0.4049832224845886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3695, device='cuda:0')\n",
      "valid_loss:  0.40624334850267735\n",
      "valid_score:  0.8813895300549579\n",
      "best_valid_score:  0.8813895300549579\n",
      "time used:  571.057210445404\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350/2633]  avg loss:  0.36400604248046875 inst loss:  0.3634847402572632\n",
      "[700/2633]  avg loss:  0.36559454781668527 inst loss:  0.3478764295578003\n",
      "[1050/2633]  avg loss:  0.36558771042596727 inst loss:  0.3488766551017761\n",
      "[1400/2633]  avg loss:  0.36468475341796874 inst loss:  0.33334654569625854\n",
      "[1750/2633]  avg loss:  0.36465182059151785 inst loss:  0.35670408606529236\n",
      "[2100/2633]  avg loss:  0.3648692394438244 inst loss:  0.3611897826194763\n",
      "[2450/2633]  avg loss:  0.3650308165258291 inst loss:  0.3838743567466736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(0.3650, device='cuda:0')\n",
      "valid_loss:  0.40561510561263725\n",
      "valid_score:  0.8815154501840012\n",
      "best_valid_score:  0.8815154501840012\n",
      "time used:  570.7957179546356\n",
      "Fold:  1\n",
      "skip\n",
      "Fold:  2\n",
      "skip\n",
      "Fold:  3\n",
      "skip\n",
      "Fold:  4\n",
      "skip\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1295<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/wandb/run-20210926_174629-95awpvh6/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/wandb/run-20210926_174629-95awpvh6/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold0] avg_train_loss</td><td>0.365</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.40562</td></tr><tr><td>[fold0] epoch</td><td>6</td></tr><tr><td>[fold0] loss</td><td>0.35093</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] val_score</td><td>0.88152</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>[fold0] avg_train_loss</td><td>█▄▃▂▂▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>█▂▁▃▂▂</td></tr><tr><td>[fold0] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold0] loss</td><td>█▄▆▅▄▅▅▅▇▂▄▆▃▄▆▅▁▂▄▅▅▄▃▄▂▄▂▃▃▄▁▃▃▂▃▃▂▄▃▃</td></tr><tr><td>[fold0] lr</td><td>▂▃▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>[fold0] val_score</td><td>▁▅▆▇██</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">124th_V2SDCBAM_PL_6ep_2em3lr_32ch_vf+gn+sc01+tm+ts</strong>: <a href=\"https://wandb.ai/kaggle_go/G2Net/runs/95awpvh6\" target=\"_blank\">https://wandb.ai/kaggle_go/G2Net/runs/95awpvh6</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folds score: [0.8815154501840012]\n",
      "Avg: 0.88152\n",
      "Std: 0.00000\n",
      "CPU times: user 55min 17s, sys: 3min 26s, total: 58min 44s\n",
      "Wall time: 57min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "#     %lprun -f DataRetriever.__getitem__ -f Trainer.train_epoch -f Trainer.fit -f Trainer.valid_epoch training_loop() \n",
    "    training_loop(train_df,Config.use_checkpoint)\n",
    "except RuntimeError as e:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()   \n",
    "    print(e)# saving oof predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a03bfac-e977-4cc1-908f-fd91ad857903",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eec60e-00c2-43aa-9768-8fb74482de2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d39375ae-356c-4742-8f68-f020b8d71fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(Config.train_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce12376c-30df-443b-b057-3a54f4c75230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# import Ipython\n",
    "# IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccdc76b4-47ca-4622-a4c1-0763f96aaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c80731b-2b25-48c4-83bc-491572148207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jarviscloud import jarviscloud\n",
    "# jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b00fb94-6a9f-43f0-b73d-5738a7e48439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "successfully saved oof predictions for Fold:  0\n",
      "1\n",
      "successfully saved oof predictions for Fold:  1\n",
      "2\n",
      "successfully saved oof predictions for Fold:  2\n",
      "3\n",
      "successfully saved oof predictions for Fold:  3\n",
      "4\n",
      "successfully saved oof predictions for Fold:  4\n"
     ]
    }
   ],
   "source": [
    "for fold in Config.train_folds:\n",
    "    print(fold)\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    # print(checkpoint['valid_preds'])\n",
    "    try:\n",
    "        # oof = pd.read_csv(f'{Config.gdrive}/Fold_{fold}_indices.csv') also works, used in replacement of next statement for previously not generated Fold_{fold}_oof_pred.csv\n",
    "        oof = pd.read_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv')\n",
    "        oof['pred'] = checkpoint['valid_preds']\n",
    "        oof.to_csv(f'{Config.model_output_folder}/Fold_{fold}_oof_pred.csv') \n",
    "        print('successfully saved oof predictions for Fold: ', fold)   \n",
    "    except:\n",
    "        raise RuntimeError('failure in saving predictions for Fold: ', fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb59e1-9caf-4636-bb62-18c237d5c716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6afe6c-0be0-485b-a2e2-a899024b8a80",
   "metadata": {},
   "source": [
    "# add TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dafe5ee8-3a67-43fd-94f8-4f04984c9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e70a9794-653f-4c95-9108-547bf07f2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tbs need pythonic way\n",
    "class TTA(Dataset):\n",
    "    def __init__(self, paths, targets, vflip=False, shuffle_channels=False, time_shift=False, \n",
    "                 add_gaussian_noise = False,  time_stretch=False,shuffle01=False,timemask=False,\n",
    "                 shift_channel=False,reduce_SNR=False, ):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        self.vflip = vflip\n",
    "        self.shuffle_channels = shuffle_channels\n",
    "        self.time_shift = time_shift\n",
    "        self.add_gaussian_noise = add_gaussian_noise\n",
    "        self.time_stretch = time_stretch\n",
    "        self.shuffle01 = shuffle01\n",
    "        self.timemask = timemask\n",
    "        self.shift_channel = shift_channel\n",
    "        self.reduce_SNR = reduce_SNR\n",
    "        if time_shift:\n",
    "            self.time_shift = A.Shift(min_fraction=-Config.time_shift_left*1.0/4096, \n",
    "                                      max_fraction=Config.time_shift_right*1.0/4096, p=1,rollover=False)\n",
    "        if add_gaussian_noise:\n",
    "            self.add_gaussian_noise = A.AddGaussianNoise(min_amplitude=0.001*0.015, max_amplitude= 0.015*0.015, p=1)\n",
    "        if time_stretch:\n",
    "            self.time_stretch = A.TimeStretch(min_rate=0.9, max_rate=1.111,leave_length_unchanged=True, p=1)\n",
    "        if timemask:\n",
    "            self.timemask = A.TimeMask(min_band_part=0.0, max_band_part=0.03, fade=False, p=1.0)\n",
    "\n",
    "              \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index] \n",
    "        waves = np.load(path)\n",
    "\n",
    "#         if Config.divide_std:\n",
    "#             waves /= 0.015\n",
    "\n",
    "        if self.vflip:\n",
    "            waves = -waves\n",
    "        if self.shuffle_channels:\n",
    "            np.random.shuffle(waves)\n",
    "        if self.time_shift:\n",
    "            waves = self.time_shift(waves, sample_rate=2048)\n",
    "        if self.add_gaussian_noise:\n",
    "            waves = self.add_gaussian_noise(waves, sample_rate=2048)\n",
    "        if self.time_stretch:\n",
    "            waves = self.time_stretch(waves, sample_rate=2048)\n",
    "        if self.shuffle01:\n",
    "            waves[[0,1]] = waves[[1,0]]\n",
    "        if self.timemask:\n",
    "            waves = self.timemask(waves, sample_rate=2048)\n",
    "        if self.shift_channel:\n",
    "            waves = shift_channel_func(waves, sample_rate=2048)\n",
    "        if self.reduce_SNR:\n",
    "            waves = reduce_SNR_func(waves, sample_rate=2048)\n",
    "        #snr, shift_channel tba\n",
    "        \n",
    "        waves = torch.from_numpy(waves) \n",
    "        target = torch.tensor(self.targets[index],dtype=torch.float)#device=device,             \n",
    "        return (waves, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30c18af1-daaf-409d-bbe8-180ca6db1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e95c39f-c917-4d9b-afe8-dc89d5d8f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(loader,model):\n",
    "    preds = []\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        if step % Config.print_num_steps == 0:\n",
    "            print(\"step {}/{}\".format(step, len(loader)))\n",
    "        with torch.no_grad():\n",
    "            X = batch[0].to(device,non_blocking=Config.non_blocking)\n",
    "            outputs = model(X).squeeze()\n",
    "            preds.append(outputs.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "def get_tta_pred(df,model,**transforms):\n",
    "    data_retriever = TTA(df['file_path'].values, df['target'].values, **transforms)\n",
    "    loader = DataLoader(data_retriever, \n",
    "                            batch_size=Config.batch_size * 2, \n",
    "                            shuffle=False, \n",
    "                            num_workers=Config.num_workers, pin_memory=True, drop_last=False)\n",
    "    return get_pred(loader,model)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f838287-0fa6-442d-b39d-f700b6ea483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['vflip', 'add_gaussian_noise', 'timemask', 'shuffle01', 'time_shift']\n"
     ]
    }
   ],
   "source": [
    "##TTA for oof\n",
    "print(conserv_transform_list_strings)\n",
    "print(aggressive_transform_list_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "709164c2-28c6-4eed-954e-3fc524bc9fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[()]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "conserv_transform_powerset = list(powerset(conserv_transform_list_strings))\n",
    "conserv_transform_powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7343696-bcac-4160-8a76-0d79c0dee4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "for transformations in conserv_transform_powerset:\n",
    "    print({transformation:True for transformation in transformations})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf5e8f7-1349-45fe-b646-42e0febed1c3",
   "metadata": {},
   "source": [
    "## generate oof tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c09fa2a-7afc-4c7d-9fbe-fa02890d73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n",
      "Fold  0\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__add_gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__time_shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__add_gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__time_shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  2\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__add_gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__time_shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  3\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__add_gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__time_shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  4\n",
      "tta__vflip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__add_gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__time_shift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Model()\n",
    "\n",
    "for fold in Config.train_folds:\n",
    "    print('Fold ',fold)\n",
    "    oof = train_df.query(f\"fold=={fold}\").copy()\n",
    "    oof['preds'] = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')['valid_preds']\n",
    "    oof['file_path'] = train_df['id'].apply(lambda x :id_2_path(x))\n",
    "    # display(oof)    \n",
    "\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "    \n",
    "    for transformations in conserv_transform_powerset:\n",
    "#         print(transformations)\n",
    "        if transformations:#to avoid double count original\n",
    "            print(\"tta_\"+('_').join(transformations))\n",
    "            oof[\"tta_\"+('_').join(transformations)] = get_tta_pred(oof,model,**{transformation:True for transformation in transformations})\n",
    "        for aggr_transformation in aggressive_transform_list_strings:#tbs combination of conservative and aggressive\n",
    "            print(\"tta_\"+('_').join(transformations)+'_'+aggr_transformation)\n",
    "            oof[\"tta_\"+('_').join(transformations)+'_'+aggr_transformation] = get_tta_pred(oof,model,**{transformation:True for transformation in transformations}, **{aggr_transformation:True})\n",
    "               \n",
    "\n",
    "    oof.to_csv(Config.model_output_folder + f\"/oof_Fold_{fold}.csv\", index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd3fa0d4-3e28-48ea-bf7a-fa6c2df077f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_all = pd.DataFrame()\n",
    "for fold in Config.train_folds:\n",
    "    oof = pd.read_csv(Config.model_output_folder + f\"/oof_Fold_{fold}.csv\")\n",
    "    oof_all = pd.concat([oof_all,oof])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ca818-92aa-4734-8a20-874db80b6c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed70f5fd-00b8-4eb9-a003-6fec6de253d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('_').join(transformations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da1a3065-5238-4efa-8851-cb60c17bc878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 0.8801739879573027\n",
      "tta__vflip 0.8802155890695967\n",
      "tta__add_gaussian_noise 0.8801488065845556\n",
      "tta__timemask 0.8779151828990358\n",
      "tta__shuffle01 0.8796113994017328\n",
      "tta__time_shift 0.8787075903592754\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",roc_auc_score(oof_all['target'], oof_all['preds']))\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    if \"tta\" in col:\n",
    "        print(col,roc_auc_score(oof_all['target'], oof_all[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2757f84-5698-4982-8d06-c3058bc0f410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.880162177937432"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_sample = oof_all[oof_all['fold']==2]\n",
    "roc_auc_score(oof_sample['target'], oof_sample['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b324aba-a293-43a2-86e5-9fae454daa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f727009f8b0>, {'preds': 0.19999999999999996, 'tta__vflip': 0.20000000000000007, 'tta__add_gaussian_noise': 0.20000000000000007, 'tta__timemask': 0.16000000000000003, 'tta__shuffle01': 0.16000000000000003, 'tta__time_shift': 0.08000000000000002})\n",
      "preds 0.19999999999999996\n",
      "tta__vflip 0.20000000000000007\n",
      "tta__add_gaussian_noise 0.20000000000000007\n",
      "tta__timemask 0.16000000000000003\n",
      "tta__shuffle01 0.16000000000000003\n",
      "tta__time_shift 0.08000000000000002\n",
      "preds\n",
      "tta__vflip\n",
      "tta__add_gaussian_noise\n",
      "tta__timemask\n",
      "tta__shuffle01\n",
      "tta__time_shift\n",
      "preds_tta_avg: 0.880417961697041\n"
     ]
    }
   ],
   "source": [
    "oof_all['avg']=0\n",
    "total_weight = 0\n",
    "#weights leaky? not fine tuned\n",
    "\n",
    "oof_weight  = defaultdict(lambda :1)\n",
    "aggr_total_weight = 0\n",
    "for trans in aggressive_transform_list_strings:\n",
    "    aggr_total_weight += getattr(Config(),trans+'_weight')\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    \n",
    "    if 'tta_' in col or 'preds' in col: \n",
    "        for trans in conserv_transform_list_strings:\n",
    "            \n",
    "            if trans in col:\n",
    "                oof_weight[col] *= getattr(Config(),trans+'_proba')\n",
    "            else:\n",
    "                oof_weight[col] *= 1-getattr(Config(),trans+'_proba')\n",
    "            \n",
    "        flag = False\n",
    "        for trans in aggressive_transform_list_strings:\n",
    "            \n",
    "            if trans in col:\n",
    "                oof_weight[col] *= getattr(Config(),trans+'_weight')/aggr_total_weight*Config.aggressive_aug_proba\n",
    "                \n",
    "                flag = True\n",
    "        if not flag:\n",
    "            oof_weight[col] *= (1-Config.aggressive_aug_proba)\n",
    "        \n",
    "print(oof_weight)\n",
    "for key,value in oof_weight.items():\n",
    "    print(key,value)\n",
    "\n",
    "for col in oof_all.columns:\n",
    "    if ('tta_' in col or 'preds' in col): # and 'time_shift' not in col and 'timemask' not in col\n",
    "        print(col)\n",
    "        total_weight+=oof_weight[col]\n",
    "        oof_all['avg'] += oof_all[col]*oof_weight[col]\n",
    "oof_all['avg'] /= total_weight\n",
    "\n",
    "print(\"preds_tta_avg:\",roc_auc_score(oof_all['target'], oof_all['avg']))\n",
    "\n",
    "oof_all.to_csv(Config.model_output_folder + \"/oof_all.csv\", index=False)\n",
    "oof_all[['id','fold','avg']].rename(columns={'id':'id','fold':'fold','avg':'prediction'}).to_csv(Config.model_output_folder + \"/oof_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a7c77-63b3-4efd-82e3-588ff224c583",
   "metadata": {},
   "source": [
    "## generate TTA for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a1c70-e352-435b-8767-3c397d4bd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelIafossV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__vflip_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__timemask_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__shuffle01_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__vflip_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__timemask_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__shuffle01_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__vflip_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__vflip_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__timemask_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__shuffle01_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__time_shift_Fold_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__vflip_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__add_gaussian_noise_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n",
      "tta__timemask_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 350/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tta__shuffle01_Fold_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "test_df['target'] = 0  \n",
    "model = Model()\n",
    "\n",
    "for fold in Config.train_folds:\n",
    "    test_df2 = test_df.copy()\n",
    "    checkpoint = torch.load(f'{Config.model_output_folder}/Fold_{fold}_best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device=device,non_blocking=Config.non_blocking)\n",
    "    model.eval()\n",
    "\n",
    "    test_df2['preds'+f'_Fold_{fold}'] = get_tta_pred(test_df2,model)\n",
    "\n",
    "    for transformations in conserv_transform_powerset:\n",
    "#         print(transformations)\n",
    "        if transformations:#to avoid double count original\n",
    "            print(\"tta_\"+('_').join(transformations)+f'_Fold_{fold}')\n",
    "            test_df2[\"tta_\"+('_').join(transformations)+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,**{transformation:True for transformation in transformations})\n",
    "        for transformation in aggressive_transform_list_strings:#tbs combination of conservative and aggressive\n",
    "            print(\"tta_\"+('_').join(transformations)+'_'+transformation+f'_Fold_{fold}')\n",
    "            test_df2[\"tta_\"+('_').join(transformations)+'_'+transformation+f'_Fold_{fold}'] = get_tta_pred(test_df2,model,**{transformation:True for transformation in transformations}, **{transformation:True})\n",
    "               \n",
    "    test_df2.to_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab6a61-d4eb-447e-ae1f-33b1af2958ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg = test_df[['id', 'target']].copy()\n",
    "test_avg['target'] = 0\n",
    "# print(test_avg.describe())\n",
    "\n",
    "total_weight = 0\n",
    "for fold in Config.train_folds:\n",
    "#     test_weight = {key+f'_Fold_{fold}':value for key,value in oof_weight.items()}\n",
    "    test_weight = oof_weight #defaultdict(lambda:1)\n",
    "    test_df2 = pd.read_csv(Config.model_output_folder + f\"/test_Fold_{fold}.csv\")\n",
    "#     print(test_df2.describe())\n",
    "    for col in test_df2.columns:\n",
    "        col_weight = col.split('_Fold_')[0]\n",
    "        if ('tta_' in col or 'preds' in col): \n",
    "#             print(col)\n",
    "#             print(test_weight[col_weight])\n",
    "            total_weight+=test_weight[col_weight]\n",
    "            test_avg['target'] += test_df2[col]*test_weight[col_weight]\n",
    "test_avg['target'] /= total_weight\n",
    "print(test_avg.describe())\n",
    "print(test_avg[\"target\"].hist(bins=100))\n",
    "print(test_avg)\n",
    "# print(total_weight)\n",
    "test_avg.to_csv(Config.model_output_folder + \"/test_avg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd406d-d7fd-4958-8e73-9d2caa624c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a2c45-3493-49b5-a750-d4bb4b33303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg[['id', 'target']].to_csv(\"./submission.csv\", index=False)\n",
    "\n",
    "test_avg[['id', 'target']].to_csv(Config.model_output_folder + \"/submission.csv\", index=False)\n",
    "\n",
    "!mkdir -p ~/.kaggle/ && cp $Config.kaggle_json_path ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f74d3-2783-4014-9b37-a0d6997797ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c g2net-gravitational-wave-detection -f ./submission.csv -m $Config.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6966b-8273-46fa-bc42-ec36bf4b2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarviscloud import jarviscloud\n",
    "jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea4e9c-95c5-4aba-9930-e13842e2c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -cf  $Config.model_version  -C  $Config.model_output_folder ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f3ae8-bce5-49f7-82a7-fdd49ef16eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "340px",
    "width": "209.2px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
